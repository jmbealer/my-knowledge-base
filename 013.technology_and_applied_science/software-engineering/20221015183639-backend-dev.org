:PROPERTIES:
:ID:       55632b72-6ba7-4571-a9ff-06c45a197471
:mtime:    20230206181400 20221015184115
:ctime:    20221015183639
:END:
#+title: backend-dev


* Backend Development - Modules 30-40
** Node and Express - Module 30
*** 30.1 Overview: Node and Express
Overview: Node and Express
9 minutesEstimated completion time
Overview

In this module, you will build a fully working server with multiple routes and error handling.

At this point in your journey, you're ready to create your own server that will respond to requests made to it. Although you will start by responding with only JSON, the servers that you build will be capable of responding with all types of data, including HTML. As you continue through the module, you will enhance your server with middleware and error handling, building a strong foundation for the modules to come.
Do this

The purpose of the Do this sections in this module is to give you important hands-on experience. In these sections, you'll perform various tasks, like setting up a development environment or executing a command. Ultimately, these practice sections will help you successfully complete graded assessments, such as projects, mock interviews, and capstones.

At the end, you'll need to take a quiz, so spend some time studying up. Alternatively, you'll join on as an apprentice, but you'll still need to ace this quiz first.
*** 30.2 Project configuration
Project configuration
1.5 hoursEstimated completion time
Learning Objective

By the end of this lesson, you will be able to run a simple server with Node and Express. You'll also be able to configure your project so that your server automatically updates upon changes.
Overview

In this lesson, you will learn how to run a server on your machine using a popular package called Express. You will also set up your development process for success by installing and using a package called nodemon.
Key Terms

Web framework
    Tools and software that help developers build web applications more easily

How are you reading this curriculum right now? Where does this text come from? If you're logged in, how does this site know that you are who you say you are? Many of these questions are important in web development and are solved by a server. Servers send information back to clients like your browser. That information could be text to read, or it could be something more complicated, like credentials.
What is Express?

The video below provides a brief introduction to this topic. Start by watching the video, and then read through the rest of the lesson and complete the practice exercises. This will give you a thorough understanding of these concepts.

To build your servers, you will be using a node package called Express. Express is a minimalist web framework for Node.js. A web framework is a tool that you can use to make building applications for the web easier. In this case, Express abstracts away many of the difficulties of working directly with Node.

To be clear, you could build a server without using Express. There are other web frameworks that work with Node. Alternatively, you could build a server with only Node. However, Express makes the process both easier and more readable.
Do this
Create a project

Now, create a folder for the work that you will do in this module. On your computer, create a new folder called node-express-getting-started. Inside that folder, run the following commands:

npm init -y

npm install express@4

File structure

A fully functional server can often have dozens of files, all of which serve a different purpose. That means that it is important for you to put together a file structure that helps you find what you need when you need it.

Building a server may be the most complex project that you've worked on yet. With dozens of folders and files, it can become confusing. So when you're creating a complex project like this, take time to name your files and folders well. Your future self will thank you.
Do this
Create your project

In your node-express-getting-started folder, create a new folder called src. This folder is where you will put the majority of the code related to your server. Then create two files: app.js and server.js.

Your file structure should look like the following once you are done:

.

├── package.json

└── src

    ├── app.js

    └── server.js

The purpose of this new folder and files is as follows:

    The src folder holds the source code for your server. Code directly related to running the server will go here.

    The app.js file is where you will configure your Express application.

    The server.js file is where you will run your Express application.

As you will see, you could just have a single file that includes all of the code from app.js and server.js. However, when building your server, you want to keep in mind the concept of separating concerns: each file should focus on only one task.
The Express application

Now that you have installed Express and set up your file structure, create your Express application.

The following code initializes an Express application.

const express = require("express");

const app = express();


module.exports = app;

Below is a description of what each line of code above accomplishes.
Code
	Description
const express = require("express");
	You require the Express package and assign it to a variable.
const app = express();
	The Express package exports a function. When you invoke that function, you get a new Express application and assign it to a variable.
module.exports = app;
	You export the Express application to be used in the server.js file.
Do this
Create your application file

Type the above content into your app.js file. Read through the table above more than once, and take note of any questions that you have at this point.
The server file

You may be happy to know that your server.js file will also be very short. This file is responsible for running the application. The following code, when run, will allow your server to "listen" on the specified port.

A port is a way to have multiple applications running on the same machine. When a request goes to a computer, it will specify a port to ensure that it goes to a specific application. For example, you could have three different servers running on your computer, each one on a different port.

Make sure to read the comments to understand what is happening on each line.

const { PORT = 5000 } = process.env;

const app = require("./app");


const listener = () => console.log(`Listening on Port ${PORT}!`);

app.listen(PORT, listener);

Below is a description of what each line of code above accomplishes.
Code
	Description
const { PORT = 5000 } = process.env;
	With destructuring and default arguments, set the variable PORT to be equal to whatever value is found inside of process.env or default to 5000.
const app = require("./app");
	You require the Express application that you exported.
const listener = () => console.log(`Listening on Port ${PORT}!`);
	This function will run when the server successfully starts.
app.listen(PORT, listener);
	The listen() method on your Express application is what runs the server. It takes two arguments: a port number and a function. The PORT variable defines where your server is running, and the listener() function will get called as soon as the server has successfully started.
Do this
Create your server file

Type the above content into your server.js file. Read through the table above more than once and take note of any questions that you have at this point.
Starting and stopping

The video below provides an introduction to this topic.

You now have all the code that you need to run your server. When you run your server, it will reserve the port that you specified. That means that nothing else can run on that port. When you stop the server, that port will be free again.

How are you going to access this port? Every computer has a special domain called localhost. This domain can be used to access whatever is running on your own machine.

localhost is a special domain that is available for applications running on your computer. The URL will look something like this:

http://localhost:5000

When you enter the above URL into a browser, the browser will attempt to contact any application running on your computer on port 5000.
Do this
Start your server

From your command line, make sure that you are in the node-express-getting-started folder. Then run the following command:

node ./src/server.js

If you get an Error: listen EADDRINUSE: address already in use :::5000 error, you may need to change the port number because another application is already using port 5000. You can change the port by specifying another port number in server.js. For example, you can specify const { PORT = 8080 } = process.env;). Alternatively, you can free any port that is already in use by exiting the application that is using that port.

Note: If you are a macOS Monterey user and are getting the Error: listen EADDRINUSE: address already in use :::5000 error, you may want to disable Airplay Receiver; this is an application that uses port 5000. To disable Airplay Receiver, go to System Preferences and disable Airplay Receiver in the Sharing system section.

You will see the following message:
Message that server is running.

You may also notice that you no longer have your command prompt back. For example, if you begin typing and press Enter, you will notice that nothing is happening.
Nothing is happening after typing and pressing Enter.

Don't worry; this is normal. This means that your server is running constantly. Servers are unlike other programs that you may have written, in that they constantly run until they are told to stop. They are constantly listening for incoming requests.
Stop your server

To stop your server, you only need to press Control+C. You will then get back your command prompt and be able to start your server again.
Creating a script

Next, create a script to make this process a bit easier. In your package.json file, replace the existing "scripts" with the following:

"scripts": {

  "start": "node src/server.js"

},

Finally, from the command, you should now be able to type npm start. This will only start your server, not stop it. If you have any issues, check the syntax of your JSON file.
Run multiple servers

Now, try to run multiple instances of your server at the same time. In one terminal window, start your server using npm start. Then, open up a new window. Make sure that you are in the same folder as before, and then run npm start again.

What happened? You likely received an error that looked something like this:

Error: listen EADDRINUSE: address already in use :::5000

This error is telling you that port 5000 is already in use. You can't run two applications that use the same port.

Instead, try running the following command:

PORT=4999 npm start

Now you will have two servers running, one on port 5000 and another on port 4999.
Make changes to your server

Make sure that at least one server is running, and then change the message of the listener() function in the server.js file. If you look at your terminal, you'll notice that nothing has happened.

Stop the server and then restart it. Now you'll be able to see the change. Once your server is running, it will not pick up any new changes that you make to it unless you restart it.
The nodemon package

The video below provides a brief introduction to the nodemon package.

Each time that you make a change to your server, you have to start and stop it once again. Manually starting and stopping your server every time that you want to make changes to it would significantly slow down your development process. Thankfully, you can avoid doing so by installing a package called nodemon.

The nodemon package will watch for any changes that happen to your server and automatically reload it for you. This package alone will save you hours of time when you're working on more complex servers in the future.

While working on your local machine, you should rely on using nodemon instead of running the server manually with node src/server.js.
Do this
Install nodemon

You can install nodemon just like you would any other package. However, in this case, use the --save-dev flag because you won't need nodemon for anything other than development purposes.

npm i nodemon --save-dev

Add nodemon to a script

To use nodemon, create a script. Add the following script to your "scripts" object in your package.json file.

"dev": "nodemon src/server.js"

You can then run the following command on your command line.

npm run dev

You'll then see something a bit different in your command line:
Using nodemon in command line.

Try changing the text in your listener() function in your server.js file. You will see the new message show up in your terminal almost immediately.
Complete example

A completed example from this lesson can be found here:

    Node and Express: Project configuration

*** 30.3 Application-level middleware
Application-level middleware
1.5 hoursEstimated completion time
Learning Objective

By the end of this lesson, you will be able to describe each parameter in an Express middleware function and create your own application-level middleware. You'll also be able to install and use common middleware packages.
Overview

In this lesson, you will learn about an Express concept called application-level middleware. This concept will allow you to potentially configure every request coming in or going out of your server.
Key Terms

Middleware
    A function that an Express server runs between receiving a request and responding to that request

Starter code

This lesson continues using the project that you created in the previous lesson. If you need to, you can download that code here:

    Node and Express: Project configuration

What is middleware?

The video below provides a brief introduction to middleware. Start by watching the video, and then read through the rest of the lesson and complete the practice tasks. This will give you a thorough understanding of this topic.

Middleware is a very general term that can have different meanings based on the context. You will be learning about middleware specifically as it pertains to building servers with Express. When it's mentioned in this program, the term middleware will refer to a function that an Express server runs between receiving a request and responding to that request.

Remember that the job of the server is to receive requests and respond to them. Middleware gets between the request-response cycle. It helps manage the request and can help determine how the server should respond. Express puts multiple pieces of middleware together, creating a middleware pipeline. This pipeline is simply a series of functions.

For example, imagine that a user is logging in to a banking website. The request pipeline might look something like the following diagram:
Request pipeline for logging on a banking website.

Now, break down each of these steps:

    A request is made to the server for checking account details.

    The request enters the first piece of middleware: logging. At this step, nothing more happens than a simple log() statement describing the request.

    Check for user credentials. Depending on whether or not the user is logged in, the server will respond differently.

    If the user is logged in, you respond with the requested information. If they aren't logged in, you respond with an error. You only respond once.

Steps two and three above act as middleware. They act on the request (in this example, logging) and help determine its outcome (in this example, checking for credentials).
Middleware acting on the request and helping to determine response.
Express middleware parameters

The following video provides an introduction to Express middleware parameters.

In Express, middleware is represented through functions. The function will always have a similar function signature:

const middleware = (req, res, next) => {

  // Middleware function body

};

Middleware functions are callback functions. It is unlikely that you will ever call a middleware function directly—Express calls the function for you, with the right arguments.

Almost every middleware function will have no more than three parameters. You'll learn about the single exception later on.

Take a look at each parameter:

    The req parameter stands for request. Information and methods related to the incoming request will be stored in this object.

    The res parameter stands for response. This object has information and methods related to sending back a response from the server.

    The next parameter, when called, will tell Express that this middleware function is complete. It will then go on to the next piece of middleware.

Responsive middleware

Take a look at the following middleware function.

const sayHello = (req, res, next) => {

  res.send("Hello!");

};

This middleware uses the send() method that comes on the res object. Calling send() in this way will send back the string to the client.

This middleware function responds. That is, it will send a response back to the client that made a request to it. At that point, your server's job is done.
Do this
Copy the middleware

Copy the above function (in other words, the sayHello() function) into your app.js file. Include them above where you export the Express application.
Nonresponsive middleware

Take a look at another middleware function:

const logging = (req, res, next) => {

  console.log("A request is being made!");

  next();

};

This middleware doesn't respond. That is, it doesn't send a response back to the client. Instead, all it does is print to the console before moving on to the next piece of middleware.
Do this
Copy the middleware

Copy the above logging() function into your app.js file. Include it above the code where you export the Express application.
Creating application-level middleware

Right now, your middleware functions exist but aren't being used. How do you use them in your application?

Attached to your Express application (app) is a method called use(). This method will allow you to attach middleware to the pipeline. You can use your middleware functions by doing the following:

app.use(logging);

app.use(sayHello);

Every request that comes in will first go through these lines of code, unless an error occurs or the server sends a response.
Do this
Test out your middleware

How do you know it's working? First, include the above lines in your app.js file, below your function definitions. Then, try going to the following website in your browser. When you go to this website, you are making a request to your server.

http://localhost:5000

You should see the phrase Hello!. Now, take a look at the terminal where you are running your server. You should see the console.log() statement.
Switch the order

Try switching the order of the two app.use() statements so that the sayHello() middleware comes before the logging() middleware. When you refresh your browser page, you will still see Hello!. But, check your terminal—do you still see the console.log() statement? Why or why not?

If you don't notice any changes, try restarting your server.
Reflect

At this point, you've created a working server. Your server now receives requests and responds to them! Now is a good time to reflect on what you've learned so far.

Write some comments in your code that describe what you have written. Consider including terms like request, response, function, and middleware.
The morgan package

The video below provides an introduction to the morgan package.

You have now built your own middleware and are using it to respond to requests. Next, install a common package that can be used as middleware and will replace your logging function. The morgan package is a small logging package that will print useful information to your terminal window on each request.
Do this
Use morgan

Run the following in a terminal window. Make sure that you are in the correct folder and that your server isn't running where you paste the following command.

npm i morgan

First, require morgan at the top of your app.js file, before all of your other app.use() statements.

const morgan = require("morgan");

Then, delete your logging() function and where it gets used. Replace it with the following:

app.use(morgan("dev"));

Exported from the morgan package is a function that can be called with a few predefined strings. The return value of the function is middleware, configured by the "dev" string.

Now go to localhost. You will- still see the phrase Hello!.

Check your terminal. You will see something like the following line in your logs:

GET / 200 0.433 ms - 6

This line is provided by morgan; morgan will log incoming requests. As you continue to build servers, this logging information will become more and more useful.
Complete example

A completed example from this lesson can be found here:

    Node and Express: Application-level middleware
*** 30.4 Building routes
Building routes
1.5 hoursEstimated completion time
Learning Objective

By the end of this lesson, you will be able to create routes to handle requests.
Overview

So far, you have been sending requests to your server via a single URL: http://localhost:5000. But in the real world, users can visit different URLs to get different kinds of information from a website. For example, the Thinkful website responds with different information when you go to thinkful.com/about as opposed to thinkful.com/blog. In this lesson, you will learn how to create routes on your server so that it can respond to client requests with varied information.
Key Terms

Route
    Also called a path, the part of the URL that comes after the domain name

Starter code

This lesson continues using the project that you created in the previous lesson. If you need to, you can download that code here:

    Node and Express: Application-level middleware

What are routes?

The video below provides a brief introduction to routes. Start by watching the video, and then read through the rest of the lesson and complete the practice tasks. This will give you a thorough understanding of this topic.

How does Thinkful know that you want different information when you go to thinkful.com/about as opposed to thinkful.com/blog? In large part, it has to do with the route. A route (or a path) is the part of the URL that comes after the domain name.

In the case of thinkful.com/blog, the route is /blog. Going to that route as opposed to /about yields different information.

A route can have multiple parts, like in the following example:

https://www.thinkful.com/bootcamp/web-development

As you can see, this route has multiple parts:

    /bootcamp

    /web-development

    /flexible

The order of each of these parts matters in getting to the route. In general, paths are arbitrary. But as you will see later on, there are some conventions to follow. For now, just know that there can be multiple parts to a route.
Express routes

The video below will show you how to work with Express routes.

Up until now, you have used the app.use() method to build middleware to handle requests. Express also has other methods to handle requests that correspond to the request's HTTP verb.
HTTP verb
	Express method
DELETE
	app.delete()
GET
	app.get()
POST
	app.post()
PUT
	app.put()

These functions work similarly to app.use(). For example, the following is a route in Express that will respond with the text OK when you go to the URL http://localhost:5000/ping.

app.get("/ping", (req, res) => {

  res.send("OK");

});

As you can see, this looks very similar to the middleware that you built earlier—except you now have a string as the first argument passed into the app.get() method. This builds a route at /ping.

Routes are middleware except they will only respond when the request URL matches the route.
Do this
Create a route

Replace where you're currently using the sayHello() function with the following route:

app.get("/hello", sayHello);

You've created your first route!
Test your application

First, make sure to start your server.

Then go to the following URL:

http://localhost:5000/hello

What do you see?

Now, try going to the following URL:

http://localhost:5000

You will see an error message this time. This error is generated by Express, although you will soon learn how to customize it.

If you recall, going to this URL before resulted in a successful response from the server. So what has changed? Before, the server was responding to every request with the sayHello() middleware. But now, the server is only responding with the sayHello() middleware when you go to the /hello route.
Complete example

A completed example from this lesson can be found here:

    Node and Express: Building routes
*** 30.5 Query and route parameters
Query and route parameters
1.5 hoursEstimated completion time
Learning Objective

By the end of this lesson, you will be able to access relevant information through route and query parameters.
Overview

Routing on its own is quite powerful. So far, you've learned how to build routes that can respond with all kinds of information. In this lesson, you will learn how to use dynamic user input to modify your routes' responses.
Key Terms

Query string
    Text that comes at the end of a URL and provides additional information to a given route
Query parameter
    A key-value pair in a query string, used to filter the resources requested from the server
Route parameter
    A part of the URL that changes depending on the data to be displayed on the website, used to access parts of a URL by storing the value in a variable

Starter code

This lesson continues using the project that you created in the previous lesson. If you need to, you can download that code here:

    Node and Express: Building routes

Query parameters

The video below provides a brief introduction to query parameters. Start by watching the video, and then read through the rest of the lesson and complete the practice tasks. This will give you a thorough understanding of this topic.

Take a look at the following URL:

https://www.google.com/search?q=javascript

In the above URL, you can see that there is a route called /search. But is the content after /search a new route? No, what comes after is called a query string, which contains a single query parameter.

A query string is text that comes at the end of a URL following a question mark ?. It provides additional information to the given route and can contain multiple query parameters. A query parameter is a key-value pair in a query string. The key and the value are strings separated by an equals sign =. In the above example, the query parameter key is q and the value is javascript.

Query strings provide additional information to your route and can be easily read by Express if formatted properly. Inside of the route function, you can access query parameters by using req.query. This object will be empty if there are no parameters; otherwise, it will be an object of key-value pairs.

For example, imagine that you had the following route:

app.get("/songs", (req, res) => {

  const title = req.query.title;

  res.send(title);

});

And imagine that you visited the following URL:

http://localhost:5000/songs?title=Distant

Then, you would receive the following text:

Distant

Do this
Use query parameters

Update your sayHello() function so that it looks like this:

const sayHello = (req, res) => {

  console.log(req.query);

  const name = req.query.name;

  const content = name ? `Hello, ${name}!` : "Hello!";

  res.send(content);

};

Take a moment to read through the above code, making sure that you understand it. Then, try making a request to the following URL:

http://localhost:5000/hello?name=Danni

What do you see in the browser? What do you see in the terminal window?
Route parameters

The following video provides an overview of route parameters.

Take a look at the following two URLs:

https://www.thinkful.com/blog/tag/software-engineering/

https://www.thinkful.com/blog/tag/data-science/

These URLs are similar except for the last part of the route. If you were to build routes like this in your Express code, would you have two separate routes?

The answer is that it depends. It is possible to use two different routes to return different information, but it is also possible to just use one with route parameters. Route parameters give you access to parts of a URL by storing the value in a variable. This means that part of the route can be swapped out with another value and potentially work.

For example, take a look at the following code:

const saySomething = (req, res) => {

  const greeting = req.params.greeting;

  const content = `${greeting}!`;

  res.send(content);

};


app.get("/say/:greeting", saySomething);

The route that you define contains two parts, one of which has a colon : in front of the string greeting. In your route function, you can access all route parameters with req.params, which will return an object. If there are any, they will be stored inside as key-value pairs.
Do this
Use route parameters

Copy the code above into your app.js file. You can place it anywhere below where your Express application is created. Then, try making a request to the following URL:

http://localhost:5000/say/Greetings

Take a moment to go to the same route while changing the route parameter.
Skip the parameter

Try making a request to the following URL:

http://localhost:5000/say

What happens? Why do you think so?
Put it all together

The following video explores how to work with query and route parameters together.

Update your saySomething() function to the following:

const saySomething = (req, res) => {

  const greeting = req.params.greeting;

  const name = req.query.name;


  const content = greeting && name ? `${greeting}, ${name}!` : `${greeting}!`;

  res.send(content);

};

Try making a request that makes use of both the route parameter and the query parameter.
Multiple routes

You now have multiple routes running on your server. For now, assume you have the requests ordered like this:

app.get("/hello", sayHello);

app.get("/say/:greeting", saySomething);

If you make a request to /say/Hola, does Express look at your /hello route at all?

The answer is yes. Express looks at each piece of middleware in order. If it doesn't have a route string, it runs the middleware as expected. But if it does have a route string, it will first look to see if the request URL matches up with the route string. If it does, it will run the route function; otherwise, it will skip over it.
Order matters

Every incoming request will go through every piece of middleware, looking for a match to the route if there is one. This means that the order of your routes can matter. Take a look at the following example:

app.get("/say/:greeting", saySomething);


const sayGoodbye = (req, res) => {

  res.send("Sorry to see you go!");

};


app.get("/say/goodbye", sayGoodbye);

If you make a request to /say/goodbye, will you be calling the saySomething() function or the sayGoodbye() function? Because the /say/:greeting route matches the path /say/goodbye, you will call the saySomething() function first. That means if you went to the following URL:

http://localhost:5000/say/goodbye

You would receive the following response:

goodbye!

This is easy to avoid once you know to look for it. All you need to do is switch the order that these routes are used.

app.get("/say/goodbye", sayGoodbye); // This goes first.

app.get("/say/:greeting", saySomething); // This goes after.

Do this
Order it right

Copy the above sayGoodbye() function and route into your app.js file. Play with the ordering to see what happens when one of the routes is above the other.
Organize your code

Your app.js is likely getting a bit messy. You likely have functions defined between routes, making it a bit less organized. Update your code to fit one of the following organization schemes:

    Instead of naming your functions, place the functions inside of the routes as anonymous functions. Here's an example:

app.get("/say/goodbye", (req, res) => {

  res.send("Sorry to see you go!");

    });

    Move all of your functions to one part of your file and all of your routes below. Add comments above each section, labeling them.

Complete example

A completed example from this lesson can be found here:

    Node and Express: Query and route parameters
*** 30.6 Error handling
Error handling
1.5 hoursEstimated completion time
Learning Objective

By the end of this lesson, you will be able to create an error handler for the case where a route doesn't exist. You'll also be able to create a general error handler that can be triggered through the next() function.
Overview

Remember that even if a user sends a bad request to your server, you should still respond. As you'll see in this lesson, Express comes with some built-in ways to handle errors that reduce the amount of code that you have to write.
Starter code

This lesson requires you to have a working server. You may use one of your existing projects, or you can download the code from an earlier lesson here:

    Node and Express: Project configuration

Not-found handler

You've likely come across a web page like this before:
GitHub's 404 error page

If a user tries to go to a route that doesn't exist, you want to let them know. Express, by default, will respond with a message like the following if you try to go to a nonexistent route:

Cannot GET /

You actually already have all the tools available to you to build a custom handler for this kind of issue. Recall the following:

    Express uses a middleware pipeline to determine how it should respond. When a request comes in, it will look at every route that it can to search for a match.

    If you provide a function as the first argument in app.use() and Express passes over it when looking for a matching route, that function will be run.

To handle an error where the route cannot be found, you can just create a new piece of middleware. This middleware should be put after all of your routes and doesn't need a specific string argument.

// Not-found handler

app.use((req, res, next) => {

  res.send(`The route ${req.path} does not exist!`);

});

Do this
Add a missing route handler

Add the code above to your server and try making a few different requests to routes that don't exist. What does req.path evaluate to?
Error handler

One common error is that the user may attempt to go to a route that is missing. But there are many more problems that could arise. Express has a special and slightly odd way of creating an error handler. It is exactly the same as other middleware, except for one feature: it has an extra parameter.

// Error handler

app.use((err, req, res, next) => {

  console.error(err);

  res.send(err);

});

You will notice that the only difference in the above middleware is that the first parameter is err. The names of the parameters do not matter; instead, Express is looking for whether or not there are four parameters.

If you try to access this function normally, you may have a hard time. The error-handler middleware only gets called in one of two cases:

    When there is a problem in the application itself (for example, if you made a mistake in your code).

    When you specifically trigger it using the next() function in a previous middleware function.

No matter where you put this error handler, it will not be triggered unless one of the above conditions arises. Express does this so that you have a tool dedicated to handling issues that may arise.
Do this
Add the error handler

Copy the code above and add it to your server. Include it below the middleware that will catch any not-found routes.
Triggering the error handler

Earlier, you learned that using the next() function that's available inside each piece of middleware will move the request to the next part of the middleware pipeline. Calling next() with an argument will move the request to the next error handler.

Take a look at the following route. Spend a few minutes making sure that you understand what the route is asking for and what will happen if it is not provided.

app.get("/states/:abbreviation", (req, res, next) => {

  const abbreviation = req.params.abbreviation;

  if (abbreviation.length !== 2) {

    next("State abbreviation is invalid.");

  } else {

    res.send(`${abbreviation} is a nice state, I'd like to visit.`);

  }

});

The above /states/:abbreviation route is checking to make sure that the abbreviation that is given is no more than two characters. If it is exactly two characters, it will respond as normal. Otherwise, it will call the next() function with a string.

Whatever value gets passed in to next() is the value that will be available to you inside of the error handler as the first argument.
Do this
Cause some errors

Copy the above route into your server, above both of your error handlers. First, make a request to the following URL and make sure that the route works as intended.

http://localhost:5000/states/NY

Then, try to make a request to the following URL.

http://localhost:5000/states/NYC

What do you see in your browser? What do you see in your terminal?
Complete example

A completed example from this lesson can be found here:

    Node and Express: Error handling
*** 30.7 Router-level middleware
Router-level middleware
1.5 hoursEstimated completion time
Learning Objective

By the end of this lesson, you will be able to filter incoming requests to routes with router-level middleware.
Overview

Many of the routes that you create will have similar constraints. Although you can build the same kind of constraints into multiple routes, it's helpful to abstract common functionality into router-level middleware.

The video below provides a brief introduction to router-level middleware. Start by watching the video, and then read through the rest of the lesson and complete the practice tasks. This will give you a thorough understanding of this topic.
Starter code

This lesson continues using the project that you created in the previous lesson. If you need to, you can download that code here:

    Node and Express: Error handling

Duplicate constraints

When a request travels to one of your routes, you will often want to verify that request in some way. For example, take a look at the following route:

app.get("/states/:abbreviation", (req, res, next) => {

  const abbreviation = req.params.abbreviation;

  if (abbreviation.length !== 2) {

    next("State abbreviation is invalid.");

  } else {

    res.send(`${abbreviation} is a nice state, I'd like to visit.`);

  }

});


app.get("/travel/:abbreviation", (req, res, next) => {

  const abbreviation = req.params.abbreviation;

  if (abbreviation.length !== 2) {

    next("State abbreviation is invalid.");

  } else {

    res.send(`Enjoy your trip to ${abbreviation}!`);

  }

});

These routes are very similar in many ways. For example, they both check for whether or not the inputted abbreviation route parameter is exactly two characters. Although the above code will work, your code will be cleaner and easier to maintain if you abstract away the duplicated code.
Do this
Include the routes

Copy the above routes into your server, making sure that you have both. Ensure that you also have an error handler for missing routes and an error handler for when the next() function is called.
Creating router-level middleware

This duplicate code problem can be solved by building router-level middleware. Router-level middleware involves using a middleware function for specific routes. The function looks like all other middleware functions.

Take a look at the function below, which abstracts out the check for the abbreviation length:

const checkForAbbreviationLength = (req, res, next) => {

  const abbreviation = req.params.abbreviation;

  if (abbreviation.length !== 2) {

    next("State abbreviation is invalid.");

  } else {

    next();

  }

};

This function looks pretty similar to the code that you saw earlier. In fact, it is exactly like the above code—except that instead of responding, you call next() once again.

You can place the checkForAbbreviationLength() function between the route name and the responding function, like this:

app.get(

  "/states/:abbreviation",

  checkForAbbreviationLength,

  (req, res, next) => {

    res.send(`${req.params.abbreviation} is a nice state, I'd like to visit.`);

  }

);


app.get(

  "/travel/:abbreviation",

  checkForAbbreviationLength,

  (req, res, next) => {

    res.send(`Enjoy your trip to ${req.params.abbreviation}!`);

  }

);

As you can see, the code for the routes has been reduced thanks to the router-level middleware.

Now, imagine that you are making a request to the following URL:

http://localhost:5000/travel/OR

The process that Express will follow will look somewhat like this:

    Express receives the request and begins to check the middleware pipeline.

    Express takes the request through an application-level middleware (such as morgan) at the top of your file.

    Express looks at the /states/:abbreviation route and, seeing that it does not match the request URL, skips over it.

    Express looks at the /travel/:abbrevation route and, seeing that it does match the request URL, calls the next middleware function.

    The next middleware function is checkForAbbreviationLength(). The function runs and determines that the abbreviation route parameter is valid, calling the next() function with no arguments.

    The route function is now called, and the server responds with "Enjoy your trip to OR!".

Do this
Include router-level middleware

Copy the checkForAbbreviationLength() function into your code and update your routes as described above. Try making requests to both routes to ensure that they work as intended.
Get specific

Update your checkForAbbreviationLength() function so that instead of responding with a generic message, it responds with a message that includes the inputted abbreviation. For example, the response might look like this:

State abbreviation "NYC" is invalid.

Do you see how you only had to make this update in one place as opposed to two?
Why use router-level middleware?

The current example that you've been working with could be solved by creating application-level middleware instead of router-level middleware. Why not just include a line like the following in the server code?

app.use(checkForAbbreviationLength);

The above will work for the current example, but it will fail later if you were to add a route that doesn't use the abbreviation route parameter.
Do this
Perils of application-level middleware

To see this in action, try including the above line in your code, above your existing routes. Then, try going to the following URL:

http://localhost:5000/unknown

In your terminal, you will likely see an error that says something like this:

TypeError: Cannot read property 'length' of undefined

What is this error referring to? What is undefined that the server is attempting to call length on? Hint: Take a closer look at the abbreviation declaration in the checkForAbbreviationLength() function.

Make sure to remove the application-level middleware that you just added before moving on.
Complete example

A completed example from this lesson can be found here:

    Node and Express: Router-level middleware

*** 30.8 Assessment: Node and Express

** Robust Server Structure - Module 31
*** 31.1 Overview: Robust server structure
Overview: Robust server structure
9 minutesEstimated completion time
Overview

Although you can design your APIs however you want, keeping the design uniform and consistent with industry standards can be helpful. This is especially true when other developers also use the API. In this module, you will learn how to build APIs using a set of constraints on how to route and return information from your API. You'll also learn to implement robust validation and error-handling techniques and organize your Express code in a modular manner.

In the previous module, you learned how to create a basic Express server with a simple file structure. Your server also handled a limited number of routes and errors. As an application grows in size and complexity, however, it becomes increasingly important to set guidelines that make it easier for developers to work with the codebase.

In the real world, you will be building and maintaining APIs that will be used by other developers. For example, you could be working as a backend engineer at an e-commerce company. One of your responsibilities could be to build and maintain an API that returns a list of the company's products, to be displayed on a React application. External advertisers might even want to call your API so that they can display your company's products on their website. As you are creating the API, some of the key questions that you might ask include the following:

    How should you structure your API routes so that they're easy for other developers to understand and use?

    How can you best structure your code to manage the complexity of the server architecture?

    How can you properly validate requests to your API and return useful error messages for a variety of API issues?

    How can you ensure that if a new developer joins the React team, they will be more or less familiar with the way that your API works?

Starter code

This module depends on an external repository. Fork and clone the following GitHub repository. Then, follow the instructions on how to get it to run.

    Starter code: Robust server structure
*** 31.2 Static data
Static data
1.5 hoursEstimated completion time
Learning Objective

By the end of this lesson, you will be able to describe what state is as it pertains to a database. You'll also be able to use a static array or object to store state.
Overview

In this lesson, you will build a basic text storage API for sharing code snippets or plain text. Many APIs use a database to store data or state. However, you can also use an array or object to store data.
Key Terms

Application state
    All of the data that an application must keep track of in order to work

Starter code

This lesson requires you to have the following repository running on your local machine.

    Starter code: Robust server structure

Fork and clone the above GitHub repository. Then, follow the instructions on how to get it to run.
Static data

You will now build a basic text-storage API (also known as a pastebin API) that allows users to store code snippets and plain text to share with others. For example, making a request to /pastes should return something like this:

[

    {

      "id": 1,

      "user_id": 1,

      "name": "Hello",

      "syntax": "None",

      "expiration": 10,

      "exposure": "private",

      "text": "Hello World!"

    },

    {

      "id": 2,

      "user_id": 1,

      "name": "Hello World in Python",

      "syntax": "Python",

      "expiration": 24,

      "exposure": "public",

      "text": "print(Hello World!)"

    },

    ...

]

Above, each object in the returned array represents the result of a paste. The object contains an exposure property that is either public or private. The object also contains a unique integer id, a name, the type of syntax, and the text itself.

All of the data above can be accessed via a GET request to /pastes, as follows:

GET http://localhost:5000/pastes

If you need data for just a single paste, you can use the paste's id to get more specific details, like this

GET http://localhost:5000/pastes/:pasteId

Later on, you will build a route that will allow you to add new pastes to the array above.
State

You've learned about state in the context of React. But state isn't exclusive to React—it's a general programming term that describes the status of something as big as an entire application or as small as an individual object. The state of an application—also called application state—is all the data that the application must keep track of in order to work.
Array as state

At this point, the starter code doesn't have anywhere to store the information on each paste. To store this information, you will use an array that needs to be created in a new file.

Now, create a file to store the initial array state.
Do this
Create a pastes-data.js file

You will need to create a folder to store the data files. Inside the project folder, create a new folder called src/data. Inside that folder, create a file called pastes-data.js. Now your project files should look like the following:

.

├── package-lock.json

├── package.json

└── src

    ├── app.js

    ├── data

    │   └── pastes-data.js

    └── server.js

Inside src/data/pastes-data.js, add the following code:

module.exports = [

  {

    id: 1,

    user_id: 1,

    name: "Hello",

    syntax: "None",

    expiration: 10,

    exposure: "private",

    text: "Hello World!"

  },

  {

    id: 2,

    user_id: 1,

    name: "Hello World in Python",

    syntax: "Python",

    expiration: 24,

    exposure: "public",

    text: "print(Hello World!)"

  },

  {

    id: 3,

    user_id: 2,

    name: "String Reverse in JavaScript",

    syntax: "Javascript",

    expiration: 24,

    exposure: "public",

    text: "const stringReverse = str => str.split('').reverse().join('');"

  },

  {

    id: 4,

    user_id: 3,

    name: "Print file sizes in Perl",

    syntax: "Perl",

    expiration: 24,

    exposure: "public",

    text: "ls -lAF | perl -e ’while (<>) { next if /^[dt]/; print +(split)[4], '\n' } ’"

  }

];

The above code defines an array of paste records and exports it for use in the app.js file.

Next, you will create an API endpoint that can access all of the paste data stored in src/data/pastes-data.js and return it to the user.
Do this
Return all pastes

Add the following to app.js, before the not-found and error handlers:

const pastes = require("./data/pastes-data");


app.use("/pastes", (req, res) => {

  res.json({ data: pastes });

});

Below is a description of what each new line of code above accomplishes.
Code
	Description
const pastes = require("./data/pastes-data");
	Reads, executes, and returns the exports object from the ./data/pastes-data file, assigning it to a variable
app.use("/pastes", (req, res)...
	Defines a handler for the /pastes path
res.json({ data: pastes });
	The json() method of the response object, which tells Express to respond to the client with data in JSON format

Ensure that your server is still running. Then visit http://localhost:5000/pastes to see the data from the API. It will look like this:

{

  "data": [

    {

      "id": 1,

      "user_id": 1,

      "name": "Hello",

      "syntax": "None",

      "expiration": 10,

      "exposure": "private",

      "text": "Hello World!"

    },

    {

      "id": 2,

      "user_id": 1,

      "name": "Hello World in Python",

      "syntax": "Python",

      "expiration": 24,

      "exposure": "public",

      "text": "print(Hello World!)"

    },

    {

      "id": 3,

      "user_id": 2,

      "name": "String Reverse in JavaScript",

      "syntax": "Javascript",

      "expiration": 24,

      "exposure": "public",

      "text": "const stringReverse = str => str.split('').reverse().join('');"

    },

    {

      "id": 4,

      "user_id": 3,

      "name": "Print file sizes in Perl",

      "syntax": "Perl",

      "expiration": 24,

      "exposure": "public",

      "text": "ls -lAF | perl -e ’while (<>) { next if /^[dt]/; print +(split)[4], '\n' } ’"

    }

  ]

}

Tip

You might find that you need to restart your server after making changes to your routes.
Why use a data property?

You might be wondering why you returned an object with a data property from your API rather than simply returning the array itself.

This is because you are following a simplified version of the JSON:API specification, a common pattern for APIs returning JSON. You don't need to read or understand this specification right now—just know that it exists. The full specification adds some complexity that you won't find useful right now, so this lesson has simplified it for you.

In short, the APIs that you build will always return an object with either a data property or an errors property. Any information sent to the API will also be an object with a data property. You will learn about errors and sending information to the API in a future lesson.
Do this
Return one paste from /:pasteId

Now, use the paste state to return one paste record by id or return an error if the id doesn't exist.

Add the following to app.js before the app.use("/pastes", ...) handler:

app.use("/pastes/:pasteId", (req, res, next) => {

  const { pasteId } = req.params;

  const foundPaste = pastes.find((paste) => paste.id === Number(pasteId));


  if (foundPaste) {

    res.json({ data: foundPaste });

  } else {

    next(`Paste id not found: ${pasteId}`);

  }

});

The table below summarizes the syntax above:
Code
	Description
app.use("/pastes/:pasteId", (req, res)...
	Defines a handler for the /pastes/:pasteId path
const { pasteId } = req.params;
	Defines the pasteId variable by destructuring it from req.params
const foundPaste = pastes.find((paste) => paste.id === Number(pasteId));
	Uses the find() array method to search for the paste by id. If no id matches, find() returns undefined.
res.json({ data: foundPaste });
	Sends data with the foundPaste object to the client as JSON
next(`Paste not found: ${pasteId}`);
	Calls next() with an error message to move the request to the error handler

Ensure that your server is still running. Then visit http://localhost:5000/pastes/3, and you'll see the following:

{

  "data":

  {

     "id": 3,

     "user_id": 2,

     "name": "String Reverse in JavaScript",

     "syntax": "Javascript",

     "expiration": 24,

     "exposure": "public",

     "text": "const stringReverse = str => str.split('').reverse().join('');"

  }

}

Next, visit http://localhost:5000/pastes/42. You'll see the message Paste id not found: 42.
Completed example

A completed example from this lesson can be found here:

    Starter code: Robust server structure—01-static-data branch
*** 31.3 RESTful APIs
RESTful APIs
1.5 hoursEstimated completion time
Learning Objective

By the end of this lesson, you will be able to build an API following RESTful design principles.
Overview

In this lesson, you'll learn how to use RESTful design principles to build robust APIs.
Key Terms

Representational state transfer
    REST, a set of constraints for building web APIs
RESTful API
    A web API that adheres to the constraints of REST
HTTP request method
    Also called an HTTP verb, a method that indicates the desired action (such as deleting a resource) to be taken on a given resource
API endpoint
    A location where a client can send a request to retrieve a resource that exists on the server
HTTP response status code
    A code sent along with every HTTP response from the server, indicating the server's response to the client

A robust API should give clear direction for API developers and consumers. It should be easy for the people who are using it to make sense of it. Accordingly, the API's design should be simple, predictable, and consistent. One way to ensure a robust API design is by following RESTful design principles when creating your API.

REST, which stands for representational state transfer, is a software architecture style. REST is a set of constraints for building web APIs. If a web API adheres to the constraints of REST, you can call the API a RESTful API.

To get a better idea of how REST works, consider the following recipe for sandwiches:

    A sandwich must contain at least one filling.

    A sandwich must contain at least two slices of bread.

    A sandwich must have one slice of bread on the top and one on the bottom.

This is a very flexible recipe; there's a lot of potential for different sandwiches here! You can have one filling or hundreds of fillings, and two slices of bread or hundreds of slices of bread! You could say that it isn't a recipe, but more like a set of constraints for sandwiches.

REST is like the above recipe in that it establishes a set of patterns and constraints, but for web APIs.
Starter code

For this lesson, it is assumed that you already have the Starter robust server structure repository running on your local machine.

This lesson builds upon the work of the earlier lessons. If you haven't successfully completed the work from the earlier lessons, go back and do that now. If you are having trouble, reach out for assistance.
Representational state transfer (REST)

When thinking about an API, it's pretty common to think about its URLs. With REST, if you have a URL, then you have a resource. Resource refers to the data returned by a request; that data can be a text file, HTML page, image, video, JSON, or something else. Every URL provides access to a resource on your server.

A RESTful API server provides access to resources. A client, like the browser or another server, can then access and change the resources.

Following RESTful design principles, each resource is identified by its URL. For example, the URL /programmers provides access to a collection of programmer resources, while the URL /programmers/dhh provides access to a single programmer resource. So, a collection of resources is also considered one resource. You have already built URLs that work like this, so this may sound familiar.

REST uses various representations of a resource. For example, plain text, JSON, and XML are all valid representations. The most popular representations of resources are JSON and XML.

The HTTP protocol represents a resource as text in the body of a request or response. All data, even binary files, is represented in HTTP as text. The text may look like JSON, or even JavaScript, but it is always text.

Suppose the client makes a GET request to /programmers/dhh and gets the following response from the server:

{

  "data": {

    "id": "dhh",

    "name": "David Heinemeier Hansson",

    "uses": ["Ruby on Rails", "Basecamp"]

  }

}

The JSON response is a representation of the current state of the resource, not the actual resource itself. The server could represent the resource in other ways, like XML, HTML, or any other format.

The same representation concept applies when a client sends data to the server. The client doesn't send the actual resource; it just sends a representation of the resource. The server's job is to interpret this representation and respond accordingly.
HTTP request methods

How does the client tell the server what it wants? A combination of an HTTP request method and URL in the request tells the server what action it should take to fulfill the request. An HTTP request method is a method that indicates the desired action (such as deleting a resource) to be taken on a given resource. Common examples include GET, POST, PUT, PATCH, and DELETE.

One of the aims of a RESTful API is to map HTTP request methods and CRUDL actions (create, read, update, delete, and list) together in a conventional pattern. This makes it easier for other developers to understand and navigate the API.
Tip

HTTP request methods are sometimes referred to as HTTP verbs. These terms are interchangeable.

Essentially, a RESTful API asserts that URLs have names and paths that accurately reflect what they're doing with each resource. What does this look like? The following table outlines standard RESTful naming conventions for a user profile API:
Route name
	URL path
	HTTP method
	Description
Index (list)
	/profiles
	GET
	Return a list of profiles.
Create
	/profiles
	POST
	Create a new profile, assign an id, and return at least the id.
Read
	/profiles/:id
	GET
	Return the profile with the specified id, or return 404 if it doesn't exist.
Update
	/profiles/:id
	PUT
	Update an existing profile with the data in the request.
Delete
	/profiles/:id
	DELETE
	Delete the profile with the specified id. Don't return 404 if it doesn't exist.

Note: In the above URLs, the word profiles represents the resource name. It is also recommended to write URLs that are plural, lowercase, and use hyphens - to separate words. Here are some examples: /cars, /states, and /showing-requests.
Temporary state

The starter-robust-server-structure application stores all of its data in memory; the data isn't saved to a database or a file. As a result, any changes to the data will be lost when the application restarts. This is fine for now, and it's exactly what you should expect when storing data in memory. You will learn how to store the data in a database in a future module.
Express and HTTP methods

So far, every route handler that you have written has used app.use(), which matches only on the optional path parameter. But now that you know about HTTP methods, you will create API endpoints that also match on HTTP methods. An API endpoint is a location where a client can send a request to retrieve a resource that exists on the server. It includes both the URL path and the HTTP method for the given URL path.

You can use methods on the app Express application that match on both the path and the HTTP method.
Do this
Create a new paste record

Update the application to create a new paste record when the user adds the data by sending a POST request to /pastes.

To accomplish this, you need to do three things:

    Add middleware to parse incoming requests that contain JSON payloads.

    Modify the existing handler for /pastes to handle only GET requests.

    Create a new handler for POST requests to /pastes.

First, add app.use(express.json()) near the top of app.js. The express.json() function is a built-in middleware that adds a body property to the request (req.body). The req.body request will contain the parsed data—or it will return an empty object ({}) if there was no body to parse, the Content-Type wasn't matched, or an error occurred.
Tip

This middleware must come before any handlers that will make use of the JSON in the body of the request.

Now locate the existing handler for /pastes in your code:

app.use("/pastes", (req, res) => {

  res.json({ data: pastes });

});

Change it as follows:

- app.use("/pastes", (req, res) => {

+ app.get("/pastes", (req, res) => {

  res.json({ data: pastes });

});

By changing the code from app.use(...) to app.get(...), you're making it so that the handler will be called only if the HTTP method of the incoming request is GET.

Next, add the following POST handler after the GET handler:

// Variable to hold the next ID

// Because some IDs may already be used, find the largest assigned ID

let lastPasteId = pastes.reduce((maxId, paste) => Math.max(maxId, paste.id), 0);


app.post("/pastes", (req, res, next) => {

  const { data: { name, syntax, exposure, expiration, text, user_id } = {} } = req.body;

  const newPaste = {

    id: ++lastPasteId, // Increment last ID, then assign as the current ID

    name,

    syntax,

    exposure,

    expiration,

    text,

    user_id,

  };

  pastes.push(newPaste);

  res.json({ data: newPaste });

});

The line const { data: { name, syntax, exposure, expiration, text, user_id } = {} } = req.body; may look a bit strange, but it is still standard destructuring. This way, if the body doesn't contain a data property, the destructuring will still succeed because you have supplied a default value of {} for the data property.

Start the server using npm run dev, open Postman, and send a POST request to add a new paste result to /pastes.
Sending a POST request to add a new paste result to /pastes in Postman.

In Postman, under the Headers tab, don't forget to set the value of Content-Type to application/json. Otherwise, the server won't be able to properly interpret your request's JSON payload.

Now, send a GET request to /pastes to see the list of paste results.
List of paste results in Postman.

Awesome! You now have the same URL responding with two different representations depending on the HTTP method used.
HTTP response status codes

Now that you know the URL and HTTP method to use when interacting with the API, how do you know if the server fulfilled the request successfully? What happens if the developer using this API makes a mistake and misspells a property, or sets an incorrect value to a property? How do you signal to the client that the information is incorrect?

The status code of the response can help with this. An HTTP response status code is a code that's sent along with every HTTP response from the server. You can use status codes to alert the client about the success or failure of the operation.

Every HTTP response from the server contains a status code. HTTP response status codes are divided into five classes that have similar or related meanings. Understanding the classes can help you determine the appropriate status code to return with a response. The table below explains these classes.
Status code class
	Meaning
100-199
	Indicates an informational response; it is unlikely that you will need to return a response in this range.
200-299
	Indicates success; the request was received, understood, and successfully processed.
300-399
	Indicates redirection; a resource at a different URL has been substituted for the requested resource.
400-499
	Indicates a client error; there is a problem with the way that the client submitted the request.
500-599
	Indicates a server error; the request was accepted, but an error on the server prevented the request's fulfillment.

Within each of these classes, there are a variety of status codes that the server may return. Each individual status code has a specific and unique meaning.

As a developer, you don't need to memorize every status code—there are lots of them! But you should know the most common status codes and how they are used. The following table lists the most common status codes and their meanings.
Status code
	Status text
	Meaning
200
	OK
	The request is successful.
201
	CREATED
	The request resulted in a resource being successfully created.
204
	NO CONTENT
	The request is successful, and nothing is being returned in the response body. DELETE operations often return 204.
400
	BAD REQUEST
	The server cannot process the request because of bad syntax, invalid data, excessive size, or another client error.
403
	FORBIDDEN
	The client doesn't have permission to access this resource.
404
	NOT FOUND
	The resource couldn't be found at this time. It may have been deleted or doesn't exist yet.
405
	METHOD NOT ALLOWED
	The HTTP method isn't supported by the target resource.
500
	INTERNAL SERVER ERROR
	The generic answer for an unexpected failure if there is no more specific information available.
Do this
Create-paste status code

Now, update the create-paste handler to return 201 when the paste is successfully created.

app.post("/pastes", (req, res, next) => {

  const { data: { name, syntax, exposure, expiration, text } = {} } = req.body;

  const newPaste = {

    id: ++lastPasteId, // Increment last ID, then assign as the current ID

    name,

    syntax,

    exposure,

    expiration,

    text,

  };

  pastes.push(newPaste);

-  res.json({ data: newPaste });

+  res.status(201).json({ data: newPaste });

});

The code above added a chained method call to .status(201) to change the status code from 200 (the default for success) to 201.
201 status code in Postman.

Now, update the code to return 400 if the text property is missing or empty. In a future lesson, you will implement a robust error handler—but for now, you can simply return 400 with no response data.

app.post("/pastes", (req, res, next) => {

  const { data: { name, syntax, exposure, expiration, text } = {} } = req.body;

+ if (text) {

    const newPaste = {

      id: ++lastPasteId, // Increment last ID, then assign as the current ID

      name,

      syntax,

      exposure,

      expiration,

      text,

    };

    pastes.push(newPaste);

    res.status(201).json({ data: newPaste });

+  } else {

+    res.sendStatus(400);

+  }

});

In this case, there is no new paste record created, so there is no data to return. When this happens, you call sendStatus() on the response to quickly set the response HTTP status code and send its string representation as the response body.

With this code in place, posting an object with a missing or empty text property will return 400.
Returning 400 in Postman.

You will be using the same starter code for the next lesson, so make sure to keep the work that you completed in this lesson.
Completed example

A completed example from this lesson can be found here:

    Starter code: Robust server structure—02-restful-apis branch
*** 31.4 API testing with SuperTest
API testing with SuperTest
1.5 hoursEstimated completion time
Learning Objective

By the end of this lesson, you will be able to test an Express API with Jest and SuperTest.
Overview

In the previous lesson, you learned how to create a basic Express API. APIs are a vital part of many applications, and it's important to ensure that future updates to an API don't accidentally change the behavior of the API. But how do you ensure that your API continues to work as expected as the codebase grows? To do so, you can write automated tests for your Express API. You can build automated tests for an Express API using the Jest testing framework and an HTTP assertion library called SuperTest.
Starter code

For this lesson, it is assumed that you already have the following repository running on your local machine.

    GitHub: Starter robust server structure paste

This lesson builds upon the work of the earlier lessons. If you haven't successfully completed the work from the earlier lesson, go back and do that now. If you are having trouble, reach out for assistance.
Jest and SuperTest

Jest is a JavaScript testing framework that includes both an assertion library and a test runner. SuperTest allows you to programmatically make HTTP requests (such as GET, PUT, POST, and DELETE) to your Express API. You will use Jest to run the backend test suite and to check that the API returns correct results.
Testing the /pastes API endpoints

Previously, you created several Express API endpoints to interact with the pastes data, which is stored in a JavaScript file. You will now add automated tests for a subset of these endpoints, as follows:
Route name
	URL path
	HTTP method
	Description
Index (list)
	/pastes
	GET
	Return a 200 status code and a list of pastes.
Create
	/pastes
	POST
	Create a new paste, assign an id, and return a 201 status code and the newly created paste.
Do this
Install Jest and SuperTest

First, install Jest and SuperTest as development dependencies, as follows:

npm install --save-dev jest supertest

Create a test file

Now, create a folder called tests/ and create a file called app.test.js inside of it. Test files should have the suffix .test.js. By default, Jest will be checking filenames for that suffix when looking for test files to execute.

Then, set up app.test.js with the following code:

const request = require("supertest");

const pastes = require("../src/data/pastes-data");

const app = require("../src/app");


describe("path /pastes", () => {

  // Add tests here

});

The first three lines load the SuperTest library, pastes data, and the Express server, respectively, into the file. The describe() block groups together all the tests related to the /pastes path. You will add these tests later on in the lesson.
Update package.json

Add the following test script to your package.json so that it executes the jest command whenever you run npm test:

{

  "scripts": {

    "test": "jest"

  }

}

Run npm test

If you run the npm test command now, the test output would show that the test suite has failed to run, because you haven't written any tests.

 FAIL  tests/app.test.js

  ● Test suite failed to run


    Your test suite must contain at least one test.

Jest refresher

Keep in mind that Jest includes the following functions to help you write tests:

    describe(), which groups together a set of related tests

    test() (or it()), which describes an individual test case and is typically nested inside of the describe() function

    The expect object, which provides access to matchers (like toBe() and toEqual()) that allow you to check whether some part of your code has produced an expected outcome

Setup and teardown

Often tests involve some setup and teardown work that needs to be performed before and after tests run, respectively. For example, setup might involve initializing variables and opening file or database connections. Teardown might involve resetting variables, closing file or database connections, or even resetting the test database.

Suppose you have several tests that interact with a database of students, a method initializeStudentsDatabase() that must be called before each of these tests, and a method clearStudentsDatabase() that must be called after each of these tests. You can do this with the beforeEach() and afterEach() helper methods, as follows:

beforeEach(() => {

  initializeStudentsDatabase();

});


afterEach(() => {

  clearStudentsDatabase();

});


it("students database has John", () => {

  expect(isStudent("John")).toBeTruthy();

});


it("students database has Jane", () => {

  expect(isStudent("Jane")).toBeTruthy();

});

Keep in mind that the starter-robust-server-structure-paste application stores all of its data in memory; the data isn't saved to a database or a file. The pastes data is stored in an array defined in a JavaScript file. Before running each test, you will want to remove any existing entries in the pastes array.

Resetting the pastes array to an empty array before running each test helps ensure the integrity of the test. For example, if you're writing a test to check whether the API is allowing the creation of a paste record in the database, then you'd want to be sure that there was no paste record in the database before the test was run. That way, you can be sure that any record stored in the array was created as a result of your API call.
Do this
Add a beforeEach() method

Add a beforeEach() method in the describe block to reset the pastes data prior to running each test in the describe block, as follows:

describe("path /pastes", () => {

  beforeEach(() => {

    pastes.splice(0, pastes.length); // Clears out the pastes data

  });

});

Note that because the beforeEach() method is created in the describe block, it will only affect any tests written inside the describe block.
Testing GET /pastes

Keep in mind that the GET /pastes endpoint should return a 200 status code and a list of pastes. Now, you will write a test for the following list-paste handler from src/app.js:

app.get("/pastes", (req, res) => {

  res.json({ data: pastes });

});

Do this
Create a test for GET /pastes

In tests/app.test.js, add a describe block for the GET method that contains an individual test as follows:

describe("path /pastes", () => {

  beforeEach(() => {

    pastes.splice(0, pastes.length);

  });


  describe("GET method", () => {

    it("returns an array of pastes", async () => {

      const expected = [

        {

          id: 1,

          user_id: 1,

          name: "Hello",

          syntax: "None",

          expiration: 10,

          exposure: "private",

          text: "Hello World!"

        },

        {

          id: 2,

          user_id: 1,

          name: "Hello World in Python",

          syntax: "Python",

          expiration: 24,

          exposure: "public",

          text: "print(Hello World!)"

        },

        {

          id: 3,

          user_id: 2,

          name: "String Reverse in JavaScript",

          syntax: "Javascript",

          expiration: 24,

          exposure: "public",

          text: "const stringReverse = str => str.split('').reverse().join('');"

        }

      ];


      pastes.push(...expected);


      const response = await request(app).get("/pastes");


      expect(response.status).toBe(200);

      expect(response.body.data).toEqual(expected);

    });

  });

});

Keep in mind that the callback function passed as a second argument to the test() method describes the steps for testing the API endpoint. First, an expected array is defined which contains a list of paste objects. Then, copies of the expected paste objects are added to the pastes array. Next, the test runs await request(app).get("/pastes") to send an API request to the GET /pastes endpoint, and the response of the asynchronous call is stored in a response variable. Finally, the test uses the expect() method in conjunction with the toBe() and toEqual() matchers to ensure that the response status code and response body, respectively, contain the expected results.
Run the test

If you run npm test, you will see the test passing, as follows:

PASS  tests/app.test.js

path /pastes

  GET method

    ✓ returns an array of pastes (20 ms)

If you change one of the assertions in the test (for example, changing the expected response status code from 200 to 201), you should see a failing test instead because the assertion won't be passing anymore. This step will give you confidence that your test is working properly. Make sure to revert any changes that you made so that your test is passing again before you proceed to the next step.
Run tests in watch mode

It's quite a hassle to have to manually restart the test every time that you make a change to your test code. So, run the tests in watch mode instead.

In your package.json, add the following script:

"scripts": {

    "start": "node src/server.js",

    "dev": "nodemon src/server.js",

    "test": "jest",

+    "test:watch": "jest --watch"

  },

Then run npm run test:watch. You'll be presented with the following options to refresh your tests automatically:

Watch Usage

 › Press a to run all tests.

 › Press f to run only failed tests.

 › Press p to filter by a filename regex pattern.

 › Press t to filter by a test name regex pattern.

 › Press q to quit watch mode.

 › Press Enter to trigger a test run.

You will need to keep this terminal open to run the tests in watch mode. If you need to run other terminal commands in development, simply open a new terminal window.
Test POST /pastes

Keep in mind that the POST /pastes endpoint should create a new paste, assign an id, and return a 201 status code and the newly created paste. Now, you will write a test for the following create-paste handler from src/app.js:

app.post("/pastes", (req, res, next) => {

  const { data: { name, syntax, exposure, expiration, text } = {} } = req.body;

  if (text) {

    const newPaste = {

      id: ++lastPasteId, // Increment last ID, then assign as the current ID

      name,

      syntax,

      exposure,

      expiration,

      text,

    };

    pastes.push(newPaste);

    res.status(201).json({ data: newPaste });

  } else {

    res.sendStatus(400);

  }

});

Do this
Create tests for POST /pastes

Under the GET method block that you just created, add a sibling describe block to group together the tests for the create-paste handler, as follows:

describe("POST method", () => {

  it("creates a new paste and assigns id", async () => {

    const newPaste = {

      name: "String Reverse in JavaScript",

      syntax: "Javascript",

      expiration: 24,

      exposure: "public",

      text: "const stringReverse = str => str.split('').reverse().join('');"

    };

    const response = await request(app)

      .post("/pastes")

      .set("Accept", "application/json")

      .send({ data: newPaste });


    expect(response.status).toBe(201);

    expect(response.body.data).toEqual({

      id: 5,

      ...newPaste,

    });

  });


  it("returns 400 if result is missing", async () => {

    const response = await request(app)

      .post("/pastes")

      .set("Accept", "application/json")

      .send({ data: { message: "returns 400 if result is missing" } });


    expect(response.status).toBe(400);

  });


  it("returns 400 if result is empty", async () => {

    const response = await request(app)

      .post("/pastes")

      .set("Accept", "application/json")

      .send({ data: { result: "" } });


    expect(response.status).toBe(400);

  });

});

Instead of a single test, you're now testing three different aspects of the create-paste handler:

    The first test ensures that the API endpoint can successfully create a new paste record. The post() method is used to make a POST request to the server. Because the request payload includes JSON data, you'd have to call set() to set the Accept header of the request to "application/json". Finally, the send() method accepts an object as an argument that contains the data that you'd like to send to the server.

    The second and third tests ensure that the API endpoint returns a 400 status code if the request contains incorrectly formatted data.

Run the test

If you run npm test, you should see the test passing, as follows:

PASS  tests/app.test.js

path /pastes

  GET method

    ✓ returns an array of pastes (7 ms)

  POST method

    ✓ creates a new paste and assigns id (11 ms)

    ✓ returns 400 if result is missing (4 ms)

    ✓ returns 400 if result is empty (3 ms)

Now you know how to test an Express API using Jest and SuperTest. Take a moment to think about any additional tests that you might add to test the API. For example, you could add a test to ensure that the GET /pastes/:pasteId endpoint returns the correct paste record or an appropriate error message if the pasteId doesn't exist in the records. You could also write a test to check that the API returns the correct error message for a nonexistent URL path (such as /asdfghjkl).
Complete example

A completed example from this lesson can be found here:

    GitHub: Starter robust server structure—api-testing-with-supertest-complete branch
*** 31.5 Major error types and handling
Major error types and handling
1.5 hoursEstimated completion time
Learning Objective

By the end of this lesson, you will be able to implement a central error handler that returns status codes and error information to the client.
Overview

In this lesson, you'll learn how to implement a centralized error-handling approach; this is especially important when you begin to build bigger and more complex APIs.

In the previous lesson, you followed RESTful design principles to create your API. Another key feature of a robust API is its error-handling approach. When building RESTful APIs using Express, or any other framework or library, validation checks are always necessary as a best practice. And it's always important to return an error response to the client, so that the client can stay informed on why their request isn't working.

However, as the API grows in size and complexity, handling every possible error and returning a response for every validation check can quickly become tedious; it can make it difficult to quickly grasp what the code is doing. Having a centralized error-handling approach can simplify the code.
Starter code

For this lesson, it is assumed that you already have the Starter robust server structure repository running on your local machine.

This lesson builds upon the work of the earlier lessons. If you haven't successfully completed the work from the earlier lessons, go back and do that now. If you are having trouble, reach out for assistance.
Validation

Consider the example below:

let lastPasteId = pastes.reduce((maxId, paste) => Math.max(maxId, paste.id), 0);


app.post("/pastes", (req, res, next) => {

  const { data: { name, syntax, exposure, expiration, text, user_id } = {} } = req.body;

  if(text){

      const newPaste = {

        id: ++lastPasteId, // Increment last ID, then assign as the current ID

        name,

        syntax,

        exposure,

        expiration,

        text,

        user_id

      };

      pastes.push(newPaste);

      res.status(201).json({ data: newPaste });

  } else {

      res.sendStatus(400);

  }

});

Looking at the code snippet above, you can see that the route handler is starting to look a little complicated. It's responsible for returning a response to the client, and it's also doing some validation checks (returning a 400 status code if the result variable is falsy). That means that it violates the single-responsibility principle.

To ensure that each route handler has a single responsibility, you can move all validation code into middleware functions. By doing all of the validation in the middleware layer, the route handler will never have to directly make any check related to the request. All these checks will be done in the middleware.

Take a look at the existing error handler:

app.use((error, req, res, next) => {

  console.error(error);

  res.send(error);

});

This error handler will catch every error, but it doesn't respond with JSON data like the route handlers. That makes error handling more difficult for developers using the API. So to demonstrate the existing issues with the error handler, add a validation middleware function to the create-paste route.
Do this
Return validation error

Update the POST handler for /pastes to move the validation code into a middleware function that returns information about the validation failures, as follows:

// New middleware function to validate the request body

function bodyHasTextProperty(req, res, next) {

  const { data: { text } = {} } = req.body;

  if (text) {

    return next(); // Call `next()` without an error message if the result exists

  }

  next("A 'text' property is required.");

}


let lastPasteId = pastes.reduce((maxId, paste) => Math.max(maxId, paste.id), 0);


app.post(

  "/pastes",

  bodyHasTextProperty, // Add validation middleware function

  (req, res) => {

    // Route handler no longer has validation code.

    const { data: { name, syntax, exposure, expiration, text, user_id } = {} } = req.body;

    const newPaste = {

      id: ++lastPasteId, // Increment last id then assign as the current ID

      name,

      syntax,

      exposure,

      expiration,

      text,

      user_id,

    };

    pastes.push(newPaste);

    res.status(201).json({ data: newPaste });

  }

);

Now, if validation fails within the validation middleware (bodyHasTextProperty()), then next() is called with an error message. This will cause the error handler to be called. The value passed into next() will be passed to the error handler as the first argument. Here, you are only doing validation for the text property. In the next lesson, you will complete validation for all the properties.

Now send a POST request to add an empty object to /pastes.
Adding an empty object to /pastes and getting the status code 200 in Postman.

The error message is returned, but the status code is 200. This is because the status code is 200 by default. You need to tell the error handler the specific status code to return. Here, because the request is malformed, it is appropriate to return a 400 status code.

To do this, you need to change the validation middleware and the error handler.

function bodyHasTextProperty(req, res, next) {

  const { data: { text } = {} } = req.body;

  if (text) {

    return next();

  }

  next({

    status: 400,

    message: "A 'text' property is required.",

  });

}

Now, the validation middleware is calling next() with an object that has two properties: status and message.

Note: The message property matches the JavaScript Error object for consistency. You are using the status property here because the default error handler for Express will set res.statusCode to the value stored in error.status.

When any of the route handlers or middlewares call next() with an error object, that error object is passed along to the centralized error handler that exists at the bottom of the file. Moreover, the error object can be accessed via the first argument of the error handler. Update the error handler as follows:

app.use((error, req, res, next) => {

  console.error(error);

  const { status = 500, message = "Something went wrong!" } = error;

  res.status(status).json({ error: message });

});

Now the error handler will return a 500 Internal Server Error by default, but middleware functions (such as bodyHasTextProperty()) can set the status code and error messages if necessary.

With these changes, the handler can correctly report both custom validation error and any JavaScript Error.

Then, to make sure that this route is working properly with these changes, make another POST request to the /pastes endpoint:
Making another POST request to the /pastes endpoint.

Developers can now determine if their request to the API was successful by assessing the status code. Alternatively, developers can use the JSON returned from the API; if a request is successful, the JSON will have a data property. Otherwise, the JSON will have an error property.

Now that this pattern of error handling is working properly, update the existing next() calls.
Do this
Update existing calls to next()

Now, go through your code and change next(`Paste id not found: ${pasteId}`); to next({ status: 404, message: `Paste id not found: ${pasteId}` });. Here, you're updating the next() call to pass an object with status and message as arguments.

Now, test out your code in Postman by making POST calls to /pastes/:pasteId with nonexistent :pasteId values.

You will be using the same starter code for the next lesson, so make sure to keep the work that you completed in this lesson.
Completed example

A completed example from this lesson can be found here:

    Starter robust server structure—03-error-types branch
*** 31.6 Organizing Express code
Organizing Express code
1.5 hoursEstimated completion time
Learning Objective

By the end of this lesson, you will be able to organize code into small files grouped by resource. You'll also be able to use the express.Router class to create modular, mountable route handlers.
Overview

In this lesson, you will reorganize your code to make it easier to understand, maintain, and modify.
Key Terms

Group-by-resource structure
    A file organization structure in which any code that handles requests to a resource is stored in a folder with the same name as the resource, regardless of the URL to that resource
Controller file
    A file that defines and exports the route handler functions and is responsible for managing the state of a single resource in an API
Express router
    A modular middleware and routing system that can be attached to an Express app

Besides following RESTful design principles and having centralized error handling, a robust API is built on top of well-organized and well-structured code. Like all software projects, Express APIs tend to get larger and more complex over time. The more files that you have in the project, the more important that the file organization becomes. The files need to be organized in a way that makes it easy to find and modify existing code, and to add new code in a location consistent with the existing code.
Starter code

For this lesson, it is assumed that you already have the Starter robust server structure GitHub repository running on your local machine.

This lesson builds upon the work of the earlier lessons. If you haven't successfully completed the work from the earlier lessons, go back and do that now. If you are having trouble, reach out for assistance.
What's the problem?

The application that you have been modifying for this module has a file named app.js where you have defined all of the routes and handlers. Defining all of the route handers as anonymous functions inside of app.js will get overwhelming, even for small applications. For example, your app.js file likely has more than 70 lines of code.
Group by resource

For this and the remaining lessons in this module, you will organize your Express code using the group-by-resource structure. Grouping by resource, as you learned earlier in this program, means that any code that handles requests to a resource (such as /pastes) is stored in a folder with the same name as the resource, regardless of the URL to that resource. This is a very common project structure for Express.

Here's an example of what your directory structure will look like after you organize the code by resource.

src

    ├── app.js

    ├── data

    │   ├── pastes-data.js

    ├── pastes

    │   ├── pastes.controller.js

    │   └── pastes.router.js

    └── server.js

You will learn about the controller.js and router.js files soon.

You may also be wondering why the data file (pastes-data.js) isn't in the resource folder. Although there is nothing wrong with storing the data files in their corresponding resource folders, the convention is that only the router.js file can be required from a resource folder. The data files are stored in a data folder so the /pastes controller files can require the data without violating the convention. That said, data is normally stored in a database or other external repository, so this isn't an issue that you will encounter frequently.

If a function can be called from any file, the project may quickly look like spaghetti code. Grouping by resource allows you to clarify your architecture; any file in the folder can import functions from any other file in the same folder, but may not import functions from files in other folders. There are, of course, exceptions for folders that are understood to only contain shared code (such as a utilities folder).
Controller

You will be creating several controller files. A controller file defines and exports the route-handler functions. This file's single responsibility in an API is to manage the state of a single resource (for example, to create, read, update, delete, or list the requested data).

So far, your route-handler functions have been written as anonymous functions defined inline with calls to app.use(), app.get(), or app.post(). Now you will move these functions to named functions exported from the controller file.
Tip

Don't export validation middleware functions from the controller. No other code is responsible for managing the state of this controller's resource, so there's no need to export the middleware functions.

You will reorganize your code by making many small changes and making sure the code still works after each change, rather than making all of the changes at once and then checking to make sure that your API still works. If you make all the changes at once without checking your code incrementally, you won't know if you made a mistake on the first change or the fiftieth change.

You will reorganize the code starting with the list of pastes returned by a GET request to /pastes.
Do this
Create a controller for the /pastes resource

You should move the anonymous route-handler functions for the /pastes route into a new controller file. First, create the controller file at src/pastes/pastes.controller.js.

In that file, add the following code:

const pastes = require("../data/pastes-data");


function list(req, res) {

  res.json({ data: pastes });

}


module.exports = {

  list,

};

Now that you have the list route handler defined in the controller, you can create a router to connect the GET /pastes endpoint to the router-handler function (which is list()).
Router

The Express router is a modular middleware and routing system that can be attached to an Express app, which is why it is often referred to as a mini-app.

You only need to specify the starting path, and the router will handle the rest for you.

For example, after creating the pastes router, you can attach it to the app just like a route handler (like app.use('/pastes' or pastesRouter)), except it's only attached once and it will handle any path defined in the router.

The router file defines and exports an instance of Express router. The router file is only responsible for connecting a path (/) with the route handler for that path (pastesController.list()).

You might expect to define the routes starting with /pastes, as you did in app.js. However, recall how Express router was defined above: it's a modular middleware and routing system that can be attached to an Express app. The modularity of the router means that it can be "attached" to the Express app using any starting point. As a result, the paths in the router are always defined independently of the starting point.

A starting point is any part of the path defined when the router is attached to the app. For example, if the router is attached as app.use('/pastes', pastesRouter), then the starting point is /pastes. The full URL to any handler in a router will be the starting point followed by the path defined in the router.

Being able to "attach" the router to the app using any starting point, or move it to a different starting point without changing the router, is a key benefit of using the Express router.

So far, you have been using app to specify the handler for each path. But you have also had app parse the body of the request (app.use(express.json());), handle routes that aren't found, and also handle errors. The app has multiple responsibilities, so you will extract the router functionality out of app and into router files.

The router file is the only file that should be used outside of its folder. Consider the controller file, and any other files you create in the /pastes folder, to be private and not for use outside of the folder.
Do this
Create a router for the /pastes resource

Now create the router file at src/pastes/pastes.router.js.

In that file, add the following code:

const router = require("express").Router();

const controller = require("./pastes.controller");


router.route("/").get(controller.list);


module.exports = router;

The above code does the following:

    const router = require("express").Router(); creates a new instance of Express router.

    const controller = require("./pastes.controller"); imports the /pastes controller that you created earlier.

    router.route("/") using route() allows you to write the path once, and then chain multiple route handlers to that path. Right now you have only get(), but later on, you will add post() and all() to the method chain.

    get(controller.list) uses the list() route handler defined in the controller for GET requests to /.

    module.exports = router; exports the router for use in app.js.

Now that you have a router defined, attach it to the app.

First, import the router into app.js:

const pastesRouter = require("./pastes/pastes.router");

Then find the following code in app.js:

app.get("/pastes", (req, res) => {

  res.json({ data: pastes });

});

Replace it with the following:

app.use("/pastes", pastesRouter); // Note: app.use

Now, make sure that it's working properly.
Pastes list in Postman.

If you're up for a challenge, take a break here and try, on your own, to reorganize the /pastes route handlers into controller and router files. When you are ready, read on for the detailed instructions.
Reorganize the create-pastes handler

Now, you will reorganize the create-paste handler (POST /pastes). You will move the POST /pastes route handler and validation middleware out of app.js and put it into pastes.controller.js.

First, find and remove the following code from app.js:

function bodyHasTextProperty(req, res, next) {

  const { data: { text } = {} } = req.body;

  if (text) {

    return next();

  }

  next({

    status: 400,

    message: "A 'text' property is required.",

  });

}


let lastPasteId = pastes.reduce((maxId, paste) => Math.max(maxId, paste), 0)


app.post("/pastes", bodyHasTextProperty, (req, res, next) => {

  const { data: { name, syntax, exposure, expiration, text, user_id } = {} } = req.body;

  const newPaste = {

    id: ++lastPasteId, // Increment last ID, then assign as the current ID

    name,

    syntax,

    exposure,

    expiration,

    text,

    user_id,

  };

  pastes.push(newPaste);

  res.status(201).json({ data: newPaste });

});

Then, add the following code to pastes.controller.js:

let lastPasteId = pastes.reduce((maxId, paste) => Math.max(maxId, paste.id), 0)


function bodyHasTextProperty(req, res, next) {

  const { data: { text } = {} } = req.body;

  if (text) {

    return next();

  }

  next({

    status: 400,

    message: "A 'text' property is required.",

  });

}


function create(req, res) {

  const { data: { name, syntax, exposure, expiration, text, user_id } = {} } = req.body;

  const newPaste = {

    id: ++lastPasteId, // Increment last id then assign as the current ID

    name,

    syntax,

    exposure,

    expiration,

    text,

    user_id,

  };

  pastes.push(newPaste);

  res.status(201).json({ data: newPaste });

}


module.exports = {

  create: [bodyHasTextProperty, create],

  list,

};

Note that the create export includes an array of the middleware function bodyHasTextProperty() and the create route handler. The router doesn't need to know about the middleware. Using the array syntax allows you to export the middleware and route handlers together, and the router code will look the same as if only the route-handler function is exported.

Next, modify your router code in pastes.router.js as follows:

- router.route("/").get(controller.list);

+ router.route("/").get(controller.list).post(controller.create);

Then, make sure that you are still able to create paste records:
Confirming that paste records can still be created in Postman.

Finally, make sure that the middleware function still reports an error if the text property is missing.
Confirming that the middleware function reports an error in Postman.
Enhance create-paste validation

Now that you know that the create-paste handler is working in the reorganized code, you will enhance the create-paste validation to make sure that the other properties are also in the request.

Remove the following code to pastes.controller.js:

function bodyHasTextProperty(req, res, next) {

  const { data: { text } = {} } = req.body;

  if (text) {

    return next();

  }

  next({

    status: 400,

    message: "A 'text' property is required.",

  });

}

Then add the following code:

function bodyDataHas(propertyName) {

  return function (req, res, next) {

    const { data = {} } = req.body;

    if (data[propertyName]) {

      return next();

    }

    next({ status: 400, message: `Must include a ${propertyName}` });

  };

}

As an alternative to writing a function for validating each property, the bodyDataHas() function allows you to validate any given parameter. Now, update module.exports to include the new validation middleware:

module.exports = {

  create: [

      bodyDataHas("name"),

      bodyDataHas("syntax"),

      bodyDataHas("exposure"),

      bodyDataHas("expiration"),

      bodyDataHas("text"),

      bodyDataHas("user_id"),

      create

  ],

  list,

};

You can add additional validation to test that the properties have valid values. Add the following code to pastes.controller.js:

function exposurePropertyIsValid(req, res, next) {

  const { data: { exposure } = {} } = req.body;

  const validExposure = ["private", "public"];

  if (validExposure.includes(exposure)) {

    return next();

  }

  next({

    status: 400,

    message: `Value of the 'exposure' property must be one of ${validExposure}. Received: ${exposure}`,

  });

}


function syntaxPropertyIsValid(req, res, next) {

  const { data: { syntax } = {} } = req.body;

  const validSyntax = ["None", "Javascript", "Python", "Ruby", "Perl", "C", "Scheme"];

  if (validSyntax.includes(syntax)) {

    return next();

  }

  next({

    status: 400,

    message: `Value of the 'syntax' property must be one of ${validSyntax}. Received: ${syntax}`,

  });

}


function expirationIsValidNumber(req, res, next){

  const { data: { expiration }  = {} } = req.body;

  if (expiration <= 0 || !Number.isInteger(expiration)){

      return next({

          status: 400,

          message: `Expiration requires a valid number`

      });

  }

  next();

}

Then change the export to include these new middleware functions:

module.exports = {

  create: [

      bodyDataHas("name"),

      bodyDataHas("syntax"),

      bodyDataHas("exposure"),

      bodyDataHas("expiration"),

      bodyDataHas("text"),

      bodyDataHas("user_id"),

      exposurePropertyIsValid,

      syntaxPropertyIsValid,

      expirationIsValidNumber,

      create

  ],

  list,

};

Then test that the validation works:
Testing validation in Postman.

As you can see, to add this validation for the create-pastes route, you only needed to update the controller. The router file and app.js didn't have to be updated.
Do this
Reorganize the read-paste handler

Now you will reorganize the read-paste handler (GET /pastes/:pasteId). First, move the anonymous function and validation middleware out of app.js into paste.controller.js.

Find and remove the following code from app.js:

app.get("/pastes/:pasteId", (req, res, next) => {

  const { pasteId } = req.params;

  const foundPaste = pastes.find((paste) => paste.id === Number(pasteId));


  if (foundPaste) {

    res.json({ data: foundPaste });

  } else {

    return next({

      status: 404,

      message: `Paste id not found: ${pasteId}`,

    });

  }

});

Then add the following code to pastes.controller.js:

function pasteExists(req, res, next) {

  const { pasteId } = req.params;

  const foundPaste = pastes.find((paste) => paste.id === Number(pasteId));

  if (foundPaste) {

    return next();

  }

  next({

    status: 404,

    message: `Paste id not found: ${pasteId}`,

  });

}


function read(req, res) {

  const { pasteId } = req.params;

  const foundPaste = pastes.find((paste) => paste.id === Number(pasteId));

  res.json({ data: foundPaste });

}

Next, update the export statement in pastes.controller.js:

module.exports = {

  create: [

      bodyDataHas("name"),

      bodyDataHas("syntax"),

      bodyDataHas("exposure"),

      bodyDataHas("expiration"),

      bodyDataHas("text"),

      bodyDataHas("user_id"),

      exposurePropertyIsValid,

      syntaxPropertyIsValid,

      expirationIsValidNumber,

      create

  ],

  list,

+  read: [pasteExists, read],

};

Then, update pastes.router.js by adding a new route:

router.route("/:pasteId").get(controller.read);

Then, make sure that you are still able to read paste records:
Confirming the GET request works properly on Postman.

Finally, make sure that an error is reported if there is no paste record with the specified ID.
Error report showing up on Postman when the paste ID isn't found.

Now that you can create new paste records and read the new paste once it is created, you will add the ability to update an existing paste.
Do this
Implement update-paste handler

In this section, you will add the update-paste handler. This handler will return a 200 status code when the paste is successfully updated, a 404 status code if a paste with the specified id doesn't exist, and a 400 status code if a property has an invalid value.

First, add the following update-handler code to pastes.controller.js:

function update(req, res) {

  const { pasteId } = req.params;

  const foundPaste = pastes.find((paste) => paste.id === Number(pasteId));

  const { data: { name, syntax, expiration, exposure, text } = {} } = req.body;


  // Update the paste

  foundPaste.name = name;

  foundPaste.syntax = syntax;

  foundPaste.expiration = expiration;

  foundPaste.exposure = exposure;

  foundPaste.text = text;


  res.json({ data: foundPaste });

}

Next, update the export statement in pastes.controller.js:

module.exports = {

  create: [

      bodyDataHas("name"),

      bodyDataHas("syntax"),

      bodyDataHas("exposure"),

      bodyDataHas("expiration"),

      bodyDataHas("text"),

      bodyDataHas("user_id"),

      exposurePropertyIsValid,

      syntaxPropertyIsValid,

      expirationIsValidNumber,

      create

  ],

  list,

  read: [pasteExists, read],

+  update: [

+      pasteExists,

+      bodyDataHas("name"),

+      bodyDataHas("syntax"),

+      bodyDataHas("exposure"),

+      bodyDataHas("expiration"),

+      bodyDataHas("text"),

+      exposurePropertyIsValid,

+      syntaxPropertyIsValid,

+      expirationIsValidNumber,

+      update

+  ],

};

Notice that you are reusing previously defined middleware functions to do the necessary validation before the update handler is called.

Then, update the /:pasteId route in pastes.router.js to include the put() method:

- router.route("/:pasteId").get(controller.read);

+ router.route("/:pasteId").get(controller.read).put(controller.update);

Then, make sure that you are able to update paste records:
Updating paste record with PUT request in Postman.

Finally, make sure that an error is reported if there is no paste record with the specified ID.

Next, you will add the ability to delete an existing paste record.
Do this
Implement delete-paste handler

Now, you will add the delete-paste handler. Generally, a successful response to an HTTP DELETE method can be one of the following:

    200 OK if the response includes a body describing the status.

    202 Accepted if the action hasn't been completed yet.

    204 No Content if the action has been completed but the response doesn't include a body.

The most common implementation of DELETE returns 204 with no response body, so that is what you will do.

First, add the following to pastes.controller.js:

function destroy(req, res) {

  const { pasteId } = req.params;

  const index = pastes.findIndex((paste) => paste.id === Number(pasteId));

  // `splice()` returns an array of the deleted elements, even if it is one element

  const deletedPastes = pastes.splice(index, 1);

  res.sendStatus(204);

}

Note that the delete-paste handler cannot be named delete because delete is a reserved word in JavaScript.

Next, update the export statement in pastes.controller.js:

module.exports = {

  create: [

      bodyDataHas("name"),

      bodyDataHas("syntax"),

      bodyDataHas("exposure"),

      bodyDataHas("expiration"),

      bodyDataHas("text"),

      bodyDataHas("user_id"),

      exposurePropertyIsValid,

      syntaxPropertyIsValid,

      expirationIsValidNumber,

      create

  ],

  list,

  read: [pasteExists, read],

  update: [

      pasteExists,

      bodyDataHas("name"),

      bodyDataHas("syntax"),

      bodyDataHas("exposure"),

      bodyDataHas("expiration"),

      bodyDataHas("text"),

      exposurePropertyIsValid,

      syntaxPropertyIsValid,

      expirationIsValidNumber,

      update

  ],

+  delete: [pasteExists, destroy],

};

Then, update pastes.router.js by adding a new route handler:

router

  .route("/:pasteId")

  .get(controller.read)

  .put(controller.update)

+  .delete(controller.delete);

Then, make sure that you are able to delete paste records:
DELETE request on Postman.

Finally, make sure that 404 is returned if there is no paste record with the specified ID.
404 error returned after GET request when there's no paste record with the requested ID.

Congratulations! You have reorganized all of the /pastes routes: create, read, update, delete, and list.

You will be using the same starter code for the next lesson, so make sure to keep the work that you completed in this lesson.
Completed example

A completed example from this lesson can be found here:

    Starter code: Robust server structure—04-organizing-express-code branch
*** 31.7 Advanced tips
Advanced tips
1.5 hoursEstimated completion time
Learning Objective

By the end of this lesson, you will be able to pass data on the response. You'll also be able to use the all() method of the router.
Overview

As an Express API gets larger and more complex, you may notice some code that violates the single-responsibility principle. You may also notice code that is duplicated in several places. In this lesson, you will learn some advanced techniques to improve your Express APIs.
Starter code

For this lesson, it is assumed that you already have the Starter robust server structure GitHub repository running on your local machine.

This lesson builds upon the work of the earlier lessons. If you haven't successfully completed the work from the earlier lessons, go back and do that now. If you are having trouble, reach out for assistance.
Passing data on the response

As you look at the read(), update(), and pasteExists() controller functions, you will notice there is some duplicated code: each function looks up the paste record by ID. How can you reorganize this code so the lookup is done once in a middleware function and, if found, the paste record is passed to the route handler?

There is a special locals property on the response that can be used to share variables scoped to the request. The locals property is an object where you can add properties that will be available only during that request-response cycle. Once the request-response cycle ends (meaning that the response has been sent to the client), the locals object is deleted.

Now, you will change the pasteExists() function to store the paste record as res.locals.paste, if found, or call next() with a 404 error if it isn't found.

Note: There is also a convention of setting a new property on the request object to share data between middleware functions (that is, req.paste = foundPaste rather than res.locals.paste = foundPaste). Both approaches work well, and both approaches can be found in the official Express documentation.
Do this
Store found paste on response

Now you will update pasteExists() to store the paste as res.locals.paste, if found.

First, in paste.controller.js, update pasteExists() to use res.locals.paste:

function pasteExists(req, res, next) {

  const { pasteId } = req.params;

  const foundPaste = pastes.find(paste => paste.id === Number(pasteId));

  if (foundPaste) {

    res.locals.paste = foundPaste;

    return next();

  }

  next({

    status: 404,

    message: `Paste id not found: ${pasteId}`,

  });

};

Then, change the read() and update() methods to also use res.locals.paste:

function update(req, res) {

  const paste = res.locals.paste;

  const { data: { name, syntax, expiration, exposure, text } = {} } = req.body;


  // Update the paste

  paste.name = name;

  paste.syntax = syntax;

  paste.expiration = expiration;

  paste.exposure = exposure;

  paste.text = text;


  res.json({ data: paste });


}


function read(req, res, next) {

  res.json({ data: res.locals.paste });

};

Finally, make sure that you are still able to read and update paste records.
The /users controller and router

Now you will add a /users route which will return a list of users associated with the pastes.
Do this

Create a users-data.js file in the data folder and paste the following code:

module.exports = [

  {

    id: 1,

    username: "shakira",

  },

  {

    id: 2,

    username: "coltrane",

  },

  {

    id: 3,

    username: "beethoven",

  },

  {

    id: 4,

    username: "baesuzy",

  }

];

Create the /users controller

Create the controller file in src/users/users.controller.js.

In the controller file, add the following code:

const users = require("../data/users-data");


function list(req, res) {

  res.json({ data: users });

}


function userExists(req, res, next) {

  const { userId } = req.params;

  const foundUser = users.find(user => user.id === Number(userId));

  if (foundUser) {

    res.locals.user = foundUser;

    return next();

  }

  next({

    status: 404,

    message: `User id not found: ${userId}`,

  });

};


function read(req, res, next) {

  res.json({ data: res.locals.user });

};


module.exports = {

  list,

  read: [userExists, read]

};

Now that you have all of the route handlers defined, it's time to create the router.
Create /users router

Now you will create the /users router. The routes will start with /, not /users, just like the routes in the /pastes router. Later, you will attach this router to the app using /users as the starting point.

This is the complete code for the /users router at src/users/users.router.js:

const router = require("express").Router();


const controller = require("./users.controller");


router.route("/:userId").get(controller.read);


router.route("/").get(controller.list);


module.exports = router;

Now, in app.js, attach the /users router to the app using /users as the starting point.

const usersRouter = require("./users/users.router");

const pastesRouter = require("./pastes/pastes.router");


app.use("/users", usersRouter);

app.use("/pastes", pastesRouter);

Finally, test it out and make sure that the /users routes are working correctly.
405 Method Not Allowed

You can help developers better understand the API by using status codes and the errors property on the response object to report errors. This is how you let them know what went wrong. And more importantly, it's where you indicate what to change to correct the error, if possible.

For example, explore what happens if you send a PUT request to /pastes without an ID.
A PUT request generating an error in Postman.

The reason that the API returns an error is because there is no route handler for the PUT method on the /pastes route. Ideally, this response would have a 405 status code and a message explaining that PUT isn't allowed. Rather than having to add a handler for each method, you can add a handler for all methods.

The all() method on the route lets you define a handler for all HTTP methods. If you place all() at the beginning of the route, the handler can do some work, like making sure that the user is logged in, and then call next() to pass the request to the next handler. But if you place all() at the end of the route, the handler will be called only if no earlier handler completes the request, or the earlier handler calls next().

With this new information, you will implement an all() handler that returns a 405 status code. Because this isn't a resource-specific handler, you will create an errors folder inside src. Then, in the src/errors folder, create a file named methodNotAllowed.js.
Do this
Implement method-not-allowed handler

In src/errors/methodNotAllowed.js, add the following code:

function methodNotAllowed(req, res, next) {

  next({

    status: 405,

    message: `${req.method} not allowed for ${req.originalUrl}`,

  });

};


module.exports = methodNotAllowed;

Then, update pastes.router.js to add the methodNotAllowed() handler at the end of each route:

const router = require("express").Router();

const controller = require("./pastes.controller");

const methodNotAllowed = require("../errors/methodNotAllowed");


router.route("/:pasteId").get(controller.read).put(controller.update).delete(controller.delete).all(methodNotAllowed);

router.route("/").get(controller.list).post(controller.create).all(methodNotAllowed);


module.exports = router;

Next, send another PUT request to /pastes:
Descriptive error message from PUT request in Postman.

Much better! The /pastes router returns a status code of 405 for any method that is not otherwise handled.

Next, update users.router.js to add the methodNotAllowed handler at the end of each route:

const router = require("express").Router();

const controller = require("./users.controller");

const methodNotAllowed = require("../errors/methodNotAllowed");



router.route("/:userId").get(controller.read).all(methodNotAllowed);

router.route("/").get(controller.list).all(methodNotAllowed);


module.exports = router;

Great! Now the /users router returns a status code of 405 for any method that isn't otherwise handled.
Nested routers

What happens if you attach a router to another route, instead of attaching it to the app? Give it a try!
Do this
Attach the /pastes router to the /users router

In users.router.js, add the following code before any other routes:

const pastesRouter = require("../pastes/pastes.router");


router.use("/:userId/pastes", pastesRouter);

Now, send a GET request to /users/1/pastes:
GET request returning all the data in Postman.

Wow! It just worked! Of course, it isn't returning the expected data. How can you get it to return only records related to the :userId? By default, the route parameters from the parent router are not available on nested routes. Fortunately, Express has a way to merge the router parameters from parent routes if necessary.

In pastes.router.js, find the following code:

const router = require("express").Router();

Change it to this:

const router = require("express").Router({ mergeParams: true });

Then, in pastes.controller.js, change the list() function to be the following:

function list(req, res) {

  const { userId } = req.params;

  res.json({ data: pastes.filter(userId ? paste => paste.user_id == userId : () => true) });

}

Now the list() function will filter the pastes by userId if the userId is a route parameter.

Verify that it works. Send a GET request to /users/1/pastes:
GET request returning pastes with user_id 1 in Postman.

Now only those pastes that have 1 as the user id are included.

However, if the userId doesn't exist, you will get an empty array rather than the expected 404 error.
Do this
Add a middleware function before the /pastes router

You can also attach middleware functions in front of the nested router, if necessary, to make sure that the error messages accurately reflect the problem. For example, you could export the userExists() middleware function from the controller and use it to verify that the userId exists before calling the nested pastes router.

For example, in users.controller.js, export the userExists() middleware function:

module.exports = {

  list,

  read: [userExists, read],

  userExists,

};

Then in users.router.js, add the userExists() middleware function in front of the pastesRouter.

router.use("/:userId/pastes", controller.userExists, pastesRouter);

Now you get the expected 404 error if the user doesn't exist. This is one of the few cases where it is necessary to export middleware functions from the controller.

This demonstrates the power and flexibility of the Express router.

Note: You wouldn't want to leave the pastes router attached to the /users router in a real app. That's because as it is written, it will behave in some unexpected and non‑RESTful ways. Now you know how to organize your code, create controller files, router files, pass data on the response, and nest routers!
Completed example

A completed example from this lesson can be found here:

    Starter code: Robust server structure—05-advanced-tips branch
*** 31.8 Assessment: Robust server structure

** Postgres- Module 33
*** 33.1 Overview: Postgres
Overview: Postgres
9 minutesEstimated completion time
Overview

Databases are everywhere, and they're invaluable for storing persistent information. So in this module, you'll learn the basics of working with databases.

Databases are incredibly prevalent. They underlie the technology that you use every day—or even every hour!

Databases reside behind a huge percentage of websites. They're a crucial component of e-commerce systems, telecommunications systems, banking systems, video games, and just about any other software system or electronic device that maintains some amount of persistent information. They're also reliable, efficient, and scalable, and these properties make them exceptionally useful and convenient.

Databases are so ubiquitous and important that every company has at least one database to store company information such as employee records, credit and payment records, salary details, and more.
Do this

The purpose of the Do this sections in this module is to give you important hands-on experience. In these sections, you'll perform various tasks, like setting up a development environment or executing a command. Ultimately, these practice sections will help you successfully complete graded assessments, such as projects, mock interviews, and capstones.

At the end of the module, you'll need to take a quiz, so spend some time studying up. Alternatively, you'll join on as an apprentice, but you'll still need to ace this quiz first.
*** 33.2 Creating and deleting databases
Creating and deleting databases
1.5 hoursEstimated completion time
Learning Objective

By the end of this lesson, you will be able to create and delete a remote PostgreSQL database.
Overview

Installing and configuring PostgreSQL can be tricky on some operating systems. So rather than install a database server on your computer, during this program, you will use a free database hosted on the internet. Using a hosted database will let you quickly get set up with a PostgreSQL database, regardless of your operating system and your computer's performance characteristics. You'll set up your database in this lesson.
Key Terms

Hosted database
    Also called a managed database, a cloud-computing service in which the end user pays a cloud service provider for access to a database

Hosted databases

A hosted database, sometimes called a managed database, is a cloud-computing service in which the end user pays a cloud service provider for access to a database. Unlike a local database, you don't have to set up or maintain a hosted database on your own; rather, it's the provider's responsibility to oversee the database's infrastructure. This allows you to focus on building your application instead of spending time installing, configuring, and updating your database.
ElephantSQL

The video below provides a brief introduction to ElephantSQL.

In this program, you'll work with ElephantSQL, which will install and manage PostgreSQL databases for you. ElephantSQL offers databases ranging from small projects up to enterprise-grade multiserver setups. For this module, you will only create Tiny Turtle instances, which are free.

To get started, you need to sign up for a customer plan.

As with all websites, ElephantSQL will be updated over time, so some of the screenshots in this lesson may not exactly match what you see on your screen. Focus on completing the Do this objectives, such as creating a new account or creating a new database. The page titles and inputs will be substantially the same.
Do this
Create an ElephantSQL account

First, go to the ElephantSQL sign-up page, enter your email address (or use your existing GitHub or Google account), and click the Sign up button.

Note: ElephantSQL may change the following screens and sign-up workflow at any time. The screens that you see may be different, but that's okay—do your best to create an account regardless. If you need help, view the documentation on their website.
Signing up for an ElephantSQL account.

Once you click Sign up, you'll see a message telling you to check your email.
Email verification for ElephantSQL account.

An email will be sent to you with email address confirmation information. You need to confirm your email before you can create a free database.

Note: Some email providers, such as Yahoo, can take 15 minutes or more to deliver the confirmation email to your inbox. If you have a Gmail account, the email confirmation is usually delivered very quickly.
Confirming email for ElephantSQL account.

After clicking the Confirm Email button, you will see a screen like the following:
Continue creating ElephantSQL account.

Enter a strong password and click Submit. Next, you'll see something like the following screen:
ElephantSQL account created.

Next, you will create a new Tiny Turtle database. Don't let the name fool you; it isn't slow.
Creating a database

The video below provides a brief introduction to creating a database on ElephantSQL.

Now that you have an account, it's time to create a new database. ElephantSQL lets you create multiple free databases. You will create two databases in this lesson, and then you'll delete one of them.
Do this
Create a new database

To create a new instance, click the Create New Instance button.
Creating a new instance.

Then you will see a screen where you can enter the name, plan, and tags for your new instance.
Defining new instance.

In the Name field, enter development. The name of the instance isn't the same as the name of the database (more on this later). The name of the database will be a unique name assigned by ElephantSQL. The instance name that you enter in the Name field is for your reference only, and you should give each instance a unique name.

The Plan field determines the available size and performance of the database instance. For this module, you will always select a free plan. In this case, select Tiny Turtle.

Tags help you group related instances. Now, add a tag of Thinkful to your instance to indicate that you created it as part of this program.

Once you have filled in all of the information, click the Select Region button. You will see the following screen:
Selecting closer data center for faster database.

Typically, it's helpful if the location of the database is physically close to where you are located; this reduces network latency and the database will seem faster as a result. For the purposes of this lesson, leave the region that is already selected.

Then click the Review button. You will see the following screen:
Configuration entered for database displayed.

The above screen shows the configuration that you entered for the database.

If everything looks good to you, click Create instance. You will see the following screen:
All the instances you've created using ElephantSQL.

The Instances screen above shows all of the instances that you have created with ElephantSQL.

Now, click the name of the instance. That will bring you to the following screen, which shows more information about the instance.
Details about an instance.

On the Details screen above, you can see the server, region, username, password, and much more.

In a future lesson, you will use the server, user and default database, and password information from this screen to connect to this database.
Deleting a database

The video below provides a brief introduction to the process of deleting a database on ElephantSQL.

The free databases hosted on ElephantSQL cost the company a little bit of money for every second that each database exists. To ensure that the free database service continues to be free, you should promptly delete your database as soon as you are done with it. Even if you are only going to stop using the database for a short amount of time, it is best to extract the data, delete the database, and then create a new database later when you need it.
Do this
Delete a new database

First, create a new database instance named delete-me. If you need help with this, use the instructions above.

Now, click the Edit button next to the newly created instance.
Editing an instance.

You will see the Edit instance screen, which will look something like the following:
Edit instance screen.

At the bottom of the screen, you will see a Delete instance section. Enter the name of the instance and click the Delete button. Then you will see a confirmation dialog like the one below.
Confirmation dialog for deleting instance.

If you wish to continue, click the Delete button. Then you will return to the Instances screen, and a confirmation message will be displayed. It will look something like the following:
Message confirming instance deletion.

Now you have created and deleted a database hosted on ElephantSQL.
*** 33.3 Installing DBeaver
Installing DBeaver
1.5 hoursEstimated completion time
Learning Objective

By the end of this lesson, you will be able to install DBeaver and connect to a remote PostgreSQL database.
Overview

The easiest way to manage, view, and update a database hosted on the internet is to install a graphical user interface client. Once you install a GUI client in this lesson, you'll learn how to use it to manage almost every aspect of the hosted database.
Key Terms

Structured Query Language
    SQL, a programming language used to manipulate and query data in relational databases

DBeaver

DBeaver is a free multiplatform database tool for developers, SQL programmers, database administrators, and analysts. It essentially supports any database.
Do this
Install DBeaver

The videos below will give you a overview of the process for installing and configuring DBeaver.

Visit the DBeaver Download page and download the Community Edition installer for your operating system. When the download is complete, double-click the installer to begin the installation process. You will then see a screen similar to the following:
DBeaver setup.

The instructions shown here are for installing version 7.2.1. As the versions change over time, the order of the screens and options that you see will likely change. That's okay; when in doubt, select the default options and you should be fine.

You should install the latest version of DBeaver, and accept the default values for all of the installation options.

Once the installation is complete, start the application. On Windows, you will likely see a Windows Security Alert appear during the first startup. Allow DBeaver to communicate on public, private, or both networks; choose whichever is appropriate for your situation.
Allowing DBeaver to communicate on your network.

During the first startup, DBeaver will ask if you want to create a sample database. Select Yes. You can explore the sample database on your own later.
Confirm sample database creation.

Next, you will create a new database connection to the development database that you created in the previous lesson. If DBeaver automatically opens the Create a New Database Connection screen, you are all set. If not, click the New Database Connection button as shown on the screen below:
Click to create a new database connection.

When creating a new database connection, the first thing that DBeaver asks for is your database system:
Select a database system.

As you can see, DBeaver can connect to many different databases. Select PostgreSQL from the list and click Next. The first time that you select a database (PostgreSQL in this case), you will be prompted to download the database drivers. Click Download to download the drivers for your database.
Downloading drivers for your database.

DBeaver will download the latest versions of the drivers, so the versions may not be exactly the same versions shown in the screenshot above.

Once the drivers are downloaded, you will see the Connection Settings dialog. This is where you fill in the connection information from the database that you created in the previous lesson.
Filling in the connection information from your existing database.

On the DBeaver New connection screen, you must enter the connection information from the ElephantSQL database that you created earlier.
Source ElephantSQL field
	Destination DBeaver field
Server: (Note: Don't include anything in parentheses (...), as marked by the red X in the image above.)
	Host:
User & Default database:
	Database:
User & Default database:
	Username:
Password: (Note: Click the Reveal 👁 icon to see the entire password.)
	Password:

Now click the Test Connection button. If everything is correct, you'll see a message like the following:
Message if everything is correct in your connection.

Click OK to dismiss the Connection Test dialog. Then click Finish to save the connection information.

You will now see your database in the list on the left.

However, it currently doesn't have the same name that you gave it in ElephantSQL—it has the randomly generated database name assigned when the instance was created. Next, you are going to rename this connection so that you know that the connection and instance are related.
Do this
Rename database connection

In DBeaver, right-click the database connection, and you will see the following menu displayed:
Menu of options for database connection.

Select Rename on the menu, or click the database connection and press F2 to show the rename dialog.
Renaming database connection.

Rename the database connection to development so that it has the same name as the database instance on ElephantSQL. This does not change the name of the database, only the connection.

Now you have installed DBeaver and connected to a remote PostgreSQL database!
Database Navigator

The video below provides a brief introduction to executing SQL scripts in DBeaver.

Once the connection is created and named the same as your ElephantSQL instance, you can start to explore the development database. Select the development database by clicking the expander arrow ▶ next to its name. Under development, you will see the name of your database. Click the expander arrow again to see the contents of the database. Then, under Schemas, expand Public.

A schema is a container, similar to a folder on your hard drive, that holds named objects. One database can contain multiple schemas. PostgreSQL automatically creates a schema called public for every new database. Unless you specify the schema name, PostgreSQL will place all new objects into this public schema. In this lesson, you will only use the public schema.

For example, the screenshot below shows the public schema of a new database:
Public schema of a new database.

As you can see in the above screenshot, there are many different aspects of a database. There are Tables, Views, Materialized Views, Indexes, and so on. Databases are quite complex and have evolved over the years to satisfy the requirements of very large organizations. That is why there are specialized database administrators working at organizations that implement large databases. Thankfully, as a developer, you don't need to get too far into the weeds in order to build applications that use these databases. You're mainly going to focus on tables in this module, and mostly ignore the other objects.
Executing SQL scripts

Structured Query Language (SQL) is the language used to perform operations on a database. Later in this module, you will learn about SQL. For now, you will be given some scripts to execute, even though you don't yet know what all of the scripts do. That's okay; the goal of this lesson is to get you comfortable with running SQL scripts in DBeaver.

DBeaver comes with an SQL editor that provides syntax highlighting and intelligent statement completion. To open the editor, first select the development connection in the database navigator, then select the SQL Editor icon in the toolbar. Or, from the menu, click SQL Editor > SQL Editor.
Accessing SQL Editor.

This will open the SQL Editor window, which will look similar to the following screenshot:
SQL Editor window.
Tip

Sometimes, the active connection is displayed as <none> or <N/A>. This happens when DBeaver cannot determine which connection should be active. If you see <none> or <N/A>, you can set the active connection by selecting a connection at the top of the screen.

You can use the SQL editor to execute any valid SQL statement against your databases. For example, to create a new table with SQL, type the following SQL statement in the editor. The SQL statement below creates a table named users, which contains three columns: user_id, user_name, and role.

As you type in the following SQL, DBeaver will try to help you by autocompleting the text for you. Ignore this and keep typing. Don't worry about capitalization except for the names of things and default values, which should be lowercase here: users, user_id, user_name, role, and 'user'. Indentation is there for readability only, and won't impact the functionality of the script.

CREATE TABLE users (

  user_id INTEGER PRIMARY KEY GENERATED BY DEFAULT AS IDENTITY,

  user_name varchar(50) NOT NULL UNIQUE,

  role varchar DEFAULT 'user'

);

Then click the Execute script button to see the results.
Results of executed script.

The new table won't immediately show up under the tables in the database connection. You need to select Tables, then press F5 to refresh the Tables list.
Refreshing tables list.

This shows you the table that you just created. This process of executing a SQL script is something that you will do a lot throughout this module, so practice this task until you are comfortable with it.

As you learn more about SQL, you are free to use DBeaver or your own favorite database client to write the SQL statements.

It is possible to use the DBeaver user interface to create the users table, but this module will use SQL exclusively. Using SQL not only means that you can commit the SQL to Git and have the normal advantages of source code control; it also means that you can automate database-management tasks using SQL rather than having to run them manually through a UI.
*** 33.4 Database architecture
Database architecture
1.5 hoursEstimated completion time
Learning Objective

By the end of this lesson, you will be able to describe a PostgreSQL database and the components that make up the database.
Overview

In this lesson, you will learn about PostgreSQL, a relational database management system, which means that it is a system for managing data stored in rows and columns, similar to that of a spreadsheet.
Key Terms

Relational database management system
    Also known as RDBMS, a class of programs that can be used to create, update, and administer a relational database
Primary key
    A field that uniquely identifies the records in a table

What is a database?

The video below provides a brief introduction to databases. Start by watching the video, and then read through the rest of the lesson and complete the practice tasks. This will give you a thorough understanding of this topic.

A database is defined as an organized collection of data, generally stored and accessed electronically from a computer system.

Companies rely heavily on databases to store the data that powers their applications. Databases store a variety of data, such as usernames, email addresses, and nearly every other type of information that you can imagine.
Storing data

The video below provides a brief introduction to storing data in databases.

Imagine that you are opening an online store to sell handmade treats to pet owners. You would want to keep track of all sorts of information about your sales. Most people would open a spreadsheet and start inputting information as the orders come in.

Suppose that you have a customer, Mary, who buys a box of Bacon Bark Sticks. You capture all of that order information. Then, as you receive more orders, you record the same information for each customer. The spreadsheet might look something like this:
Name
	Product
	Qty
	Address
	Date
Mary Brown
	Bacon Bark Sticks
	1
	13352 Green Pass
	4/5/2020
Margarita Bullimore
	Puppy Pancakes
	4
	571 La Follette Point
	4/21/2020
Ashby Weagener
	Peanut Butter Pupcakes
	6
	679 Barnett Circle
	2/10/2021
Harmonia Kniveton
	Gingerbread Dog Biscuits
	12
	9 Johnson Trail
	6/6/2021

Later, Mary buys something else: a dozen Cheesy Dog Biscuits. But now, she's moved to a bigger home to have more room for all of her pets, so her address is different.
Name
	Product
	Qty
	Address
	Date
Mary Brown
	Bacon Bark Sticks
	1
	13352 Green Pass
	4/5/2020
...
	...
	...
	...
	...
Mary Brown
	Cheesy Dog Biscuits
	12
	7509 Bowman Trail
	6/6/2020

Now you have Mary's name duplicated, and you have contradicting values for her address. If your handmade treats store becomes enormously popular, these issues will escalate.

A few months later, Mary calls to ask about one of her orders. When you pull up the orders with her name, you see three different addresses:
Name
	Product
	Qty
	Address
	Date
Mary Brown
	Bacon Bark Sticks
	1
	13352 Green Pass
	4/5/2020
Mary Brown
	Cheesy Dog Biscuits
	12
	7509 Bowman Trail
	6/6/2020
Mary Brown
	Paw-print Pretzels
	36
	29 Lerdahl Parkway
	9/6/2020

You can't be sure whether each of these orders belongs to the person calling, because there is nothing uniquely identifying the three different Marys. As you can see, this might lead to a messy situation. Customers might get mixed up, or the wrong products could be sent to a customer.

How would you resolve this? Instead of having one massive spreadsheet, you can use a database and separate the information into separate bite-size "spreadsheets" called tables.
Tables

A table is a collection of related data held in a table format within a database. Like a spreadsheet, a table consists of columns and rows.

Like a spreadsheet, a table has a fixed number of columns and can have any number of rows. Each row, which represents a complete record of the specific data, is typically identified by one or more values called the primary key.

A primary key is a special table column, or combination of columns, that uniquely identifies each table record. No two rows in a table can have the same primary key. Sometimes, the data itself has some unique property that may be used as a primary key. In such instances, you can let the client that inserts data into the database be responsible for ensuring that the key is unique. For example, a driver's license number could be a primary key: it uniquely identifies each driver.

So, with your pet treat store, you might start with a table named customers, to store all of the customers. The primary key is the customer_id column. In the table below, for example, you know that there are two different customers with the same name (Mary Brown) because they have different customer_id values. This illustrates how to use the primary key to differentiate data such as customers.
customer_id
	customer_name
	address
1
	Mary Brown
	13352 Green Pass
28
	Mary Brown
	7509 Bowman Trail

You then create a separate table, products, to store all of your products. The primary key is the product_id column.
product_id
	description
	quantity_in_stock
5
	Paw-print Pretzels
	100
...
	...
	...
42
	Bacon Bark Sticks
	200

You then create another table, orders, to store all of your orders. The primary key is the order_id column.
order_id
	customer_id
	date
67
	1
	9/6/2021

And finally, you create a table named order_items, to store all of the items in an order. This time, the primary key is a combination of the order_id and product_id columns, which ensures that a given product can only appear once in an order.
order_id
	product_id
	quantity
67
	5
	36
67
	42
	4

Given the records above, you see that Mary Brown living at 13352 Green Pass ordered 36 Paw-print Pretzels and 4 Bacon Bark Sticks on 9/6/2021, which was order number 67.

Relationships between tables are defined by putting the primary key of one table into a column in the related table. For example, you know that order_id 67 is for Mary Brown at 13352 Green Pass, and not Mary Brown at 7509 Bowman Trail, because the orders table has a customer_id field that contains 1, the primary key for Mary Brown at 13352 Green Pass.

A field is the container that stores data in a table. Each table has a fixed set of fields, or containers, to hold data. The data that can be stored in a field is determined by the data type of the field. The number of columns and rows in a table determines the number of fields. For example, if you have a table with two rows and two columns, your table will have four fields.

Now, you know that a database is a collection of tables. Each table is a named object that contains a collection of rows. Each row of a table has the same fixed set of fields, and each field is of a specific data type. Like a spreadsheet, the fields in the table have a fixed order in each row. However, unlike a spreadsheet, there is no fixed order for the rows.
Fields

Every table is made up of a fixed set of fields. A field's data type is the most important property because it determines what kind of data the field can store.

Data types can seem confusing. For example, if a field's data type is text, it can store data that is either text or numerical characters. But a field whose data type is numeric can store only numerical data. So, you have to know what can be stored with each data type.

A field's data type also determines other important field qualities, such as the following:

    Which formats can be used with the field

    The maximum size of a field value

When to use which data type

Think of the field's data type as a set of rules that apply to all the values that are contained in the field. For example, values that are stored in a varchar field can contain only a limited number of characters, and a text field can contain an unlimited number of characters.
Tip

Sometimes, a field might look like it is made up of one data type when it is actually a different type. For example, a field might seem to be composed of numeric values but actually contain text values.

You don't need to know every data type available, but you should know the most common ones and how they are used. The following table lists the most common data types available in PostgreSQL.
Data type
	Use to store
bool
	Only logical boolean values: True or False
char
	Fixed-length character data where the number of characters is always the same. This could be used to store information such as state code, zip code, or phone number.
varchar
	Variable-length character data, up to around 1 GB max. This could be used to store information such as name, address, or city.
date
	Calendar date (year, month, day) without time (for example, 2020-09-09)
int
	Integers from -2,147,483,648 to +2,147,483,647
numeric
	Exact numeric values
smallint
	Integer from -32,768 to +32,767
smallserial
	Auto-incrementing integer from 1 to 32,767. This could be used to store information such as a primary key for a table that will never have more than 32,766 rows—like a table to store application settings.
serial
	Auto-incrementing integer from 1 to 2,147,483,647. This could be used to store information such as a primary key for a table.
text
	Variable-length characters of any length
time
	Time of day (no date)
timestamp
	Date and time (no time zone)
uuid
	Universally unique identifier (for example, 28e3d683-e619-46c8-b444-a0f45817db2d)

PostgreSQL offers a large number of data types in addition to the types listed above. More information about the data types in PostgreSQL can be found in the documentation.
*** 33.5 Creating and viewing tables
Creating and viewing tables
1.5 hoursEstimated completion time
Learning Objective

By the end of this lesson, you will be able to create, read, update, delete, and list tables in a PostgreSQL database.
Overview

A PostgreSQL database is made up of several components, of which the table is most significant. The database table is where all of the data in a database is stored. And without tables, there wouldn't be much use for a database. At a minimum, a PostgreSQL database consists of one or more tables. In this lesson, you'll learn how to define a table, the columns in the table, and each column's data type; this is a fundamental skill that you will need when working with databases.
Key Terms

Data definition language
    DDL, a subset of SQL commands used to define the tables in the database

Starter code

This lesson builds upon the work of the earlier lessons. If you haven't successfully completed the work from the earlier lessons, go back and do that now.

If you are having trouble, reach out for assistance.
Structured Query Language (SQL)

As you learned earlier in this module, Structured Query Language (SQL) is the database language used to perform operations on a database. You can also use SQL to create a database. SQL uses certain commands, such as CREATE, DROP, and INSERT, to carry out various tasks.

SQL operations can be performed on any object in the database, not just tables. But to simplify things, this lesson will focus on SQL commands operating on tables only.

SQL commands fall into four categories:

    Data definition language (DDL)

    Data query language (DQL)

    Data manipulation language (DML)

    Data control language (DCL)

This lesson will focus on DDL. DQL and DML will be covered in future lessons. DCL, which is used by database administrators to configure security access to the database, is out of scope for this module.
Data definition language

Data definition language (DDL) is a subset of SQL commands that is used to define the tables in the database. It is used to create and modify any object in the database, but you will focus on using it to create and modify the tables.

The following list contains the most common DDL commands:

    CREATE is used to add new tables to the database.

    ALTER is used to alter the structure of the database.

    TRUNCATE is used to remove all records from a table, including removing all storage space allocated for the records.

    DROP is used to delete tables from the database.

CREATE TABLE statement

The SQL CREATE TABLE command is used to create a new table. The basic syntax of a CREATE TABLE is as follows:

CREATE TABLE table_name (

   column_name data_type  column_constraint [ , ]

);

Tip

Table names in PostgreSQL are lowercase; words are separated by an underscore _ and are plural by default (such as users, products, orders, and order_items).

The line column_name data_type column_constraint [ , ] is called a column definition, and there must be at least one column definition in the table. A column definition describes a column in a table. There are a number of decisions that must be made for each column. In particular, the following decisions must be made:

    What is the name of the column?

    What is the data type of the data in the column?

    What is the size or max length of the column?

    Are null values allowed in the column?

    Is there a default value?

    Is this column the primary key or part of the primary key?

For example, suppose that you want to create a table to store some information about your users, so you create a table named users. It is made up of these three columns:
Column name
	Data type
	Max length
	Allow null?
	Default value
	Primary key
	Unique?
user_id
	integer

	N/A

	Yes

user_name
	varchar
	50
	No


	Yes
role
	varchar

	No
	user


Tip

Although you could use name and value for the column names, both NAME and VALUE are keywords in SQL. Generally, it is best to avoid using any keywords as column names.

In the users table, there are two columns that must store character data of varying lengths. That is, you don't know in advance the length of characters that will be stored in the user_name and role columns. PostgreSQL allows you to specify a fixed-size column with the data type character(n) or variable-length column with the varchar(n) data type. The (n) part of varchar(n) is optional and should be omitted by default. If what you really need is a text field with a length limit, then varchar(n) is great. But if you pick an arbitrary length and choose varchar(20) for a surname field, you're risking production errors in the future when Hubert Wolfeschlegelsteinhausenbergerdorff signs up for your service.

A NOT NULL column constraint means that the column isn't allowed to have null values. Sometimes null values are necessary because the value may be optional, and representing a missing value is the exact purpose of NULL. Say, for example, that you want to include a mobile_phone column in the users table, but not every user has a mobile phone. In that case, you would allow the mobile_phone column to contain NULL.

When designing a table and defining constraints for the fields, you need to carefully consider how the data will be used.
Tip

It's best to declare a NOT NULL constraint on each column for which a null would be nonsensical. Rather than rely on application code, allow the database to enforce constraints uniformly.

Fields are filled in with the DEFAULT value when a new row is created and no value is specified for that field. If no default value is declared explicitly, then NULL is the default value. Sometimes it makes sense to set a default value for a column so that, when data is inserted into the table, that column's value need not be specified. Looking at the table above, you can see that the role column has a default value of user, which means that the role will be user unless another value is specified when the row is created. However, it isn't considered good practice to define a DEFAULT for every column. For example, the user_name field shouldn't be null and has no default. What default, if any, should you declare for this column? It's valid and common for a column to need a NOT NULL constraint yet have no logical default value.

The primary key must have a unique value for each row in the table. Sometimes, the data itself has some unique property that may be used as a primary key. In such instances, you can let the client be responsible for ensuring the key is unique.

More often than not, however, you need to introduce a column specifically to be a primary key, as in the user_id column for the users table. In such instances, you need to consider how you would make that ID unique for each new row inserted into the table. There are several options available. You can use a library such as uuid to generate a random value. Or you can simply create a serial value that increments the primary key by 1 each time that a new row is inserted. Because an automatically incremented serial value is fairly common and quite easy to do, you will use that option here.

Finally, if a column must have a unique value in the table, you can add UNIQUE to constrain the table to only allow unique values in the column. For example, it would be very confusing for two users to have the same user_name value, so you should add the UNIQUE constraint to that column. This means every row in the entire table must have a unique value in the user_name column.

The basic syntax of a column definition is as follows:

column_name data_type column_constraint

For example, the following column definition creates a primary key column that automatically increments for each new row:

user_id INTEGER PRIMARY KEY GENERATED BY DEFAULT AS IDENTITY

So, the following DDL will create the users table:

CREATE TABLE users (

  user_id INTEGER PRIMARY KEY GENERATED BY DEFAULT AS IDENTITY,

  user_name varchar(50) NOT NULL UNIQUE,

  role varchar DEFAULT 'user'

);

Do this
Execute the create-users-table script

Over time, you will add more and more tables to your database. Ideally, you will have one script that can fully create all database tables in an empty database. Later, when you add a new table, you will add the necessary SQL to your database script. Wouldn't it be nice to run the entire script, creating all new tables, and skipping the existing tables? Most of the time, you only want to create the table if it doesn't already exist.

PostgeSQL, and several other databases, have special syntax to create the table only if it doesn't exist; you do this by adding IF NOT EXISTS.

Now, execute the following create-users-table script multiple times. Notice that no matter how many times you execute the script, it is successful.

CREATE TABLE IF NOT EXISTS users (

  user_id INTEGER PRIMARY KEY GENERATED BY DEFAULT AS IDENTITY,

  user_name varchar(50) NOT NULL UNIQUE,

  role varchar DEFAULT 'user'

);

As you observed, this script can be executed multiple times without error and without changing the database beyond the first successful execution. Writing your DDL scripts this way makes it possible to have a single script to automate the creation of the database from any starting point, without encountering the errors that would occur when trying to create tables that already exist.
Listing tables

At times you will want to get information about existing PostgreSQL tables. PostgreSQL provides an information_schema schema that can return information about all of the objects in the database, including tables.
Do this
Query list of tables

Execute the following script to see a list of tables in your database:

SELECT

	table_name

FROM

	information_schema.tables

WHERE

	table_schema = 'public'

	AND table_type = 'BASE TABLE'

ORDER BY

	table_name;

After running the above SQL, you will see something like the following:
	table_name
1
	users

The above SQL is asking the following question: which tables exist in the public schema? Any time that a script starts with SELECT, it is asking a question (which is also called querying). SELECT is part of data query language (DQL), a subset of SQL. You will learn more about querying in future lessons. For now, you will learn two queries:

    List all tables—the query that you just learned about

    Describe a table's structure—the query that you will learn next

Describing a table

At times, you will want to get information about the existing columns in a table. For example, what are the column names? For each column, what is the data type? Is it nullable? Does it have a default value? And so on. This type of query is commonly called a table description.

Execute the following script to see a description of the users table:

SELECT

   table_name,

   column_name,

   data_type,

   is_nullable,

   column_default,

   is_identity

FROM

   information_schema.columns

WHERE

	table_schema = 'public'

    and table_name = 'users';

Note the part of the script that reads and table_name = 'users';. You can change 'users' to the name of any table in the public schema.

After running the above SQL, you will see something like the following:
table_name
	column_name
	data_type
	is_nullable
	column_default
	is_identity
users
	user_id
	integer
	NO

	YES
users
	user_name
	character varying
	NO

	NO
users
	role
	character varying
	YES
	'user'::character varying
	NO

If you are using DBeaver, you can click the users table in the Database Navigator pane, and press F4 to see all of the properties of the table.

Suppose you later decide to keep track of each user's birthday so that you can send them a birthday card. What can you do to add a row to the table after it's created? You can alter it.
ALTER TABLE statement

Sometimes, it's necessary to make changes to a table after it has been created. Especially when the database already contains data, deleting and recreating the table may not be possible.

Use the DDL ALTER TABLE statement to change the structure of a table. There are many ways that a table can be altered, and the full documentation for the ALTER TABLE statement lists all the possibilities. For now, you will only add and drop columns.

The syntax to add a column with the ALTER TABLE statement is as follows:

ALTER TABLE <table name> ADD COLUMN IF NOT EXISTS <column definition>;

And to remove a column, the syntax is like this:

ALTER TABLE <table name> DROP IF EXISTS <column name>;

Do this
Add a birthday column

You now need to add a birthday column to the users table. This will be the date part only, and no time value is needed. Next, there is no default value for birthday that makes sense, so the column won't have a default value. Then, it cannot be null, because every user has a birthday. Finally, the script should only add the birthday column if the column doesn't already exist.

Run the script below:

ALTER TABLE users ADD COLUMN IF NOT EXISTS birthday date NOT NULL;

After running the above SQL, run the describe query again. You will see something like the following:
table_name
	column_name
	data_type
	is_nullable
	column_default
	is_identity
users
	user_id
	integer
	NO

	YES
users
	user_name
	character varying
	NO

	NO
users
	role
	character varying
	YES
	'user'::character varying
	NO
users
	birthday
	date
	NO

	NO

Note the last row has birthday in the column_name field.
Do this
Make role non-nullable

Note that the role column will allow null values. Suppose that you don't want the user role to be null because the role will be used to determine which menu options the user will see on the website using this database. As a result, the role column should always have a value.

You will use the ALTER TABLE statement to set NOT NULL on the role column.

Run the following script to alter the role column in the users table:

ALTER TABLE users ALTER COLUMN ROLE SET NOT NULL;

After running the above SQL, run the describe query again, and you will see something like the following:
table_name
	column_name
	data_type
	is_nullable
	column_default
	is_identity
users
	user_id
	integer
	NO

	YES
users
	user_name
	character varying
	NO

	NO
users
	role
	character varying
	NO
	'user'::character varying
	NO
users
	birthday
	date
	NO

	NO

As you can see, the role column no longer allows null values.

Also, note that you can run this script multiple times and it will be successful each time.

Suppose that as time goes on, some of your users stop using the application. Rather than delete the user, you would like to set a TRUE-FALSE value indicating whether or not the user is an active user. Next, you will add a boolean column named is_active, which will be TRUE by default.
Do this
Add is_active column

You now need to add an is_active column to indicate whether the user is active or not. This will be a boolean type. The boolean type can have three states: TRUE, FALSE, and a third state, UNKNOWN, which is represented by NULL. You will learn more about boolean types and nulls in a future lesson. For now, don't allow NULL in the is_active column.

Run the following script to add an is_active column to the users table:

ALTER TABLE users

ADD COLUMN IF NOT EXISTS

	is_active boolean NOT NULL default true;

After running the above SQL, run the describe query again, and you will see something like the following:
table_name
	column_name
	data_type
	is_nullable
	column_default
	is_identity
users
	user_id
	integer
	NO

	YES
users
	user_name
	character varying
	NO

	NO
users
	role
	character varying
	YES
	'user'::character varying
	NO
users
	birthday
	date
	NO

	NO
users
	is_active
	boolean
	NO
	true
	NO
DROP TABLE statement

Sometimes, as the structure of a database changes over time, you may find that you no longer need a table. Maybe the data that was stored in the table was distributed to other tables, or perhaps you are retiring certain functionality in the application and no longer need the data. Maybe you just made a mistake creating the table. Whatever the reason, there are times when you will want to delete a table.
Do this
Drop the users table

Run the following script to drop the users table:

DROP TABLE IF EXISTS users;

Then run the list tables query from earlier, and you will see that the users table no longer exists.
*** 33.6 Inserting data
Inserting data
1.5 hoursEstimated completion time
Learning Objective
By the end of this lesson, you will be able to insert one or more new records into a table.

Overview
So far, you have been focused on creating tables using data definition language. Eventually, you will have to learn SQL statements to read information out of a table. But that begs the question: how did the data get there in the first place? In this lesson, you will focus on the INSERT statement, which is part of data manipulation language. If you want to create data, you need to use the SQL keyword INSERT.

Key Terms
Data manipulation language
DML, a subset of SQL commands used to manipulate data in the database
Starter code
This lesson builds upon the work of the earlier lessons. If you haven't successfully completed the work from the earlier lessons, go back and do that now.

If you are having trouble, reach out for assistance.

Create a new table
At this point, your database does not have any tables, so you will need to add one.

Do this
Create the articles table
Execute the following script to create the articles table:

CREATE TABLE IF NOT EXISTS articles (
  article_id INTEGER PRIMARY KEY GENERATED BY DEFAULT AS IDENTITY,
  title varchar(255) NOT NULL UNIQUE,
  abstract TEXT NOT NULL,
  body varchar,
  author TEXT NOT NULL,
  created_date DATE NOT NULL DEFAULT CURRENT_DATE
);
The INSERT INTO statement
The video below provides a brief introduction to inserting data into a table. Start by watching the video, and then read through the rest of the lesson and complete the practice tasks.


Now that a table exists in the database, you can insert rows of data into the table. As described in the video above, you can use SQL's INSERT INTO statement to insert new rows into a table. The basic syntax is as follows:

INSERT INTO <table name>
  (<column list>)
  VALUES (<value list>);
The column list is a comma-separated list of the column names for which you are providing a value. The columns may be listed in any order, but the order of the values in the value list must match the order of the column names.

Do this
Create first article
Imagine that you want to add an article with the following title: "Five uses for paper that you never imagined." The abstract is as follows: "List five surprising uses for paper that you would not think of on your own." It's authored by "Dwight Schrute." To add this article, you can run the following INSERT INTO statement:

INSERT INTO articles
  (title, abstract, author)
  VALUES (
    'Five uses for paper that you never imagined',
    'List five surprising uses for paper that you would not think of on your own',
    'Dwight Schrute'
  );
Tip
As demonstrated above, character data in SQL is enclosed in single quotes '.

You will learn about the SQL SELECT statement in detail later, but to simply see the data in a table, use the following script:

SELECT * FROM articles;
Notice in this response that there is one row of data in the table. The article_id was generated and should be 1 because this is the first row inserted. The title, abstract, and author have the values that you specified. The created_date is set to the default CURRENT_DATE. And the body is empty because there is no default and you did not provide a value.

Do this
Create a second article
Try another example. Execute the following script:

INSERT INTO articles
  (title, abstract, author, created_date)
  VALUES (
    'Pulp Fiction',
    'Myths about paper making',
    'Dwight Schrute',
    '2021-01-01'
  );
This time, a created_date is provided. Use the SELECT statement from above to view the data. Notice that the created_date is set to the value that you provided rather than the default.

Table constraints
The video below provides a brief introduction to testing table constraints.


Do this
Try to insert a duplicate title
Try to insert another article with the title "Pulp Fiction."

INSERT INTO articles
  (title, abstract, author)
  VALUES (
    'Pulp Fiction',
    'A Movie Review of the greatest S. Jackson movie ever made.',
    'Pam Beesly'
  );
This time, you get an error message.

ERROR:  duplicate key value violates unique constraint "articles_title_key"
DETAIL:  Key (title)=(Pulp Fiction) already exists.
That is, the UNIQUE constraint that you placed on the title column was violated.

Note: Unlike DDL statements (such as CREATE TABLE), it is difficult to write INSERT INTO statements that can be run multiple times and only insert the new record if the data doesn't already exist.

To illustrate that the order of columns does not matter, try one more example:

INSERT INTO articles
  (author, abstract, title)
  VALUES (
    'Pam Beesly',
    'A thesis on why the rectangle is the only shape we need',
    'Ode to Rectangle'
  );
Do this
Try to insert a null title
Now, try out one final example to test the NOT NULL constraint. Try leaving out the title.

INSERT INTO articles
  (author, abstract)
  VALUES (
    'Pam Beesly',
    'Why A4 is way better than letter size'
  );
As expected, the error message indicates that the NOT NULL constraint was violated.

ERROR:  null value in column "title" violates not-null constraint
DETAIL:  Failing row contains (8, Why A4 is way better than letter size, Paper, null, Pam Beesly, null).
Inserting multiple records
You can also insert multiple records at one time by providing multiple VALUES separated by a comma. The video below demonstrates this process.


Do this
Insert multiple articles
Execute the following script:

INSERT
	INTO
	articles (title, abstract, author)
VALUES
	(
	  'The year of PostgreSQL is every year',
	  'PostgreSQL development began during the Reagan administration — in 1986!',
	  'Gerik Haslegrave'
	),
	(
	  'A Getting Started PostgreSQL Tutorial',
	  'PostgreSQL is a free open source database system that uses and extends the SQL language',
	  'Calv Baudrey'
	),
	(
	  'Experimenting with PostgreSQL in a Container',
	  'When I heard I needed a PostgreSQL database, I immediately thought container.',
	  'Gracie Brann'
	);
Practice writing INSERT statements by inserting a few more rows into the articles table.

Script files
The video below provides a brief introduction to creating SQL script files.


It's common to write a number of SQL statements in a file and then execute them using a client to perform some task. For example, you can save your SQL scripts in a plain text file using Visual Studio Code, NotePad, TextPad, DBeaver, or any other plain text editor. Then you can save the file as a SQL file.

Tip
You can also use word-processing programs such as Microsoft Word or Pages, but you must save the files as plain text and not the default format of the editor.

Whenever you need to create a table or insert data for testing purposes, consider saving the script to a file. That way, you can rerun the script whenever you need it.

Do this
Execute a script stored in a file
Note: If you are using VSCode, you may see a warning to install a missing extension for mssql. You can safely ignore this warning.

Use your preferred editor to create a new file named customers.sql and add the following SQL script:

-- First, remove the table if it exists
DROP TABLE IF EXISTS customers;

-- Create the table anew
CREATE TABLE customers (
  customer_id INTEGER primary key generated by default as identity,
  customer_name varchar NOT NULL,
  phone VARCHAR(30)
);

-- Insert some test data
-- using a multi-row insert statement here
INSERT INTO customers (customer_name, phone)
VALUES
  ('Sam', '555-1234'),
  ('Ham', '555-4321'),
  ('Ram', '555-5678');
Tip
As demonstrated above, comments in SQL start with --.

To run this script in DBeaver, first make sure that you are connected to a database. Then, select File from the menu bar, select Open file, and open the SQL script that you just created. Make sure that the active data source is development and the active schema is public, then click Execute script.

*** 33.7 Querying data
Querying data
1.5 hoursEstimated completion time
Learning Objective
By the end of this lesson, you will be able to use the SELECT statement to extract useful information from a database.

Overview
One of the most important aspects of using a database is the practice of retrieving data, whether it's on an ad hoc basis or part of a process that's been coded into an application. There are several ways to retrieve information from a database, but one of the most commonly used methods is submitting queries. A query is any command used to retrieve data from a table. In SQL, queries are almost always made using the SELECT statement.

Starter code
This lesson builds upon the work of the earlier lessons. If you haven't successfully completed the work from the earlier lessons, go back and do that now.

If you are having trouble, reach out for assistance.

Querying data
Now that some data exists in the database, you need a way to retrieve that data. You can use the SQL SELECT statement to retrieve rows from a table. This command has a lot of options, but the basic syntax of the SELECT statement is as follows:

SELECT <column list>
FROM <table name>;
In the previous lessons, you used this statement with an asterisk * as the column list. The * is simply a shortcut for all columns. However, it is advisable to explicitly list the columns that you want to retrieve and never select more than you need.

For the small databases that you have created so far, using * makes no difference, but when scaled to millions of rows in a real production database, it starts to make a big difference. The more specific that you can be about the columns and rows that you want to retrieve, the more efficiently the query will run.

You can select the titles and abstracts of all articles using the following script:

SELECT title, abstract
FROM articles;
Or you can select just the titles, like this:

SELECT title
FROM articles;
The WHERE clause
Sometimes, you do not want all of the data in the table. To restrict the rows that are returned by a SELECT statement, use the WHERE clause. The basic syntax is as follows:

SELECT <column list>
FROM <table name>
WHERE <conditions>;
The conditions are boolean expressions—statements that evaluate to either TRUE or FALSE for each row in the results.

Do this
Find articles written by Dwight Schrute
For example, suppose that you want to see the titles of all articles written by Dwight Schrute. Execute the following SQL to select the title column from the articles table, but only where the author column has the value Dwight Schrute:

SELECT title
FROM articles
WHERE author='Dwight Schrute';
Searching for null
The key concept for understanding how null values behave in boolean expressions is that NULL is neither TRUE nor FALSE.

Suppose Stan is thirty years old, while Oliver's age is unknown. If you are asked whether Stan is older than Oliver, your only possible answer is "I don't know." If you are asked whether Stan is the same age as Oliver, your answer is also "I don't know." If you are asked for the sum of Stan's age and Oliver's age, your answer is the same.

Suppose Charlie's age is also unknown. If you are asked whether Oliver's age is equal to Charlie's age, your answer is still "I don't know." This shows why the result of comparisons like NULL = NULL and NULL <> NULL are NULL instead of TRUE.

Tip
<> is a comparison operator that means "not equal to."

A null value certainly isn't TRUE, but it isn't FALSE either. If it were, then applying NOT to a null value would result in TRUE. But that isn't the way it works; NOT (NULL) results in another NULL. This confuses some people who try to use boolean expressions with NULL.

Because neither equality nor inequality returns TRUE when comparing one value to a null value, you need some other operation if you are searching for a null. SQL has IS NULL, which returns TRUE when compared to a null value. The opposite, IS NOT NULL, returns FALSE when compared to a null value.

Do this
Find articles with a null body
Execute the following SQL to find all articles where the body field is NULL:

SELECT title
FROM articles
WHERE body IS NULL;
In addition, you can use IS DISTINCT FROM. This works like an ordinary inequality operator <>, except that it always returns TRUE or FALSE, even when using NULL.

Execute the following SQL to find all articles with a null body value:

SELECT *
FROM articles
WHERE body IS NOT DISTINCT FROM NULL;
Execute the following SQL to find all articles with a non-null body value:

SELECT *
FROM articles
WHERE body IS DISTINCT FROM NULL;
Execute the following SQL to find all articles with a null body value or a body value that is not equal to some-value:

SELECT *
FROM articles
WHERE body IS DISTINCT FROM 'some-value';
The above SQL will find any article where the body value is DISTINCT FROM 'some-value'. Because NULL is distinct from some-value, null values are included in the results.

Products
To fully exercise the range of options available in the SELECT statement, you will use an existing dataset of products. This dataset contains thousands of products.

Next, you will create a new table named products and import all of the data into the table by running one script.

Do this
Execute the products.sql script
Download the SQL file named products.sql and save it in some location on your computer (you can right-click on the link and choose Save As). This SQL file contains a script to create a products table and add data to it.

Open the SQL file in your client and execute the script. (Revisit the previous lesson if you don't recall how to run a script file.)

The results of the script should show "Updated rows: 49688." This means that there are 49,688 rows of data in the products table after executing the script.

Once you have successfully executed the script, try answering the following questions about this data. Be sure to run each of the following SQL scripts and confirm that the results are as expected.

Do this
List all items in the coffee aisle
Run the following script to list all items in the coffee aisle:

SELECT product_name, aisle, department, price
FROM products
WHERE aisle = 'coffee';
As you can see, it's a long list. In all the rows, you don't need aisle and the department columns, so you can eliminate those columns from the query, like so:

SELECT product_name, price
FROM products
WHERE aisle = 'coffee';
List all desserts in the bakery department
First, look at all of the products in the bakery department:

SELECT product_name, aisle
FROM products
WHERE department = 'bakery';
Here you see products across many aisles. There is one aisle named bakery desserts, so if you restrict the results to only the bakery desserts aisle, that might satisfy the question. However, to do so, you need a WHERE clause with two conditions: department = 'bakery' and aisle = 'bakery desserts'.

In the WHERE clause, you can combine multiple conditions using AND and OR operators. If you want both conditions to be TRUE, use the AND operator.

SELECT product_name, aisle, department
FROM products
WHERE department = 'bakery'
AND aisle = 'bakery desserts';
You can remove the aisle and department columns because they are not needed to answer the question. Those columns are in the query just to show that the query only returned rows that satisfy both conditions.

What are the names of the aisles?
Suppose that you need to see a list of all aisles. You might start with the following query:

SELECT aisle
FROM products;
This returns a list of all the aisles. Unfortunately, there are lots of duplicates in the results. In fact, it lists the aisle value for all 49,688 rows. You can be sure that the store does not have 49,688 aisles. Add the DISTINCT operator to the query to remove duplicates from the results.

SELECT DISTINCT aisle
FROM products;
This time, the list is much shorter and there are no duplicates.

How many aisles are there?
Even though the list is much shorter, there are still a large number of results. So how many are there? You can use the count() aggregate function to return only the count of the number of rows.

SELECT count(DISTINCT aisle)
FROM products;
And you see that there are 134 distinct aisles in the table.

Now, count the number of aisles again, this time without using the DISTINCT operator:

SELECT count(aisle)
FROM products;
You'll see that 49,688 rows of data are returned.

Which aisles have beverages?
Here again, you have to use multiple features of the SELECT statement. Try to find the data that you need.

First, how do you know that an item is a beverage? Start with a list of departments, as follows:

SELECT DISTINCT department
FROM products;
So there are 21 departments, and beverages is one of them.

Now, look at a list of aisles in the beverages department. You can use the condition department = 'beverages' to get only those rows that belong to the beverages department. But you are only interested in the aisles, not the actual products or prices, so you can select just the aisle column. And because you only want a list of aisles, you can make it DISTINCT.

Run the following query:

SELECT DISTINCT aisle
FROM products
WHERE department = 'beverages';
And you see that there are beverages in eight different aisles.

Find a decent tea
Suppose you want to find a tea drink. There is an aisle named tea in the beverages department. Select all products in that aisle and include their prices with the following script:

SELECT product_name, price
FROM products
WHERE department = 'beverages'
AND aisle = 'tea';
Hmmm, that is a lot of products—and some of them are quite expensive. Now, look for teas that cost less than $15. But not too cheap either; you still want some quality, so only include teas that cost more than $10.

SELECT product_name, price
FROM products
WHERE department = 'beverages'
AND aisle = 'tea'
AND price < 15
AND price > 10;
That's still quite a lot. You can use count() to see how many products match those criteria.

SELECT count(*)
FROM products
WHERE department = 'beverages'
AND aisle = 'tea'
AND price < 15
AND price > 10;
So there are 85 tea drinks between $10 and $15. Raspberry is nice, so look for a raspberry-flavored tea. Unfortunately, although there are several raspberry-flavored teas, there is no column of data that gives you this information cleanly. That is, you may find products with such names as Raspberry Royale Black Tea or Rio Raspberry Tea. But if you just search for Raspberry, using the following script, you will get no results. Take a look:

SELECT count(*)
FROM products
WHERE department = 'beverages'
AND aisle = 'tea'
AND price < 15
AND price > 10
AND product_name = 'Raspberry';
What you need is some way to ask if the product_name contains the word Raspberry. In SQL, you can use the LIKE operator and the % wildcard character to perform such a search. The LIKE operator is a comparison similar to =, but it allows you to use the % symbol to match zero or more characters. So searching for product_name LIKE '%Raspberry%' will match any product that contains the word Raspberry anywhere in the name, followed by zero or more characters.

SELECT product_name, price
FROM products
WHERE department = 'beverages'
AND aisle = 'tea'
AND price < 15
AND price > 10
AND product_name LIKE '%Raspberry%';
And there are five products that match those criteria.

List all pet products, from most expensive to least expensive
First, get a list of products in the pet department:

SELECT product_name, price
FROM products
WHERE department = 'pets';
But you want this list sorted by price. In SQL, you use the ORDER BY clause to sort the results by certain columns. To sort by the price column, run the following query:

SELECT product_name, price
FROM products
WHERE department = 'pets'
ORDER BY price;
You are making progress: the cheapest pet product is first in the list. As you scroll down the list, the products get more expensive. However, you want to sort the list in the reverse order. By default, ORDER BY sorts in ascending order; that is, it sorts from the lowest numerical value to the highest. Or, if it's a text column, it sorts alphabetically from A to Z. To sort in descending order, use the DESC option.

SELECT product_name, price
FROM products
WHERE department = 'pets'
ORDER BY price DESC;
How many products are there per department?
Previously, you used the count() function to get a count of rows. If you want a count of products in the pets department, for example, you could first select only products from the pets department, then do a count.

SELECT count(*)
FROM products
WHERE department = 'pets';
But what if you wanted the count for each department? Manually running the same statement once for each department would be difficult and clumsy. Instead, you can first group the products by department and then perform a count in each group. To group rows, use the GROUP BY clause, like this:

SELECT department, count(*)
FROM products
GROUP BY department;
What is the total price of all products per department?
Previously, you used the GROUP BY clause and the count() function to get a count of the number of products per department.

But what if you wanted the total price of all products in each department? Manually adding up the prices of the products in each department would be difficult and error prone. Instead, you can use the sum() function to add up the prices, like so:

SELECT department, sum(price)
FROM products
GROUP BY department;
What are the minimum, maximum, and average prices per department?
Data query language offers many built-in aggregate functions that you can use to calculate information about the data in your database.

For example, expanding on the earlier query, you can add min(price), max(price), and avg(price) to the query and calculate all of these values simultaneously.

Run the following query:

SELECT department, count(*), sum(price), min(price), max(price), avg(price)
FROM products
GROUP BY department
ORDER BY department;
*** 33.8 Updating and deleting data
Updating and deleting data
1.5 hoursEstimated completion time
Learning Objective
By the end of this lesson, you will be able to use the UPDATE statement to update existing data in a table. You'll also be able to use the DELETE statement to delete existing data in a table.

Overview
Eventually, you will learn that the only constant in software is change. You can be sure that, over time, the data in the database will have to be changed or removed. In this lesson, you will focus on data manipulation language statements to change or remove data. If you want to change or remove data in the database, you need to use the SQL keywords UPDATE and DELETE.

Starter code
This lesson builds upon the work of earlier lessons. If you haven't successfully completed the work from the earlier lessons, go back and do that now.

If you are having trouble, reach out for assistance.

Employees
To fully exercise the range of options available in the UPDATE statement, you will use an existing dataset of employees. This dataset contains hundreds of rows.

You can create the new table named employees and import all of the data into the table by running one script.

Do this
Execute the employees.sql script
Download the SQL file named employees.sql and save it in some location on your computer. To do so, you can right-click the link and choose Save As. The script, when executed, will create an employees table and add data to it.

Once the script is done, continue with this lesson.

Updating data
Once data has been inserted into rows within the database, those rows can have one or more of their column values modified through use of the UPDATE statement. Column values may be updated either with constants, identifiers to other datasets, or expressions. They may apply to an entire column or a subset of a column's values through specified conditions. The UPDATE statement uses the following syntax:

UPDATE table_name
SET column_name_1 = <value_1>,
    column_name_n = <value_n>;
Do this
Set country to USA
All existing employee records have the country as United States, and you have decided that you would rather use the three-character country code USA.

Run the following SQL to update the country:

UPDATE employees
SET country = 'USA';
The above SQL sets the value of country to USA for every row in the table. Verify that the change was successful by running the following SQL:

SELECT country
FROM employees;
The results from the above query should show that the country is USA for every record.

DEFAULT
Using the keyword DEFAULT in place of a specific value will set the value to the default value for that particular column in the table. If no default value has been set for the column, the column will be set to NULL.

Do this
Set fax to DEFAULT
Now run the following SQL to set the fax column to DEFAULT. Note that the fax column does not have a specific default value, so this will result in the fax value being set to NULL. This is an acceptable update because it is no longer 1995, and nobody uses fax machines anymore.

UPDATE employees
SET fax = DEFAULT;
UPDATE with the WHERE clause
Sometimes you won't want to update every row in the table. To restrict the rows that are updated by a UPDATE statement, add the WHERE clause. The syntax for the WHERE clause is the same no matter where it is used.

The syntax for an UPDATE query with a WHERE clause is as follows:

UPDATE table_name
SET column_name_1 = <value_1>,
    column_name_n = <value_n>
WHERE <conditions>;
Do this
Update reports_to
At your company, Willie Lyons decided to quit and become a full-time PostgreSQL instructor. As a result, all of the employees that reported to Willie Lyons will now report to a newly hired manager named Fibber McGee.

Run the following SQL to update only the employee records where reports_to = 'Willie Lyons'.

UPDATE employees
SET reports_to = 'Fibber McGee'
WHERE reports_to = 'Willie Lyons';
Do this
Set multiple column values
Someone made a data-entry error; they entered two employees with the job title of Chemical Engineer living in Trenton. These employees actually should have been entered as Chemical Engineer II living in Lawrenceville.

Run the following SQL to correct the data-entry error:

UPDATE employees
SET job_title = 'Chemical Engineer II', city = 'Lawrenceville'
WHERE job_title = 'Chemical Engineer' AND city = 'Trenton';
You will see that two rows were updated.

Deleting data
When managing data in a database, it may be necessary to delete some rows from a table. In this case, the DELETE statement can be used to remove some of the existing records from the table without deleting the table itself.

DELETE statement
The DELETE statement doesn't require a WHERE clause. However, you almost never want to delete every row in the table, so it's best to get into the habit of always including a WHERE clause in a DELETE statement to restrict the rows that are deleted. Without a WHERE clause, the DELETE statement will delete every row in the table.

The syntax for an DELETE query with a WHERE clause is as follows:

DELETE FROM <table_name>
WHERE <conditions>
Do this
Delete the VPs of sales
It turns out that there was another data-entry error: several candidates who interviewed for the VP of Sales job were entered as employees, but only the VP of Sales reporting to Adele Gonzalez was actually hired.

Note: Treat every DELETE statement with caution. Always test the WHERE clause by first running it as a SELECT * statement to make sure that the correct records will be deleted.

To test the WHERE clause, run the following query:

SELECT * FROM employees
WHERE job_title = 'VP Sales' AND reports_to <> 'Adele Gonzalez';
After running the above SQL, you will see four rows returned.

Finally, once you are sure that the WHERE clause is correct, change SELECT * to DELETE and run the following query:

DELETE FROM employees
WHERE job_title = 'VP Sales' AND reports_to <> 'Adele Gonzalez';
You will see that four rows were deleted.

TRUNCATE statement
The DELETE statement is most useful when you want to clear only some of the data from a table.

TRUNCATE, on the other hand, is useful for clearing all the data at once.

Using a TRUNCATE statement is faster than using DELETE. This is because TRUNCATE removes all rows in the table without scanning every row, unlike the DELETE statement. In addition, the TRUNCATE statement reclaims the storage space right away after removing all of the data.

If you are removing all rows from a table, use the TRUNCATE statement.

The TRUNCATE TABLE statement uses the following syntax:

TRUNCATE TABLE <table_name>;
Do this
Truncate the products table
Run the following SQL to delete all rows in the products table:

TRUNCATE TABLE products;
Truncate multiple tables
The TRUNCATE statement can be used to remove all the data from multiple tables using the following syntax:

TRUNCATE TABLE <table_name_1>, <table_name_2>, …;
Do this
Truncate the articles and employees tables
Run the following SQL to delete all rows in the articles and employees tables:

TRUNCATE TABLE articles, employees;
*** 33.9 Assessment: Postgres

** Creating Relations - Module 34
*** 34.1 Overview: Creating relations
Overview: Creating relations
9 minutesEstimated completion time
Overview
In this module, you will learn about common data relationships that you are likely to encounter in your daily work as a developer, including one-to-one, one-to-many, and many-to-many relationships. You'll learn how to create tables based on these types of relationships, and you will learn how to visually represent these relationships using special database diagrams called Entity Relationship Diagrams.

When you're working as a developer, you will often need to write code that is designed to create, read, update, or delete (CRUD) data. For example, if a user registers on a website, the information that they enter will need to be stored in one or more databases. As data storage increases, more and more databases will need to be created and linked. This module is titled Creating relations because it is all about how to find and use relationships between two or more databases so that data can be manipulated and displayed in the ways that customers and businesses need.

In this module, you will write cross-table queries to find answers to several key administrative and operational questions for your imaginary bookstore business, Thinkful Books.

Do this
The purpose of the Do this sections in this module is to give you important hands-on experience. In these sections, you'll perform various tasks, like setting up a development environment or executing a command. Ultimately, these practice sections will help you successfully complete graded assessments, such as projects, mock interviews, and capstones.

*** 34.2 Database relationships
Database relationships
1.5 hoursEstimated completion time
Learning Objective
By the end of this lesson, you will be able to describe one-to-one, one-to-many, and many-to-many relationships. You'll also be able to use Entity Relationship Diagrams to visualize how tables relate to each other.

Overview
In this lesson, you'll explore data relationships. You'll learn about one-to-one, one-to-many, and many-to-many relationships, and you'll brainstorm the data relationships that are needed for your Thinkful Books database design.

Key Terms
One-to-one relationship
A relationship where a single record in one table is related to a single record in another table, and neither column has duplicate records
One-to-many relationship
A relationship where a single record in one table is related to multiple records in another table
Many-to-many relationship
A relationship where multiple records in one table are related to multiple records in another table
Entity Relationship Diagram
ERD, a diagram that lets database designers visualize the tables and the relationships between the tables in a database
In previous lessons, you wrote queries that queried data within a single table. However, one of the key advantages of a relational database is its ability to execute complex queries that link or relate data located in more than one table.

Linking your data allows you to derive rich insights from your data. An e-commerce store owner, for example, can inform a sales campaign by leveraging database relationships to retrieve all of the products sold to customers over a certain time period. They could then use this information to recommend related products to the customers.

Determining the appropriate relationships for your data requires an understanding of their business rules and applications. That said, you are likely to encounter three types of data relationships: one-to-one, one-to-many, and many-to-many.

Currently, all of the business records at Thinkful Books are recorded in a physical notebook. As the business has grown, however, this manual approach has become less feasible. So in this module, you'll migrate your data to PostgreSQL. In this lesson, you will brainstorm the data relationships that are needed for your database design.

One-to-one relationships
A one-to-one relationship is a relationship where each row in a table is connected to a single record in another table, and vice versa. Either table can be the primary table.

Examples of one-to-one relationships include the following:

A driver generally owns one license, and that license belongs to that driver only.

A student has only one student ID, and that ID is assigned to that student only.

Each person has only one social security number, and each social security number is linked to one person only.

A person generally has one name, and that name belongs to one person only.

At Thinkful Books, it is important to know who is managing what department. You currently maintain separate records for your employees and departments. Each manager manages exactly one department, and each department is only managed by one employee at any one time. That makes the "manages" relationship between employee and department a one-to-one relationship.

Because you were already maintaining separate records for your employees and departments, you decide to set up separate employees and departments tables for your database.

Because each department should have a manager, and each employee is not necessarily a manager, it makes sense to have a column in the departments table that contains the manager of that department. This column should include a foreign key that references the primary key (employee_id) of the employees table. Additionally, this employee_id foreign key needs to be unique for a given department, because each department can only be managed by a single employee.

In the tables below, an employee's record in the employees table relates only to a single record in the departments table, and vice versa, via a foreign key.

The employees table design
The employees table will have the following columns:

Column name
Description
Data type
Max length?
Allow null?
Primary key
Unique?
employee_id
Primary key for the table
integer
N/A
Yes
Yes
first_name
Employee's first name
varchar
100
No
last_name
Employee's last name
varchar
100
No
phone
Employee's phone number
varchar
100
Yes
title
Employee's job title
varchar
100
Yes
salary
Employee's salary
numeric
Yes
hire_date
Employee's date of hire
date
Yes
Sample employees table
employee_id
first_name
last_name
phone
title
salary
hire_date
1
Jane
Doe
5551234
CEO
80000
1999-12-07
2
Wright
Palmer
5554321
Manager
60000
2001-01-06
3
Jim
Doe
5555678
Accountant
50000
2015-11-06
The departments table design
The departments table will have the following columns:

Column
Description
Data type
Max length?
Allow null?
Primary key
Unique?
department_id
Primary key for the table
integer
N/A
Yes
Yes
department_name
Name of the department
varchar
100
No
manager
Foreign key from the employees table
integer
Yes
Yes
Sample departments table
department_id
department_name
manager
1
Administration
2
2
Merchandising
7
3
Customer Service
8
4
Marketing
9
One-to-many relationships
A one-to-many relationship is a relationship where each record in a table can relate to zero, one, or many records in another table. The table on the "one" side of the relationship is the primary table, whereas the table on the "many" side is the related table.

Examples of one-to-many relationships include the following:

A state can have many cities, and many cities can belong to the same state.

A city can have many zip codes, and many zip codes can belong to the same city.

An artist can have many songs, and many songs belong to the same artist.

How many orders can a customer have at Thinkful Books? And how many customers can belong to an order? If the answer is that an order may only belong to one customer and that a customer may have many orders, then you are talking about a one-to-many relationship. You decide to keep track of your customers' orders in two separate tables in your database: customers and orders.

For a given customer, the customer could potentially have no related records in orders. Or, there might be one record or many records. Additionally, those records can only be linked to a single customer. Your customers table is the primary table in this case, because it is on the "one" side of the relationship.

To implement a one-to-many relationship, create a foreign key column on the "many" side of the relationship to the primary key column on the "one" side of the relationship. So you create a foreign key column in the orders table that references the primary key of the customers table.

The customers table design
The customers table will have the following columns:

Column
Description
Data type
Max length?
Allow null?
Primary key
customer_id
Primary key for the table
integer
N/A
Yes
first_name
Customer's first name
varchar
100
No
last_name
Customer's last name
varchar
100
No
phone
Customer's phone number
varchar
100
Yes
email
Customer's email
varchar
100
Yes
street
Customer's street address
varchar
255
Yes
city
Customer's current city
varchar
100
Yes
zip_code
Customer's zip code
varchar
5
Yes
Sample customers table
customer_id
first_name
last_name
phone
email
1
Kacie
Mckee
5555234
kacie.mckee@gmail.com
2
Moses
Mcghee
5554651
moses.mcghee@gmail.com
3
Jerome
Aguilar
5555699
jerome.aguilar@yahoo.com
The orders table design
The orders table will have the following columns:

Column
Description
Data type
Max length?
Allow null?
Primary key
order_id
Primary key for the table
integer
N/A
Yes
order_status
1 = Pending; 2 = Processing; 3 = Rejected; 4 = Completed
integer
Yes
order_date
The date the order was placed
date
No
shipped_date
The date the order was shipped
date
Yes
customer_id
Foreign key from the customers table
integer
Yes
Sample orders table
order_id
order_status
order_date
shipped_date
customer_id
1
1
2020-03-04
1
2
2
2020-03-14
2
3
4
2020-04-02
2020-04-05
1
4
4
2020-04-05
2020-04-14
3
Many-to-many relationships
A many-to-many relationship is a relationship where multiple records in a table are related to multiple records in another table.

Examples of many-to-many relationships include the following:

A doctor can have many patients, and a patient can see many doctors.

A customer can purchase many products, and each product can be purchased by many customers.

A book may have one or more authors, and an author may have written multiple books.

Employees at Thinkful Books may simultaneously be assigned to different projects at the bookstore. Each project can involve more than one employee, and each employee can be working on more than one project. This scenario constitutes a many-to-many relationship. You decide to keep track of your projects in a projects table with the following columns:

Column
Description
Data type
Max length?
Allow null?
Primary key
project_id
Primary key for the table
integer
N/A
Yes
project_name
The project's name
varchar
255
No
budget
The project's budget
numeric
Yes
start_date
The project's start date
date
Yes
end_date
The project's actual end date, if applicable
date
Yes
To create a many-to-many relationship, you need to create an intermediate table, called a join table. A join table contains two foreign key columns to store the relationships between the two tables.

Specifically, you need to create a join table to track the many-to-many relationships in the employees and projects tables. The join table will also track any columns that describe the relationship of the employee to the project, such as the project assignment start and end dates.

The employees_projects table design
The join table, employees_projects, will have the following columns:

Column
Description
Data type
Allow null?
employee_id
Foreign key from the employees table
integer
Yes
project_id
Foreign key from the projects table
integer
Yes
start_date
Project assignment start date
date
Yes
end_date
Project assignment end date
date
Yes
Sample employees_projects table
employee_id
project_id
start_date
end_date
7
1
2020-03-04
2020-06-01
6
2
2020-11-20
2020-12-25
7
3
2020-04-06
2020-04-12
4
4
2020-02-11
2020-02-15
Entity Relationship Diagrams
Database designers typically use a diagram called an Entity Relationship Diagram (ERD) when they are creating a new database. An ERD is a diagram that allows database designers to visualize the tables and the relationships between the tables in a database. ERDs are fairly common because they provide an easy way to communicate to others about a database model that's being built.

An ERD for the tables described above may look like this:

An example ERD that allows database designers to visualize the tables and their relationships in a database.
The ERD above consists of the following components:

Each rectangle represents a table in the database.

The columns (or attributes) of the tables are listed, and a note may be made of any primary keys and foreign keys in the table.

Relationships are depicted by the lines joining the tables. There are two kinds of arrows, which are used to depict different kinds of relationships.

The following arrow represents the "one" side of the relationship:

Arrow representing the "one" side of the relationship.
The following arrow represents the "many" side of the relationship:

Arrow representing the "many" side of the relationship.
Bolded keys represent the primary keys in a table

Italicized keys represent the foreign keys in a table

Here are a few tools that you can use to create ERDs:

Lucidchart

Gliffy

Creately

This lesson illustrates the most common decisions that you may face when designing databases. Of course, many more complicated scenarios can arise, leading to fairly complex designs, but they would most likely be similar to the decision-making processes illustrated in this lesson.

In the next lesson, you will write queries to create the tables and data relationships described in this lesson.
*** 34.3 Relating tables through keys
Relating tables through keys
1.5 hoursEstimated completion time
Learning Objective
By the end of this lesson, you will be able to create complex associations between tables using primary and foreign keys.

Overview
In the previous lesson, you learned about the different kinds of relationships that can exist in your data, including one-to-one, one-to-many, and many-to-many relationships. You also learned how to choose the appropriate relationships for your data. You might be wondering about the syntax involved in creating these data relationships and how you could translate an ERD into actual SQL tables.

Key Terms
Composite key
A primary key created from combining multiple columns to guarantee the uniqueness of a record
Create relationships with SQL
Here is the ERD that you saw in the previous lesson:

An example ERD that allows database designers to visualize the tables and their relationships in a database.
You will translate the model depicted in the ERD above into SQL.

Before creating each table, it's best to delete it first, just in case the table already exists in your database.

Do this
Set up a database for Thinkful Books using ElephantSQL
Refer to the instructions in the previous module to set up a new instance called thinkful_books on ElephantSQL. Connect to your instance from DBeaver, and rename your database connection to thinkful_books for easy reference.

Execute the scripts in this lesson with DBeaver.

One-to-one relationship between the employees and departments tables
Recall that one employee can manage one department at most. Here, you will create a one-to-one relationship between the employee and the department records.

Do this
Create the employees table
Execute the following script to create the employees table:

DROP TABLE if exists employees;

CREATE TABLE employees
(
  employee_id INTEGER PRIMARY KEY generated by default AS identity,
  first_name VARCHAR(100) NOT NULL,
  last_name VARCHAR(100) NOT NULL,
  phone VARCHAR(100),
  title VARCHAR(100),
  salary NUMERIC,
  hire_date DATE
);
Do this
Create the departments table
Next, you can create the departments table. You will need to add a foreign key column in the departments table that references the employees table. The basic syntax is as follows:

<column name> <data type> REFERENCES <foreign table name>(<foreign column>)
Execute the following script to create the departments table:

DROP TABLE if exists departments;

CREATE TABLE departments
(
  department_id INTEGER PRIMARY KEY generated by default AS identity,
  department_name VARCHAR(100) NOT NULL,
  manager INTEGER REFERENCES employees(employee_id) unique -- Foreign key
);
The manager column stores the foreign keys from the employees table. Setting it to unique ensures that each department is linked to only one manager, thereby enforcing a one-to-one relationship.

One-to-many relationship between the customers and orders tables
Recall that a customer can have multiple orders, and the orders can only belong to a single customer. Here, you will create a one-to-many relationship between the customer (the "one" side of the relationship) and the order records (the "many" side of the relationship).

Do this
Create the customers table
Execute the following script to create the customers table:

DROP TABLE if exists customers;

CREATE TABLE customers
(
  customer_id INTEGER PRIMARY KEY generated by default AS identity,
  first_name VARCHAR(100) NOT NULL,
  last_name VARCHAR(100) NOT NULL,
  phone VARCHAR(100),
  email VARCHAR(100),
  street VARCHAR(255),
  city VARCHAR(100),
  zip_code VARCHAR(5)
);
Do this
Create the orders table
To create a one-to-many relationship between the customers and orders tables, you would have to create a foreign key reference in the orders table to the customers table. Execute the following script to create the orders table:

DROP TABLE if exists orders;

CREATE TABLE orders
(
  order_id INTEGER PRIMARY KEY generated by default AS identity,
	order_status INTEGER NOT NULL,
  order_date DATE NOT NULL,
	shipped_date DATE,
  customer_id INTEGER REFERENCES customers(customer_id) NOT NULL -- Foreign key
);
In the query above, you call the foreign key column customer_id and have it reference the primary key column of the customers table (which is customer_id).

Many-to-many relationship between the employees and projects tables
Thinkful Books employees can work on multiple projects at the same time, and each project may involve several employees. Here, you will create many-to-many relationships between the employee and project records.

Do this
Create the projects table
Execute the following script to create the projects table:

DROP TABLE if exists projects;

CREATE TABLE projects
(
  project_id INTEGER PRIMARY KEY generated by default AS identity,
  project_name VARCHAR(255) NOT NULL,
  budget NUMERIC DEFAULT 0,
  start_date DATE,
  end_date DATE
);
Do this
Create the employees_projects join table
Next, create a join table to store the relationships between the employee and project records. Execute the following script to create the employees_projects join table:

DROP TABLE if exists employees_projects;

CREATE TABLE employees_projects
(
  employee_id INTEGER REFERENCES employees(employee_id),
  project_id INTEGER REFERENCES projects(project_id),
  start_date DATE,
  end_date DATE,
  PRIMARY KEY (employee_id, project_id) -- Composite key
);
Each row in the employees_projects table links an employee to a project through two foreign key columns that reference primary keys from the employees and projects tables.

Additionally, combining the employee_id and project_id columns makes it possible to create a composite key. A composite key is a primary key created from combining multiple columns to guarantee the uniqueness of a record. Composite keys are useful when records in your dataset cannot be uniquely identified by a single column value but a combination of column values can.

Testing the database
Now, you're ready to seed the database with some data so that you can test it. Then, you will test the one-to-one, one-to-many, and many-to-many associations that you set up in the earlier steps.

Truncate a table
Before you add any data to your database, you should remove any existing data to ensure that the database remains in a consistent state. You can use the TRUNCATE command to do this. As you learned in the previous module, the TRUNCATE command lets you remove all the records stored in a table, without deleting the table itself.

For example, if you want to clear the records from tables tableA, tableB, and tableC, you can run the following:

TRUNCATE
  tableA,
  tableB,
  tableC
RESTART IDENTITY;
The RESTART IDENTITY clause is helpful if you have primary key fields in the tables that you want to restart at 1 when you start adding new records to the tables again.

Do this
Seeding the database
Take some time to understand what the script below does, then execute the script to seed your database with data:

TRUNCATE
  employees,
  departments,
  projects,
  employees_projects,
  customers,
  orders
RESTART IDENTITY;

-- Insert some data into the employees table
INSERT INTO employees
  (first_name, last_name, phone, title, salary, hire_date)
  VALUES
    ('Jane', 'Doe','5551234', 'CEO', 80000, '12/07/1999'),
    ('Wright', 'Palmer', '5554321', 'Manager', 60000, '01/06/2001'),
    ('Jim', 'Doe', '5555678', 'Accountant', 50000, '11/06/2015'),
    ('Toby', 'Bestley', '5558765', 'Associate', 35000, '09/07/2008') ,
    ('Meredith', 'Hartford', '5559876', 'Associate', 30000, '02/17/2014'),
    ('Tom', 'Flenderson', '5558769', 'Associate', 32000, '11/23/2013'),
    ('Bently', 'Singh', '5554326', 'Manager', 60000, '06/06/2005'),
    ('Winnie', 'Lim', '5554527', 'Manager', 62000, '10/24/2003'),
    ('Ruda', 'Bross', '5554428', 'Manager', 66000, '11/06/2004');

-- Insert four projects into the projects table
INSERT INTO projects
  (project_name, budget, start_date)
  VALUES
    ('Build Database', 20000, '3/4/2020'),
    ('Plan christmas party', 500, '11/20/2020'),
    ('Remove old stock', 1000, '4/6/2020'),
    ('Watch paint dry', 3000, '2/11/2020');

-- Insert some customers into the customers table
INSERT INTO customers
  (first_name, last_name, phone, email, street, city, zip_code)
  VALUES
    ('Kacie', 'Mckee','5555234', 'kacie.mckee@gmail.com', '61 Shadow Brook Court', 'Melrose, MA', '02176'),
    ('Moses', 'Mcghee', '5554651', 'moses.mcghee@gmail.com', '570 Old York St.', 'Upland, CA', '91784'),
    ('Jerome', 'Aguilar', '5555699', 'jerome.aguilar@yahoo.com', '68 Victoria Road', 'Hoboken, NJ', '07030'),
    ('Ainsley', 'Bonner', '5558564', 'ainsley.bonner@hotmail.com', '60 Winchester Road', 'Dundalk, MD', '21222') ,
    ('Delilah', 'Bateman', '5523124', 'delilah.bateman@gmail.com', '482 E. Oxford St.', 'Thibodaux, LA', '70301');
If all goes well, you will see the following data in your employees, projects, and customers tables when you select all records from each table:

The employees table
employee_id
first_name
last_name
phone
title
salary
1
Jane
Doe
5551234
CEO
80000
2
Wright
Palmer
5554321
Manager
60000
3
Jim
Doe
5555678
Accountant
50000
4
Toby
Bestley
5558765
Associate
35000
5
Meredith
Hartford
5559876
Associate
30000
6
Tom
Flenderson
5558769
Associate
32000
7
Bently
Singh
5554326
Manager
60000
8
Winnie
Lim
5554527
Manager
62000
9
Ruda
Bross
5554428
Manager
66000
The customers table
customer_id
first_name
last_name
phone
email
street
city
zip_code
1
Kacie
Mckee
5555234
kacie.mckee@gmail.com
61 Shadow Brook Court
Melrose, MA
02176
2
Moses
Mcghee
5554651
moses.mcghee@gmail.com
570 Old York St.
Upland, CA
91784
3
Jerome
Aguilar
5555699
jerome.aguilar@yahoo.com
68 Victoria Road
Hoboken, NJ
07030
4
Ainsley
Bonner
5558564
ainsley.bonner@hotmail.com
60 Winchester Road
Dundalk, MD
21222
5
Delilah
Bateman
5523124
delilah.bateman@gmail.com
482 E. Oxford St.
Thibodaux, LA
70301
The projects table
project_id
project_name
budget
start_date
end_date
1
Build Database
20000
2020-03-04
2
Plan christmas party
500
2020-11-20
3
Remove old stock
1000
2020-04-06
4
Watch paint dry
3000
2020-02-11
Do this
Testing the one-to-one relationship between the employees and departments tables
First, execute the script below to insert four departments into the departments table:

INSERT INTO departments
  (department_name, manager)
  VALUES
    ('Administration', 2),
    ('Merchandising', 7),
    ('Customer Service', 8),
    ('Marketing', 9);
Then, execute the script below to insert another record for a department. Here, you are using the employee with employee_id of 2. This employee is already managing the Administration department:

INSERT INTO departments (department_name, manager) VALUES ('New Department', 2);
If you are using DBeaver, you will see an error telling you that a unique key constraint on the manager foreign key has been violated:

An error in DBeaver telling you that a unique key constraint on the manager foreign key is violated.
An employee cannot manage more than one department!

Do this
Testing the one-to-many relationship between the customers and orders tables
First, execute the script below to insert multiple records into the orders table for the same customers:

INSERT INTO orders
  (order_status, order_date, shipped_date, customer_id)
  VALUES
    (1, '3/4/2020', NULL, 1),
    (2, '3/14/2020', NULL, 2),
    (4, '4/2/2020', '4/5/2020', 1),
    (4, '4/5/2020', '4/14/2020', 3),
    (3, '4/5/2020', NULL, 3),
    (4, '4/17/2020', '4/20/2020', 4),
    (3, '5/1/2020', NULL, 1),
    (4, '5/2/2020', '5/14/2020', 5),
    (3, '5/15/2020', NULL, 2);
You won't encounter any errors when you execute the script above.

Next, execute SELECT * FROM orders;, and you will see the following table:

The orders table
order_id
order_status
order_date
shipped_date
customer_id
1
1
2020-03-04
1
2
2
2020-03-14
2
3
4
2020-04-02
2020-04-05
1
4
4
2020-04-05
2020-04-14
3
5
3
2020-04-05
3
6
4
2020-04-17
2020-04-20
4
7
3
2020-05-01
1
8
4
2020-05-02
2020-05-14
5
9
3
2020-05-15
2
Notice there are several orders linked to the customer with a customer_id of 1, because a customer can have multiple orders in the database. Great!

Do this
Testing the many-to-many relationship between the employees and projects tables
First, execute the script below to insert multiple records into the employees_projects table:

-- Assign employees to projects
INSERT INTO employees_projects
  (employee_id, project_id, start_date, end_date)
  VALUES
    (7, 1, '3/4/2020', '6/1/2020'),
    (6, 2, '11/20/2020', '12/25/2020'),
    (7, 3, '4/6/2020', '4/12/2020'),
    (4, 4, '2/11/2020', '2/15/2020'),
    (7, 4, '2/25/2020', '3/15/2020'),
    (2, 4, '2/11/2020', '2/25/2020'),
    (1, 4, '2/15/2020', '4/12/2020');
You won't encounter any errors when you execute the script above.

Next, execute SELECT * FROM employees_projects;, and you will see the following table:

The employees_projects table
employee_id
project_id
start_date
end_date
7
1
2020-03-04
2020-06-01
6
2
2020-11-20
2020-12-25
7
3
2020-04-06
2020-04-12
4
4
2020-02-11
2020-02-15
7
4
2020-02-25
2020-03-15
2
4
2020-02-11
2020-02-25
1
4
2020-02-15
2020-04-12
Notice that the employee with employee_id of 7 is assigned to projects with project_id values of 1, 3, and 4.

Notice also that the project with project_id of 4 involves employees with employee_id values of 1, 2, 4, and 7.

Multiple employees can be linked to multiple projects, and vice versa. Great!

Do this
Checking the ERD in DBeaver
Now that you have created a few tables with relationships, you can use DBeaver to generate an ERD for you. Under the public folder, right-click Tables and select View Diagram from the context menu. Use DBeaver to examine the tables that have been created. Does the ERD in DBeaver look similar to the ERD from the previous lesson?

The ERD in DBeaver.
*** 34.4 Joining tables
Joining tables
1.5 hoursEstimated completion time
Learning Objective
By the end of this lesson, you will be able to perform a JOIN query to retrieve data from multiple tables.

Overview
In this lesson, you'll learn how to query data from multiple tables.

In the previous lesson, you broke up your Thinkful Books data over several tables. Until now, you've been used to querying single tables for data. For example, to view a department list, you could use this SQL query:

SELECT * FROM departments;
Then, you would get the following results:

department_id
department_name
manager
1
Administration
2
2
Merchandising
7
3
Customer Service
8
4
Marketing
9
Although this is fine, it would be more useful to have the manager's name rather than just the manager's ID. However, the name is stored in another table. Also, given your current database, how would you answer a question like the following: who are all the employees and their salaries who work on project X?

To accomplish more complex tasks like these, you need to query data from multiple tables.

Starter code
This lesson builds on the work of the earlier lessons. If you haven't successfully completed the work from the earlier lessons, go back and do that now.

JOIN queries
Do this
List the departments at Thinkful Books and their managers, including all columns
To query data from multiple tables, you need to join the tables. Tables that are related by a foreign key can be joined using the basic syntax below:

SELECT <columns>
FROM <table1>
  JOIN <table2>
  ON <related columns>;
To say the above in plain English—this query is first asking for data from a column (or columns) from table1 (SELECT...FROM). It then asks for data from a different table (table2) to be connected to these columns from the first table. The ON clause states that the data from table2 should only be pulled in where it has a relationship to the data from table1. Now, look at an actual example.

Suppose you are interested in viewing a list of departments and their managers. In that case, you can join the departments table with the employees table as follows:

SELECT *
FROM departments
  JOIN employees
  ON departments.manager = employees.employee_id;
In this query, you specify that you want all rows of data where the manager ID in the departments table matches the employee_id in the employees table. Because you specified SELECT *, all columns from both tables will be returned.

Executing the query above yields the following result:

department_id
department_name
manager
employee_id
first_name
last_name
phone
title
salary
hire_date
1
Administration
2
2
Wright
Palmer
5554321
Manager
60000
2001-01-06
2
Merchandising
7
7
Bently
Singh
5554326
Manager
60000
2005-06-06
3
Customer Service
8
8
Winnie
Lim
5554527
Manager
62000
2003-10-24
4
Marketing
9
9
Ruda
Bross
5554428
Manager
66000
2004-11-06
Do this
List the departments at Thinkful Books and their managers, including only selected columns
Instead of returning all columns from both tables, it is best practice to explicitly list the column names in a SELECT statement and return only those columns from the query. Doing so reduces the number of columns that need to be processed by the system to satisfy the query.

Suppose you are conducting an annual salary review for your departmental managers, and you only need data from the following columns:

department_id and department_name from the departments table

first_name, last_name, and salary from the employees table

-- It is also recommended to qualify all column names with the table name;
-- that is, `employees.salary` rather than just `salary`.
SELECT
  departments.department_id,
  departments.department_name,
  employees.first_name,
  employees.last_name,
  employees.salary
FROM
  departments
  JOIN employees
  ON departments.manager = employees.employee_id;
Executing the query above yields the following result:

department_id
department_name
first_name
last_name
salary
1
Administration
Wright
Palmer
60000
2
Merchandising
Bently
Singh
60000
3
Customer Service
Winnie
Lim
62000
4
Marketing
Ruda
Bross
66000
Do this
Create table aliases
Notice the previous query was quite verbose. You can make use of table aliases to help reduce the number of keystrokes needed to type such queries. The table alias only exists within the query itself. To create an alias, simply specify the new alias in the FROM clause.

SELECT
  d.department_id,
  d.department_name,
  e.first_name,
  e.last_name,
  e.salary
FROM
  departments d -- Create alias `d` for the departments table
  JOIN employees e -- Create alias `e` for the employees table
  ON d.manager = e.employee_id;
This is the same query as before, but you created the alias d for the departments table and e for the employees table. This is done in the FROM clause but used throughout, even in the SELECT clause.

Try running the query above and see if it still yields the same result!

INNER JOIN queries
The default type of JOIN query is the INNER JOIN. This query returns all rows where the two columns in the join condition match.

Do this
In the previous queries, the four departments in the departments table were matched to the corresponding managers in the employees table. But what happens if there is a department with no manager? You can check what happens by creating a new department without a manager:

INSERT INTO departments
  (department_name)
VALUES
  ('Sales');
Now there are four departments with managers and a fifth department without a manager. To list all departments, execute the following script:

SELECT * FROM departments;
And the results show that the Sales department has a NULL for the manager column. That means that there is no corresponding row in the employees table.

department_id
department_name
manager
1
Administration
2
2
Merchandising
7
3
Customer Service
8
4
Marketing
9
5
Sales
You can try to list all departments and the names of their managers:

SELECT
  d.department_id,
  d.department_name,
  e.first_name,
  e.last_name
FROM
  departments d
  JOIN
  employees e
  ON
    d.manager = e.employee_id;
That's essentially the same query from above, except you want only the names of the managers. But the results are as follows:

department_id
department_name
first_name
last_name
1
Administration
Wright
Palmer
2
Merchandising
Bently
Singh
3
Customer Service
Winnie
Lim
4
Marketing
Ruda
Bross
Where is the Sales department? Because the Sales department isn't associated with a manager, the record for that department won't be included in the result of the inner join query.

As mentioned before, the default type of a join query is an INNER JOIN. You can replace the JOIN clause in the previous query with INNER JOIN:

SELECT
  d.department_id,
  d.department_name,
  e.first_name,
  e.last_name
FROM
  departments d
  INNER JOIN
  employees e
  ON d.manager = e.employee_id;
You should get the same result as before!

INNER JOIN queries are the default because they are fairly common and satisfy the requirements for most queries.

You will try out a few more JOIN queries next.

Do this
List the order statuses, order dates, first names, and emails of all Thinkful Books customers
Suppose you would like to write an email to give each of your customers an update on their orders. To write the email, you need their order statuses, order dates, first names, and email addresses.

To get this information, you need to join the customers table to the orders table using the query below:

SELECT
  c.first_name as customer_name, -- Alias `first_name` as `customer_name`
  c.email,
  o.order_status,
  o.order_date
FROM
  customers c
  INNER JOIN
  orders o
  ON c.customer_id = o.customer_id;
The INNER JOIN matches all rows in the customers table to the rows in the orders table on the customer_id foreign key, yielding the following result:

customer_name
email
order_status
order_date
Kacie
kacie.mckee@gmail.com
1
2020-03-04
Moses
moses.mcghee@gmail.com
2
2020-03-14
Kacie
kacie.mckee@gmail.com
4
2020-04-02
Jerome
jerome.aguilar@yahoo.com
4
2020-04-05
Jerome
jerome.aguilar@yahoo.com
3
2020-04-05
Ainsley
ainsley.bonner@hotmail.com
4
2020-04-17
Kacie
kacie.mckee@gmail.com
3
2020-05-01
Delilah
delilah.bateman@gmail.com
4
2020-05-02
Moses
moses.mcghee@gmail.com
3
2020-05-15
Do this
List the order dates, first and last names, and phone numbers of Thinkful Books customers with rejected orders
Suppose you would like to call your customers whose orders were rejected (the orders with order_status of 3) to try to encourage them to submit a new order. To write this email, you will need your customers' order dates, their first and last names, and their phone numbers.

Again, to get this information, you need to join the customers table to the orders table, while using the WHERE clause to filter the rows to those whose order_status is 3.

SELECT
  c.first_name,
  c.last_name,
  c.phone,
  o.order_date
FROM
  customers c
  INNER JOIN
  orders o
  ON c.customer_id = o.customer_id
WHERE
  o.order_status = 3;
Only customers with rejected orders are returned from the query. Notice that the order_status column isn't displayed. This is because even though it was used in the WHERE clause, it wasn't included in the SELECT clause, and therefore it isn't returned in the query.

first_name
last_name
phone
order_date
Jerome
Aguilar
5555699
2020-04-05
Kacie
Mckee
5555234
2020-05-01
Moses
Mcghee
5554651
2020-05-15
Do this
List the employee's first and last names and the employees' project names
The employees' first and last names are stored in the employees table, while the project name is stored in the projects table. However, these two tables aren't directly related via a foreign key. They are related by a third table, employees_projects. That means we have to join three tables!

Here is the query for joining all three tables:

SELECT
  e.first_name,
  e.last_name,
  p.project_name as project -- Alias the column name as `project` for brevity
FROM
  employees e
  INNER JOIN
  employees_projects ep
  ON e.employee_id = ep.employee_id
  INNER JOIN
  projects p
  ON p.project_id = ep.project_id;
First, the employee_id columns from the employees and employees_projects tables are compared and all matching rows are returned. You can imagine that a temporary table was created with the results stored internally by the system.

This temporary table is then joined to the projects table by the second join. The results are as follows:

first_name
last_name
project
Bently
Singh
Build Database
Tom
Flenderson
Plan christmas party
Bently
Singh
Remove old stock
Toby
Bestley
Watch paint dry
Bently
Singh
Watch paint dry
Wright
Palmer
Watch paint dry
Jane
Doe
Watch paint dry
Note that it is possible to join an arbitrary number of tables by having multiple JOIN clauses in your query!
*** 34.5 Other joins
Other joins
1.5 hoursEstimated completion time
Learning Objective
By the end of this lesson, you will be able to perform LEFT JOIN, RIGHT JOIN, and FULL JOIN operations to connect two tables.

Overview
Most of the time, you need a result set that is formed from combining data from several tables in your database. In the previous lesson, you learned how to perform an inner join that returns the rows with matching values in both tables. In this lesson, you will learn about outer joins. You'll get familiar with the different types of outer joins—LEFT JOIN, RIGHT JOIN, and FULL JOIN—that you can perform in PostgreSQL to query data for a variety of scenarios.

Starter code
This lesson builds upon the work of the earlier lessons. If you haven't successfully completed the work from the earlier lessons, go back and do that now.

If you are having trouble, reach out for assistance.

LEFT JOIN
Think about the case of the department with the missing manager from the previous lesson (the Sales department).

Suppose that you need a list of all departments and their corresponding manager names. Suppose also that there is a new requirement this time around: if a department doesn't have a manager, then return the record for that department anyway but put a NULL value in the corresponding manager column.

From the previous lesson, you know that the INNER JOIN below doesn't satisfy this query, because it excludes department records without managers.

SELECT
  d.department_id AS id,
  d.department_name AS department,
  e.first_name,
  e.last_name
FROM
  departments d
  JOIN -- Recall that the default `JOIN` type is an `INNER JOIN`
  employees e
  ON d.manager = e.employee_id;
Consider the query above diagrammatically. Because the departments table is listed first in the FROM clause, it is considered to be on the left of the join, whereas the employees table is on the right.

To include the departments with no managers, you would need to perform a LEFT JOIN. A LEFT JOIN brings back all rows on the left side of a join, even if it doesn't match any rows on the right. If no matches are found, the right-side columns are returned with NULL values.

Do this
List all department names at Thinkful Books and their corresponding managers
To perform a LEFT JOIN between the departments and employees tables, the query looks very similar to an INNER JOIN except that you would have to change the JOIN clause to LEFT JOIN:

SELECT
  d.department_id AS id,
  d.department_name AS department,
  e.first_name AS manager_first_name,
  e.last_name AS manager_last_name
FROM
  departments d
  LEFT JOIN -- Specify `LEFT JOIN` here
  employees e
  ON d.manager = e.employee_id;
This time, all departments are returned:

id
department
manager_first_name
manager_last_name
1
Administration
Wright
Palmer
2
Merchandising
Bently
Singh
3
Customer Service
Winnie
Lim
4
Marketing
Ruda
Bross
6
Sales
RIGHT JOIN
Suppose you are interested in viewing a list of all employee names and the name of the department that they manage. However, an INNER JOIN will return only employees who actually manage departments.

In this case, you can use a RIGHT JOIN. A RIGHT JOIN will include all rows on the right of a join, regardless of whether it matches a row on the left or not. In cases where it does not match, left columns are returned with NULL values.

Do this
List all Thinkful Books employee names and the name of the department that they manage
Execute the script below to perform a RIGHT JOIN:

SELECT
  e.first_name AS manager_first_name,
  e.last_name AS manager_last_name,
  d.department_id AS id,
  d.department_name AS department
FROM
  departments d
  RIGHT JOIN -- Specify `RIGHT JOIN` as the clause
  employees e
  ON d.manager = e.employee_id;
The query above results in the nine rows, one row for each employee:

manager_first_name
manager_last_name
id
department
Wright
Palmer
1
Administration
Bently
Singh
2
Merchandising
Wright
Lim
3
Customer Service
Ruda
Bross
4
Marketing
Meredith
Hartford
Tom
Flenderson
Toby
Bestley
Jane
Doe
Jim
Doe
As an interesting note, a RIGHT JOIN is the same as changing the order of the tables listed in a LEFT JOIN. This means that the following two queries will return the same results:

SELECT
  e.first_name AS manager_first_name,
  e.last_name AS manager_last_name,
  d.department_id AS id,
  d.department_name as department
FROM
  employees e
  LEFT JOIN -- Specify `LEFT JOIN` here
  departments d
  ON e.employee_id = d.manager;
SELECT
  e.first_name AS manager_first_name,
  e.last_name AS manager_last_name,
  d.department_id AS id,
  d.department_name AS department
FROM
  departments d
  RIGHT JOIN -- Specify `RIGHT JOIN` as the clause
  employees e
  ON d.manager = e.employee_id;
FULL JOIN
Suppose you are planning to promote an employee to become a manager, and to support your decision-making process, you need to view a list of all departments and all employees at Thinkful Books. You can leverage a FULL JOIN in this instance. A FULL JOIN collates the results of both LEFT JOIN and RIGHT JOIN operations. The FULL JOIN keyword returns all rows when there is a match in the left table (the departments table in this case) or right table (the employees table in this case) records.

Tip
Be careful when using OUTER JOIN operations, because they can potentially return enormous datasets.

The joined table will contain all records from both the tables, filling in NULL values for missing matches on either side.

Do this
List all departments and all employees at Thinkful Books
Execute the script below to list all departments and all employees:

SELECT
  d.department_id AS id,
  d.department_name AS department,
  e.first_name,
  e.last_name
FROM
  departments d
  FULL JOIN -- Specify `FULL JOIN` here
  employees e
  on d.manager = e.employee_id;
The query above returns the following result:

id
department
first_name
last_name
1
Administration
Wright
Palmer
2
Merchandising
Bently
Singh
3
Customer Service
Winnie
Lim
4
Marketing
Ruda
Bross
6
Sales
Meredith
Hartford
Tom
Flenderson
Toby
Bestley
Jane
Doe
Jim
Doe
There are several other types of JOIN queries, but these are most common and should satisfy most queries that a typical application needs. The SELECT documentation lists the different types of JOIN queries that are possible in PostgreSQL.
*** 34.6 Assessment: Creating relations
** Node, Express, and Postgres - Module 35
*** 35.1 Overview: Node, Express, and Postgres
Overview: Node, Express, and Postgres
9 minutesEstimated completion time
Overview
In this module, you'll learn how to use a SQL query builder to connect your Express server to a PostgreSQL database. This will allow you to store data for your application, so that you can retrieve the data stored in the database even after your server restarts.

So far, you have learned how to use Express to create a server that follows RESTful API design principles. However, at this point, your server has been either reading data from JSON files or storing items in memory. For example, the client would issue a POST request, and the server would save the new posted entity to an array in memory. If you were to restart the Express server, the posted item would be lost.

In the previous lessons, you also learned how to set up, connect to, and write complex SQL queries to perform CRUD operations on a PostgreSQL database hosted on ElephantSQL.

To persist (or store) data for your application, you can connect your Express server to a database, such as PostgreSQL. Then, when a client issues a request like GET, POST, or DELETE to the Express server, the server can connect to the database to manipulate the persisted data. On server restarts, you can still retrieve the data stored in the database.

In this module, you have been hired as a backend developer by Thinkful Gifts, an e-commerce store where "you can pick thoughtful gifts for thankful recipients." You have been tasked with setting up a database and building out an inventory API that provides the inventory management department with access to inventory data to help them make informed decisions for the business.

After some discussion with your manager and your team, you've decided to use Express, PostgreSQL, and a SQL query builder called Knex for the project. After determining the appropriate data relationships within your database, you are now ready to implement the API.

Do this
The purpose of the Do this sections in this module is to give you important hands-on experience. In these sections, you'll perform various tasks, like setting up a development environment or executing a command.
*** 35.2 Knex configuration
Knex configuration
1.5 hoursEstimated completion time
Learning Objective
By the end of this lesson, you will be able to create and customize a knexfile.js.

Overview
You are now ready to begin building out the inventory API. To connect your Express server with a PostgreSQL database, you will use a tool called Knex, which you will install, set up, and configure in this lesson. Knex is a SQL query builder library that you can use with different types of databases (not only PostgreSQL) with Node.js. It allows you to build complex SQL queries using JavaScript.

To understand how the query building process works, consider the following analogy: Imagine that a company's finance manager is using a complex GUI with multiple forms. The forms allow the finance manager to perform multiple actions on an account, such as the following:

Transfer money in

Transfer money out

Select the amount of money

Select the target and destination accounts

Select the currencies involved

Maybe even select multiple target accounts

Select dates the transactions take place

As the finance manager is filling out the forms, nothing actually happens until they click Submit. They're building a transaction by using the controls on the page, and once they've built the transaction, they can submit it.

The complex GUI is similar to a query builder that can configure a database query before performing it. Here's how this analogy maps out:

The finance manager is you, the developer.

The GUI is a Knex instance.

The financial transaction is the SQL query.

The options on GUI forms for building transactions are the Knex instance methods.

The Submit button on the GUI is like using then() off the query builder.

Starter code
This lesson requires you to have the following repository running on your local machine.

GitHub: Node, Express, and Postgres Starter
Follow the instructions in the README to get it to run.

Project dependencies: The knex and pg libraries
You will need to install the Knex library (knex). You'll also need to install the Node-Postgres database library (pg), which contains a collection of Node.js modules for interacting with your PostgreSQL database.

Do this
Install the knex and pg libraries
To install the dependencies, run the following command:

npm install knex pg
Database configuration with knexfile.js
Next, you will need to create a knexfile.js, which contains database configurations across different environments. As you learned in a previous module, when you deploy your application, you deploy to a certain environment. There are typically four environments:

The development environment references work done on your computer.

The test environment is used when the application is under test. Tests often create, update, and delete records in the database so the database used for tests should be considered highly volatile and disposable.

The staging environment references a deployed version of the application that is used to test the changes being made.

The production environment references a deployed version of the application that is your "completed" version of the application. This version is ready for users.

For example, developers often use separate databases for the staging and production environments. Additionally, for security purposes, each environment should have a unique username and password to connect to the database. By using a separate database for testing purposes, developers can reduce the risk of inadvertently affecting the data of real users in production. You will learn more about deployment in the next module.

In the knexfile.js, you can specify the database that your server should connect to across different environments.

Do this
Create a knexfile.js
To create a knexfile.js, run the following command:

npx knex init
This will create a sample knexfile.js:

// Update with your config settings

module.exports = {
  development: {
    client: "sqlite3",
    connection: {
      filename: "./dev.sqlite3",
    },
  },

  test: {
    client: "postgresql",
    pool: { min: 1, max: 5 },
    connection: {
      database: "db_test",
      user: "username_test",
      password: "password_test",
    },
  },

  staging: {
    client: "postgresql",
    connection: {
      database: "db_staging",
      user: "username_staging",
      password: "password_staging",
    },
    pool: {
      min: 2,
      max: 10,
    },
    migrations: {
      tableName: "knex_migrations",
    },
  },

  production: {
    client: "postgresql",
    connection: {
      database: "db_production",
      user: "username_production",
      password: "password_production",
    },
    pool: {
      min: 2,
      max: 10,
    },
    migrations: {
      tableName: "knex_migrations",
    },
  },
};
The sample knexfile.js exports an object with various key-value pairs for different environments. The table below describes each part of the knexfile.js in greater detail:

Keys
Description
development, test, staging, production
Each key is set to an object that contains the database configuration for that environment. For example, the development key is set to the object containing database configuration variables for the development environment.
client
Required and set to a string (such as "postgresql") describing the database library for Knex to connect to. Knex uses the appropriate client adapter.
connection
Set to an object, connection string, or a function returning an object containing the credentials and connection URL for the database instance.
pool
Set to an object containing the min (minimum) and max (maximum) number of pooled server connections allowed on your database instance. This program won't explore the details behind these values, but know that it is generally fine to use the default values.
migrations
Set to an object containing the location of the migration files; you will learn more about migrations in a future lesson.
Because you won't be setting up testing, staging, or production environments in this module, you can go ahead and remove the keys and the objects associated with test, staging, and production.

Then, modify the object for the development key so that the client key is set to "postgresql" (because you are using the PostgreSQL database for your API). For now, set the connection key to an empty string "". In the next lesson, you will set connection to point to your ElephantSQL database URL.

Your final knexfile.js should look like this:

module.exports = {
  development: {
    client: "postgresql",
    connection: "",
  },
};
You will be using the same repository in the next lesson, so make sure to save the work that you complete in this lesson.

Complete example
A completed example from this lesson can be found here:

GitHub: Node, Express, and Postgres—knex-configuration-complete branch

*** 35.3 Connecting to the database with Knex
Connecting to the database with Knex
1.5 hoursEstimated completion time
Learning Objective
By the end of this lesson, you will be able to create a connection to your database using Knex.

Overview
In the previous lesson, you created a simple database configuration for your development environment using a knexfile.js. Now, you are ready to connect to your ElephantSQL database with Knex.

Key Terms
Environment variable
Also called an env var, a key-value pair held by your current operating system and user
Starter code
This lesson continues using the project that you created in the previous lesson. If you need to, you can download that code here:

GitHub: Node, Express, and PostgreSQL Starter

Environmental variables management with dotenv
How do you configure the server so that it knows which database it should connect to in different environments? You can make use of environment variables, which are key-value pairs stored in the application's environment. Your development environment will be different than the environment that you're deploying to.

You can use the popular dotenv library to manage your environment variables. You can store your environment variables in a .env file, and dotenv will then inject environment variables into the appropriate environment.

You will learn more about environment variables in the next module. For now, you just need to install and require dotenv in your application.

Do this
Install the dotenv library
To install dotenv, run the following command:

npm install dotenv
Update the database connection in knexfile.js
Next, modify your knexfile.js as follows:

+ require("dotenv").config();
+ const { DATABASE_URL } = process.env;

module.exports = {
  development: {
    client: "postgresql",
-    connection: "",
+    connection: DATABASE_URL,
  },
};
Here's a breakdown of the syntax shown above:

require("dotenv").config(); requires and loads dotenv into the application code. Dotenv loads the environment variables that you defined in .env (which currently only contains DATABASE_URL) into process.env.

const { DATABASE_URL } = process.env; stores the value of process.env.DATABASE_URL in a variable called DATABASE_URL.

connection: DATABASE_URL, sets the location of the database for the development environment to DATABASE_URL (which is currently the URL for your ElephantSQL database instance).

Initializing Knex
The knex module is a function that accepts a configuration object for Knex, as follows:

// Define the database configuration object
const config = {
  client: "postgresql",
  connection:
    "postgres://myfakedatabase:8t6FiWqSDF8GsR--7mrun245I9TofnWd@fakepostgres.db.elephantsql.com:5432/myfakedatabase",
};

// Initialize a Knex instance by passing `config` as an argument to the Knex module
const knex = require("knex")(config);
In the previous lesson, you defined the configuration object for Knex in your knexfile.js. Next, you will require and use that configuration object to initialize a Knex instance.

Do this
Create src/db/connection.js
In the src/db folder, create a new file called connection.js. Then add the following code to it:

const env = process.env.NODE_ENV || "development";
const config = require("../../knexfile")[env];
const knex = require("knex")(config);

module.exports = knex;
Here's a breakdown of the syntax shown above:

const env = process.env.NODE_ENV || "development"; determines the current environment where the application code is running and stores the value in the env variable. If process.env.NODE_ENV isn't defined, then set the value to "development".

const config = require("../../knexfile")[env]; requires the database configuration object from the knexfile.js for the current environment and stores it in the config variable. For example, if env is set to "development", then config will be set to the development configuration object from knexfile.js.

const knex = require("knex")(config); initializes a Knex instance by calling the knex module, passing in config as an argument.

module.exports = knex; exports the Knex instance so that other files can require it.

You won't be able to test the connection between the server and the database just yet.

You will be using the same repository in the next lesson, so make sure to save the work that you complete in this lesson.

Complete example
A completed example from this lesson can be found here:

GitHub: Node, Express, and Postgres—connecting-to-the-database-with-knex branch

*** 35.4 Migrations with Knex
Migrations with Knex
1.5 hoursEstimated completion time
Learning Objective
By the end of this lesson, you will be able to write database migrations using Knex's migration tools. You'll also be able to run and roll back migrations using command-line tools.

Overview
Now that you have your Knex set up and ready to connect to your PostgreSQL database, you are ready to create the tables needed for your Thinkful Gifts database. So in this module, you'll go through the steps to create and define your migration files and then execute the migration.

After discussing with representatives from the inventory management department and other engineers on your team, you came up with the following ERD for your database. As you can see, this ERD includes tables for the products, suppliers, and categories resources:

An ERD that includes tables for the products, suppliers, and categories resources.
The products and suppliers tables have a one-to-many relationship, because a product can only be supplied by a specified supplier.

The products and categories tables have a many-to-many relationship, because a product can belong to more than one category. For example, a fragrance diffuser might belong to both the home and electronics categories. Likewise, a category can have many products.

Starter code
This lesson continues using the project you created in the previous lesson. If you need to, you can download that code here:

GitHub: Node, Express, and Postgres Starter
Migrations
Throughout the lifetime of a web application, you often need to make changes to the database schema. There are some common hurdles:

You might need to make copies of the database (for example, for testing, development, production, or other developers on your team setting up their local environment).

When you add new features to your application, your database needs can grow more complex with the new features added. As a result, you might have to add new columns to existing tables or new tables to your database.

You might make a mistake and need to revert changes to the tables after deploying to production.

You might need to rename parts of the database.

When you need to update your database schema, you will have to create a migration. Migrations serve as a version-control file for your database, allowing you to easily modify and share the application's database schema with your coworkers. If you update your database only to realize that there were mistakes in the update, you can easily run and roll back migrations using command-line tools. Database migrations define each change made to your database in a migration file that is tracked by version control.

To create migration files for your project, you will first have to specify where the migration files will be stored.

Do this
Specify a storage location for your migration files
In your knexfile.js, specify a migrations key with a path to your migration files, as follows:

+ const path = require("path");
require("dotenv").config();
const { DATABASE_URL } = process.env;

module.exports = {
  development: {
    client: "postgresql",
    connection: DATABASE_URL,
+    migrations: {
+      directory: path.join(__dirname, "src", "db", "migrations"),
+    },
  },
};
Setting directory: path.join(__dirname, "src", "db", "migrations") within the migrations object will tell Knex to store migration files in the migrations folder at src/db/migrations.

Create a migration file
You can use the Knex CLI to create a new migration. The syntax is as follows:

npx knex migrate:make [migration_name]
Try to pick a descriptive migration name. For example, if you are creating a new table called suppliers, you might want to name the migration file createSuppliers. The migration names should inform other developers as to the intent of that migration.

Running the command above creates a new file within the migrations folder, with a filename that includes a timestamp and the name of the migration (for example, 20201129203428_createSuppliers.js). The file contains the following code:

exports.up = function (knex) {};
exports.down = function (knex) {};
Here's a breakdown of the syntax:

exports.up = function (knex) {};: Within the exports.up function, you can specify the Knex methods for making the desired database changes, such as creating tables, adding or removing a column from a table, changing indexes, and so on.

exports.down = function (knex) {};: All migration steps include two things: the SQL statements to execute the schema change and the SQL statements to undo the changes. These statements also serve as a form of documentation for new developers entering the project to reason about the changes made to the database in the past. The exports.down function allows you to quickly undo a migration, if needed. It does the opposite of exports.up. For example, if exports.up created a table, then exports.down will remove that table. If exports.up added a column, then exports.down will remove that column.

Do this
Create a migration file for the suppliers table
First, create a new migration file by running the following Knex CLI command:

npx knex migrate:make createSuppliersTable
This will create a file like /src/db/migrations/20201129203428_createSuppliersTable.js. Note that your timestamp will be different because you ran the command above at a different time.

Define a table with Knex schema methods
You can use various Knex methods to create a table and define its columns. For example, you can use knex.schema.createTable() to create a new database table. To create a column, you can specify a column type by calling special Knex methods on the table (such as increments(), string(), text(), decimal(), and enum()). You can also specify whether or not a column is nullable and its default values. You can find the full list of schema methods in the Knex documentation.

Do this
Define the suppliers table
Inside of the exports.up function of the newly created migration file, add the following code:

exports.up = function (knex) {
  return knex.schema.createTable("suppliers", (table) => {
    table.increments("supplier_id").primary(); // Sets supplier_id as the primary key
    table.string("supplier_name");
    table.string("supplier_address_line_1");
    table.string("supplier_address_line_2");
    table.string("supplier_city");
    table.string("supplier_state");
    table.string("supplier_zip");
    table.string("supplier_phone");
    table.string("supplier_email");
    table.text("supplier_notes");
    table.string("supplier_type_of_goods");
    table.timestamps(true, true); // Adds created_at and updated_at columns
  });
};
When calling knex.schema.createTable(), you passed in the name of the table ("suppliers") and a callback function that takes an argument table, which gives you a reference to the table. Then, inside of the callback function, you specified the columns that the table should have. For example, calling table.string("supplier_name") creates a column on the suppliers table called supplier_name that accepts string values.

Notice that the columns that are declared above are for string, text, and timestamp values. You've likely noticed that the columns above identify suppliers, but you may be wondering about the timestamps() method. The table.timestamps(true, true) method will add created_at and updated_at columns. These columns are important because they help keep track of the table's records. Timestamps aren't used only for logging purposes—they may also add functionality to an application. For example, a user may want to query records that were created or updated on a specific day. It's a best practice to use timestamps in all or most of your tables.

The timestamps() method has two optional arguments: Passing true as the first argument sets the columns to be a timestamp type. Passing true as the second argument sets those columns to be non-nullable and to use the current timestamp by default. You can find more information about the timestamps() method in the Knex.js documentation. Now, inside of the exports.down function of the newly created migration file, add the following code:

exports.down = function (knex) {
  return knex.schema.dropTable("suppliers");
};

When you undo the migration, exports.down will get invoked, which will call the knex.schema.dropTable() method to drop the suppliers table.

The exports.up and exports.down functions should always return a promise.

Create a migration file and define the products table
You will need to implement the same steps that you followed to create the suppliers table. Create a new migration file by running the following Knex CLI command:

npx knex migrate:make createProductsTable
Inside of the exports.up function of the newly created migration file, add the following code:

exports.up = function (knex) {
  return knex.schema.createTable("products", (table) => {
    table.increments("product_id").primary(); // Sets `product_id` as the primary key
    table.string("product_sku");
    table.string("product_name");
    table.text("product_description");
    table.integer("product_quantity_in_stock");
    table.decimal("product_weight_in_lbs");
    table.integer("supplier_id").unsigned().notNullable();
    table
      .foreign("supplier_id")
      .references("supplier_id")
      .inTable("suppliers")
      .onDelete("cascade");
    table.timestamps(true, true);
  });
};
Notice that you can chain unsigned() to table.integer("supplier_id") to prevent negative values from being inserted into the supplier_id column. Chaining notNullable() ensures that supplier_id cannot be null.

The line table.foreign("supplier_id").references("supplier_id").inTable("suppliers"); creates a foreign key constraint called supplier_id, which references the primary key of the suppliers table. Chaining onDelete("cascade") means that if a supplier is deleted, then all the products related to the supplier will be deleted from the database as well.

Inside of the exports.down function of the newly created migration file, add the following code:

exports.down = function (knex) {
  return knex.schema.dropTable("products");
};
Create a migration file and define the categories table
Create a new migration file by running the following Knex CLI command:

npx knex migrate:make createCategoriesTable
Inside of the exports.up function of the newly created migration file, add the following code:

exports.up = function (knex) {
  return knex.schema.createTable("categories", (table) => {
    table.increments("category_id").primary();
    table.string("category_name");
    table.text("category_description");
    table.timestamps(true, true);
  });
};
Inside of the exports.down function of the newly created migration file, add the following code:

exports.down = function (knex) {
  return knex.schema.dropTable("categories");
};
Create a migration file and define the products_categories table
Because products and categories have a many-to-many relationship, linking these two tables require a join table, which you will call products_categories. Create a new migration file by running the following Knex CLI command:

npx knex migrate:make createProductsCategoriesTable
Inside of the exports.up function of the newly created migration file, add the following code:

exports.up = function (knex) {
  return knex.schema.createTable("products_categories", (table) => {
    table.integer("product_id").unsigned().notNullable();
    table
      .foreign("product_id")
      .references("product_id")
      .inTable("products")
      .onDelete("CASCADE");
    table.integer("category_id").unsigned().notNullable();
    table
      .foreign("category_id")
      .references("category_id")
      .inTable("categories")
      .onDelete("CASCADE");

    table.timestamps(true, true);
  });
};
Inside of the exports.down function of the newly created migration file, add the following code:

exports.down = function (knex) {
  return knex.schema.dropTable("products_categories");
};
Run the migration
Now that the migration files have been created and defined, you will execute the migration on your development database. Remember how each migration file starts with a timestamp of when the migration was created?

Knex runs the migrations in chronological order and uses the timestamps to preserve that order. Knex also keeps track of which migrations have been run so that it doesn't try to run the same migration twice.

Do this
Run the latest migration manually
Run the following command:

npx knex migrate:latest
Now, take a look at these tables in DBeaver. In the Database Navigator, expand the Schemas section, then expand the public schema and the Tables for the public schema. You'll see the four tables that you created, along with two additional tables called knex_migrations and knex_migrations_lock, as follows:

The four tables you created, along with two additional tables, in the public schema section.
Migration rollback
Sometimes, a developer might make a mistake and need to revert changes to the tables after deploying to production.

Do this
Roll back the latest migration
To undo the latest batch of migration, run the following command:

npx knex migrate:rollback
Now check your DBeaver database navigator again—the tables should be gone except for the knex_migrations and knex_migrations_lock files.

Before proceeding to the next section, rerun npx knex migrate:latest to recreate the tables.

Take a look at the documentation for the full list of Knex migration CLI commands.

Modify a table
Sometimes, a developer might also have to make updates to a table, such as altering the name of a column or adding a new column to a table. For example, after running migration, you might realize that the product_name column in the products table should be called product_title instead.

The process of renaming a column is similar to creating a table.

Instead of modifying the existing migration, you will create a new migration to specifically rename the column.

Do this
Add product_price and rename the product_name column to product_title
First, create a new migration file by running the following:

npx knex migrate:make productsAddPriceAndChangeProductNameToProductTitle
Again, pick a descriptive name for your migration for documentation purposes.

Next, update the exports.up and exports.down functions:

exports.up = function (knex) {
  return knex.schema.table("products", (table) => {
    table.renameColumn("product_name", "product_title");
    table.decimal("product_price");  // Add a new column
  });
};
exports.down = function (knex) {
  return knex.schema.table("products", (table) => {
    table.renameColumn("product_title", "product_name");
    table.dropColumn("product_price");
  });
};
As you run more and more migrations on your project, you might want to keep track of which migrations are completed and which are pending. Run the following command:

npx knex migrate:list
This will return a list of completed and pending migrations.

Finally, run npx knex migrate:latest to apply the migration.

Undo a specific migration
You now have five migration files in your migrations folder. What if you only wanted to roll back a specific migration? For example, what if you wanted to roll back the productsAddPriceAndChangeProductNameToProductTitle migration that you just ran?

You can run the following command, replacing migration_file_name with the full filename of your migration (for example, 20201203091021_productsAddPriceAndChangeProductNameToProductTitle.js):

npx knex migrate:down migration_file_name
In the DBeaver database navigator, check the products table again: the product_price column should be gone and the product_title column should be reverted to product_name.

Before proceeding to the next section, run npx knex migrate:up migration_file_name to apply the productsAddPriceAndChangeProductNameToProductTitle migration again to add the the product_price column and change the product_name column to product_title.

You are now ready to seed your database with dummy data in the next lesson. You will be using the same repository in the next lesson, so make sure to save the work that you complete in this lesson.

Complete example
A completed example from this lesson can be found here:

GitHub: Node, Express, and Postgres—migrations-with-knex-complete branch

*** 35.5 Seeding data with Knex
Seeding data with Knex
1.5 hoursEstimated completion time
Learning Objective
By the end of this lesson, you will be able to write a seed data file to set up initial data.

Overview
Now that you have the required tables in your database, it's time to add some test data. Typically, when building an application, it is useful to run the application with sample data to make sure that the application logic is correct. Similar to migrations, Knex allows you to create scripts to insert data into your tables. Using scripts to insert test data into a database is called seeding the database.

Starter code
This lesson continues using the project that you created in the previous lesson. If you need to, you can download that code here:

GitHub: Node, Express, and Postgres Starter
Seed the database
Seed files allow you to populate your database with test or seed data, independent of your migration files. To create seed data, you will have to follow a few steps:

Define a location for storing your seed scripts

Create the seed scripts

Update the seed scripts

Run the seed scripts

Where to store your seed scripts
Knex creates seed scripts in the directory specified in the knexfile.js for the current environment. For example, the following sample seed configuration stores the seed scripts at ./seeds/dev for the development environment:

development: {
  client: ...,
  connection: { ... },
  seeds: {
    directory: './seeds/dev'
  }
}
If seeds.directory isn't defined in the knexfile.js, scripts are created in ./seeds by default.

Do this
Store seed scripts at ./src/db/seeds
Update the knexfile.js as follows:

const path = require("path");
require("dotenv").config();
const { DATABASE_URL } = process.env;

module.exports = {
  development: {
    client: "postgresql",
    connection: DATABASE_URL,
    migrations: {
      directory: path.join(__dirname, "src", "db", "migrations"),
    },
+    seeds: {
+      directory: path.join(__dirname, "src", "db", "seeds"),
+    },
  },
};
Now, Knex will create and store the seed files at ./src/db/seeds.

Create the seed scripts and seed data
The CLI syntax for creating a seed file is as follows:

npx knex seed:make seed_name
In the previous lesson, you learned that Knex automatically prepends a timestamp to the migration files to keep them in the proper order. Because seed files may rely on previous seeds, you should also keep seed files in order. However, Knex won't do the same to seed files, so you will have to design a way to maintain the proper order for your seed files.

Create the seed scripts for the products, suppliers, categories, and products_categories tables
Because you have four tables that you want to seed, you need to create four seed files. Run the following commands, one after the other:

npx knex seed:make 00-suppliers
npx knex seed:make 01-products
npx knex seed:make 02-categories
npx knex seed:make 03-products_categories
Because the products table depends on the supplier_id column from the suppliers table, the products table will have to be created after the suppliers table. A straightforward way to maintain this order is to add an incremental number at the beginning of the seed files.

So, the first seed file will start with 01, the next seed file will start with 02, and so on and so forth. Seed files are executed in order. Unlike migrations, every seed file will be executed when you run the command.

After running the commands above, you'll see four scripts created at ./src/db/seeds:

Four scripts are created at ./src/db/seeds.
Update the seed scripts
Notice that upon running the command, Knex created a file containing some boilerplate code:

exports.seed = function (knex) {
  // Deletes ALL existing entries
  return knex("table_name")
    .del()
    .then(function () {
      // Inserts seed entries
      return knex("table_name").insert([
        { id: 1, colName: "rowValue1" },
        { id: 2, colName: "rowValue2" },
        { id: 3, colName: "rowValue3" },
      ]);
    });
};
You will have to customize the boilerplate code in each seed file to suit your needs. You will also have to require the seed data stored in the ./src/db/fixtures folder into the appropriate seed file. Spend some time looking at the data in the ./src/db/fixtures folder. You should design your seed files to reset tables as needed before inserting data.

Do this
Customize /seeds/00-suppliers.js
Modify /seeds/00-suppliers.js so that the complete file looks like the following:

const suppliers = require("../fixtures/suppliers");

exports.seed = function (knex) {
  return knex
    .raw("TRUNCATE TABLE suppliers RESTART IDENTITY CASCADE")
    .then(function () {
      return knex("suppliers").insert(suppliers);
    });
};
Here's a breakdown of the syntax:

const suppliers = require("../fixtures/suppliers"); requires the suppliers seed data and stores it in the suppliers variable.

knex.raw("TRUNCATE TABLE suppliers RESTART IDENTITY CASCADE")

The knex.raw() method uses the SQL statement RESTART IDENTITY to reset the primary key values.

Adding CASCADE ensures that any references to the entries in the suppliers table are deleted as well when the entries are deleted.

As a note, the Knex truncate() method is preferable to writing a raw SQL statement to truncate the data, but it does not provide a way to reset the values in the primary key column after entries are deleted from the table.

Putting knex("suppliers").insert(suppliers) inside then() ensures that this line will only get executed after the preceding knex.raw() function is complete.

Customize /seeds/01-products.js
Similarly, modify /seeds/01-products.js so that the complete file looks like the following:

const products = require("../fixtures/products");

exports.seed = function (knex) {
  return knex
    .raw("TRUNCATE TABLE products RESTART IDENTITY CASCADE")
    .then(function () {
      return knex("products").insert(products);
    });
};
Customize /seeds/02-categories.js
Similarly, modify /seeds/02-categories.js so that the complete file looks like the following:

const categories = require("../fixtures/categories");

exports.seed = function (knex) {
  return knex
    .raw("TRUNCATE TABLE categories RESTART IDENTITY CASCADE")
    .then(function () {
      return knex("categories").insert(categories);
    });
};
Customize /seeds/03-products_categories.js
Similarly, modify /seeds/03-products_categories.js so that the complete file looks like the following:

const productsCategories = require("../fixtures/productsCategories");

exports.seed = function (knex) {
  return knex
    .raw("TRUNCATE TABLE products_categories RESTART IDENTITY CASCADE")
    .then(function () {
      return knex("products_categories").insert(productsCategories);
    });
};
Run the seed scripts
To run seed files, execute the following:

npx knex seed:run
If all goes well, you will see a Ran 4 seed files message in your command line.

Now, check your database in DBeaver and see if the tables have been populated with the seed data!

If you ever need to run a specific seed file only, you can execute the following:

npx knex seed:run --specific=seed-filename.js
You will be using the same repository in the next lesson, so make sure to save the work that you complete in this lesson.

Complete example
A completed example from this lesson can be found here:

GitHub: Node, Express, and Postgres—seeding-data-with-knex branch
*** 35.6 CRUD with Knex
CRUD with Knex
1.5 hoursEstimated completion time
Learning Objective
By the end of this lesson, you will be able to write database queries to complete CRUD routes in an Express server.

Overview
When a user makes a request to the server, the server typically responds with some data. To do so, the server must first query data from a database. The server must execute the appropriate database queries to get or manipulate the requested data. Now that your database contains seed data, you are ready to perform CRUD operations (create, read, update, and delete) on your PostgreSQL database. In this lesson, you'll use Knex to do just that.

In your preliminary discussions with the inventory management department, you determined that your API should provide access to the following endpoints:

HTTP verb
Path
Description
GET
/categories
Retrieve a list of all categories
GET
/products
Retrieve a list of all products
GET
/products/:productId
Retrieve a product by ID
POST
/suppliers
Create a new supplier
PUT
/suppliers/:supplierId
Update a specific supplier
DELETE
/suppliers/:supplierId
Delete a specific supplier
Trying to access any other API endpoints should return a 405 error to the API consumer.

You will be using Knex to build SQL commands. In the real world, complex SQL queries can quickly make your codebase difficult to read. Instead of writing raw—and often verbose—SQL queries from scratch, you can use Knex. Knex allows you to write JavaScript to build SQL commands for various CRUD operations using a relatively clean syntax.

Starter code
This lesson continues using the project that you created in the previous lesson. If you need to, you can download that code here:

GitHub: Node, Express, and Postgres Starter
Services
As mentioned before, to perform the data manipulations requested by the API consumer, the server needs to connect to the database and execute the necessary queries against the database. That means that you need to write code in your server to generate those database queries.

But where should you store the code for generating the database queries? The most straightforward option is probably to include all the database query logic in your *.controller.js files. However, as your API and codebase continue to grow in size and complexity over time, putting all your queries there would quickly clutter up your application logic and make your code difficult to test and reuse. This relates to other potential problems when building express API applications:

Long files containing endpoint code and database query code are hard to navigate.

If multiple parts of the codebase need to perform similar database queries, it is better to reuse them.

If you want to change something like the name of the table, column name, or type of database, it is better to have to update fewer parts of the code with this new change.

To alleviate these problems, you can use some of the following best practices:

Don't repeat yourself (DRY) for using functions

Separation of concerns (SOC) for organizing functions

Modularization and layering to structure files

Encapsulation for bundling methods together that operate on the same data

More concretely, you can introduce a service into your Express application. A service, which is just regular old JavaScript, aims to group together related functions as a service to the rest of your application. Services are typically self contained, so they could be remotely called or imported as a local module and not depend on data already being available.

The services that you will create for the API will include functions that make all the CRUD transactions for one table.

A service aligns with these best practices:

A service makes the code DRY because you can use the service methods in multiple places if needed.

A service helps with SOC because it is responsible for only one set of concerns. For example, it may manage database transactions and the details of how they occur. The middleware is concerned with the shape and properties of requests and responses.

You can move some code out of your middleware or controllers and into a service, which increases modularization.

Note: Using a service isn't required; services are a technique for organizing code. You can use services for grouping methods in other situations, not just database transactions. For example, you can use them to group multiple related API requests, file system commands, and so forth.

A good analogy is the post office. You can say that the post office is a service for interacting with parcels. When you want to send a parcel, you go to the post office. At the post office, you can send parcels of different sizes to different locations. You could also collect parcels there if you had the correct information. It would be frustrating if you had to go to a different post office for each parcel with a different destination or size! You want a single post office that can send parcels anywhere. Also, you don't want to know the full process for sending or receiving a parcel, but instead expect the post service to manage these steps for you behind the scenes.

Now that you understand the potential advantages that using a service offers, you will create services for performing CRUD operations on the categories, products, suppliers, and products_categories tables next. Again, the services that you will create for the API will include functions that make all CRUD transactions for one table.

Knex syntax
Knex allows you to write JavaScript to create SQL queries. For example, consider how you might build the following SQL query with Knex:

SELECT * FROM categories;
First, you must first tell the Knex instance which table to query. To tell Knex to query the categories table, you can use either of the following commands:

knexInstance.from("categories");
knexInstance("categories");
You can then chain a select() method to choose which columns to select, as follows:

knexInstance("categories").select("*");
There are many other Knex instance methods, such as where() and join(), which you can chain to create even more complex queries. For example, consider the following SQL query:

SELECT * FROM categories
WHERE category_id = 2;
The above query will look like this in Knex:

knexInstance("categories").select("*").where({ category_id: 2 });
The Knex instance methods only build the query, not execute it. To execute the query, you will need to chain the then() promise method to end of the Knex query, like this:

knexInstance
  .from("categories")
  .select("*")
  .then((result) => {
    console.log(result);
  });
GET /categories endpoint
The API should support the GET /categories endpoint, which retrieves a list of all categories.

Do this
Create the categories service object in categories.service.js
Inside /src/categories, create a new file called categories.service.js. Add the following code:

const knex = require("../db/connection");

function list() {
  return knex("categories").select("*");
}

module.exports = {
  list,
};
Here's a breakdown of the syntax:

const knex = require("../db/connection"); requires the Knex instance initialized in ./db/connection.js.

function list() {return knex("categories").select("*"); } declares a function called list(), which builds a query that selects all columns from the categories table.

module.exports = { list } exports the list() function so that it can be required in other files. You can add any other functions that you'd like to export inside the module.exports object, separated by commas.

Require the categories service object in categories.controller.js
Next, update categories.controller.js so that the complete file looks like the following:

const categoriesService = require("./categories.service");

function list(req, res, next) {
  categoriesService
    .list()
    .then((data) => res.json({ data }))
    .catch(next);
}

module.exports = {
  list,
};
const categoriesService = require("./categories.service"); requires the service object that you created in the previous step and assigns it to categoriesService.

You can then access the methods on the service object to perform CRUD operations on a table (for example, categoriesService.list()). Chaining then() to categoriesService.list() executes the Knex query. Chaining catch(next) onto the promise will call next() passing in the error. If the Knex promise doesn't have a catch(next) at the end, it will not correctly handle errors that occur during when running the query.

Now, make sure that your server is running and visit localhost:5000/categories in the browser. You will get back a list of all the categories data. The list will look like the following:

{
  data: [
    {
      category_id: 1,
      category_name: "electronics",
      category_description: "A category for electronic gadgets of any kind, appealing to all genders.",
      created_at: "2020-12-01T20:37:09.550Z",
      updated_at: "2020-12-01T20:37:09.550Z"
    },
	...
  ]
}
GET /products endpoint
The API should support the GET /products endpoint, which retrieves a list of all products.

Do this
Create the products service object in products.service.js
Inside /src/products, create a new file called products.service.js. Add the following code:

const knex = require("../db/connection");

function list() {
  return knex("products").select("*");
}

module.exports = {
  list,
};
The code above looks and functions very similarly to categories.service.js.

Require the products service object in products.controller.js
Next, require the products service object at the top of products.controller.js, as follows:

const productsService = require("./products.service");
Update the list() function to call the productsService.list() method and return a JSON response to the client on successful promise resolution, as follows:

function list(req, res, next) {
  productsService
    .list()
    .then((data) => res.json({ data }))
    .catch(next);
}
Now, either use Postman to send a GET request, or navigate to localhost:5000/products in the browser. You will get back a list of products.

GET /products/:productId endpoint
The API should support the GET /products/:productId endpoint, which retrieves a specific product by its ID.

Do this
Add read() to products.service.js
In products.service.js, add a new function called read(), as follows:

function read(productId) {
  return knex("products").select("*").where({ product_id: productId }).first();
}
This read() function creates a Knex query that selects all columns from the products table where the product_id column matches the argument passed to the read() function. The first() method returns the first row in the table as an object.

Remember to export read() by including it in the module.exports object at the bottom of the file.

Add productExists() validation middleware to products.controller.js
Then, as you have done in previous modules, create a validation middleware called productExists() that checks whether or not a given product exists based on ID:

function productExists(req, res, next) {
  productsService
    .read(req.params.productId)
    .then((product) => {
      if (product) {
        res.locals.product = product;
        return next();
      }
      next({ status: 404, message: `Product cannot be found.` });
    })
    .catch(next);
}
Chaining then() to productsService.read(productId) will execute the Knex query that you defined previously to retrieve a product given an id. The query returns a promise, which is handled in the then() function.

If the product exists, it is stored in res.locals.product so that it can be readily accessed in the rest of the middleware pipeline. Otherwise, next() is called with an error.

Update the read() function, as follows:

function read(req, res) {
  const { product: data } = res.locals;
  res.json({ data });
}
Add the productExists() function as a validation middleware in module.exports:

module.exports = {
-  read: [read],
+  read: [productExists, read],
   list,
};
Now, either use Postman to send a GET request or navigate to localhost:5000/products/1 in the browser. You will get back a product with product_id of 1.

Route params validation with regex
You can validate a route param using regex. For example, to ensure that a route param :movieId in the path movies/:movieId consists of just one or more digits, you can prepend the regex expression ([0-9]+) to the end of the path, as follows:

app.get("/movies/:movieId([0-9]+)", moviesController);
Do this
Validate :productId with regex
In src/products/products.router.js, modify the /products/:productId route, as follows:

- router.route("/:productId").get(controller.read).all(methodNotAllowed);
+ router.route("/:productId([0-9]+)").get(controller.read).all(methodNotAllowed);
Now, either use Postman to send a GET request or navigate to localhost:5000/products/1asdf in the browser. You will get back a Not found error telling you that the path isn't found.

POST /suppliers endpoint
The API should support the ability to create a new supplier in the database. You can use the Knex insert() method to insert a new row into the table. The insert() method accepts as its argument an object containing data for the new supplier.

You can then chain a returning() method to insert() to specify which columns should be returned by the insert(). The passed column parameter may be a string or an array of strings. The returning() method also works for the update() and delete() methods.

Do this
Create the suppliers service object in suppliers.service.js
Inside /src/suppliers, create a new file called suppliers.service.js. Add the following code:

const knex = require("../db/connection");

function create(supplier) {
  return knex("suppliers")
    .insert(supplier)
    .returning("*")
    .then((createdRecords) => createdRecords[0]);
}

module.exports = {
  create,
};
The code above looks and functions very similarly to categories.service.js and products.service.js. The create() function creates a Knex query that inserts a new supplier into the suppliers table while returning all columns from the newly inserted row (because of returning(*)). The .insert() method of Knex can be used to insert more than one record, so it returns an array of the records inserted. For this API, only one supplier will ever be inserted at a time so you chain .then((createdRecords) => createdRecords[0]); } onto the query to return only the one inserted record.

Require the suppliers service object in suppliers.controller.js
Next, require the suppliers service object at the top of suppliers.controller.js:

const suppliersService = require("./suppliers.service.js");
Then, similar to what you have done in previous modules, create a validation middleware called hasOnlyValidProperties() to check whether the request body contains a specified set of allowed fields:

const VALID_PROPERTIES = [
  "supplier_name",
  "supplier_address_line_1",
  "supplier_address_line_2",
  "supplier_city",
  "supplier_state",
  "supplier_zip",
  "supplier_phone",
  "supplier_email",
  "supplier_notes",
  "supplier_type_of_goods",
];

function hasOnlyValidProperties(req, res, next) {
  const { data = {} } = req.body;

  const invalidFields = Object.keys(data).filter(
    (field) => !VALID_PROPERTIES.includes(field)
  );

  if (invalidFields.length) {
    return next({
      status: 400,
      message: `Invalid field(s): ${invalidFields.join(", ")}`,
    });
  }
  next();
}
Tip
Never trust what the client is sending to your server without verifying it using validation middleware.

Add another validation middleware, hasProperties(), which checks whether or not the request body includes two required fields: supplier_name and supplier_email.

Because hasProperties() will be able to be used by any controller as validation middleware, you will create a new file to define the function.

In src/errors/hasProperties.js, add the following code:

function hasProperties(...properties) {
  return function (req, res, next) {
    const { data = {} } = req.body;

    try {
      properties.forEach((property) => {
        if (!data[property]) {
          const error = new Error(`A '${property}' property is required.`);
          error.status = 400;
          throw error;
        }
      });
      next();
    } catch (error) {
      next(error);
    }
  };
}

module.exports = hasProperties;
Then, use hasProperties() to define a hasRequiredProperties() middleware function in src/suppliers/suppliers.controller.js:

const hasProperties = require("../errors/hasProperties");
const hasRequiredProperties = hasProperties("supplier_name", "supplier_email");
Then add the create() function:

function create(req, res, next) {
  suppliersService
    .create(req.body.data)
    .then((data) => res.status(201).json({ data }))
    .catch(next);
}
The function above calls the suppliersService.create() method, passing in req.body.data as the argument. The req.body.data argument references the object containing the supplier information. Chaining then() to suppliersService.create() executes the Knex query. If the promise resolves successfully, the server responds with a 201 status code along with the newly created supplier.

Finally, add the create() function and the validation middleware to the module.exports object:

module.exports = {
  ...
+ create: [hasOnlyValidProperties, hasRequiredProperties, create],
};
Now, use Postman to send a POST request to localhost:5000/suppliers. If the supplier is successfully created in the database, you should get back a JSON response that includes the newly created supplier.

PUT /suppliers/:supplierId endpoint
The API should support the ability to update an existing supplier in the database. You can use the Knex update() method to update a row in the table. The update() method accepts as its argument an object that contains the data for updating the existing supplier.

If a returning array is passed (for example, ["supplier_id", "supplier_name"] as the second argument, it resolves the promise with an array of all the updated rows with specified columns. That's like a shortcut for the returning() method. You can also pass "*" as the second argument to return all of the columns of the updated rows.

Do this
Add read() and update() to suppliers.service.js
In suppliers.service.js, add the following functions:

function read(supplier_id) {
  return knex("suppliers").select("*").where({ supplier_id }).first();
}

function update(updatedSupplier) {
  return knex("suppliers")
    .select("*")
    .where({ supplier_id: updatedSupplier.supplier_id })
    .update(updatedSupplier, "*");
}
Although the API doesn't have to support a GET /suppliers/:supplierId endpoint, you are creating read() so that you can use this function for validation in the route handlers later on.

The .update() method of Knex can be used to update more than one record, so it returns an array of the records updated. For this API, only one supplier will ever be updated at a time so you chain .then((updatedRecords) => updatedRecords[0]); } onto the query to return only the one record.

Remember to export read() and update() by including these functions in the module.exports object at the bottom of the file.

In suppliers.controller.js, add the following code:

module.exports = {
  create: [hasOnlyValidProperties, hasRequiredProperties, create],
+ update: [hasOnlyValidProperties, hasRequiredProperties, update],
};
In suppliers.service.js, add the following code:

module.exports = {
  create,
+ read,
+ update,
};
Add the supplierExists() validation middleware to suppliers.controllers.js
Then, as you have done in previous modules, create a validation middleware called supplierExists() that checks whether or not a given supplier exists based on ID:

function supplierExists(req, res, next) {
  suppliersService
    .read(req.params.supplierId)
    .then((supplier) => {
      if (supplier) {
        res.locals.supplier = supplier;
        return next();
      }
      next({ status: 404, message: `Supplier cannot be found.` });
    })
    .catch(next);
}
Chaining then() to suppliersService.read() will execute the Knex query that you defined previously to retrieve a supplier given based on ID. The query returns a promise, which is handled in the then() function.

If the supplier exists, it is stored in res.locals.supplier so that it can be readily accessed in the rest of the middleware pipeline. Otherwise, next() is called with an error object.

Modify the update() function, as follows:

function update(req, res, next) {
  const updatedSupplier = {
    ...req.body.data,
    supplier_id: res.locals.supplier.supplier_id,
  };
  suppliersService
    .update(updatedSupplier)
    .then((data) => res.json({ data }))
    .catch(next);
}
The function above calls the SuppliersService.update() method, passing in the updatedSupplier object. Note that the supplier_id of updatedSupplier is always set to the existing supplier_id (res.locals.supplier.supplier_id) to prevent the update from accidentally, or intentionally, changing the supplier_id during an update. If the promise resolves successfully, then the server responds with the updated supplier.

Add the supplierExists() function as a validation middleware in module.exports:

module.exports = {
  create: [hasOnlyValidProperties, hasRequiredProperties, create],
+ update: [supplierExists, hasOnlyValidProperties, hasRequiredProperties, update],
};
Now, use Postman to send a PUT request to localhost:5000/suppliers/1 with some updated information. If the supplier is successfully updated in your database, you will get back a JSON response that includes the updated supplier.

DELETE /suppliers/:supplierId endpoint
The API should support the ability to delete a supplier. You can use the Knex del() method to delete a row in the table based on the conditions specified in the query. Unlike update() and insert(), del() does not accept any arguments.

Do this
Add delete() to supplier.service.js
In suppliers.service.js, add the following function to delete a supplier based on ID:

function destroy(supplier_id) {
  return knex("suppliers").where({ supplier_id }).del();
}
Remember to export destroy() by including it in the module.exports object at the bottom of the file.

module.exports = {
   ...
+  delete: destroy,
};
In src/suppliers/suppliers.controller.js, modify the destroy() function, as follows:

function destroy(req, res, next) {
  suppliersService
    .delete(res.locals.supplier.supplier_id)
    .then(() => res.sendStatus(204))
    .catch(next);
}
The function above calls the suppliersService.delete() method, passing in the supplier_id of the supplier to be deleted as an argument. If the promise resolves successfully, the server responds with a 204 status code.

In src/suppliers/suppliers.controller.js, add the supplierExists() function as validation middleware in module.exports:

module.exports = {
   create: [hasOnlyValidProperties, hasRequiredProperties, create],
   update: [supplierExists, hasOnlyValidProperties, hasRequiredProperties, update],
+  delete: [supplierExists, destroy],
};
Now, use Postman to send a DELETE request to localhost:5000/suppliers/1. If the supplier is successfully deleted from your database, you will get back a 204 status code.

The basic CRUD functionality is now complete.

You will be using the same repository in the next lesson, so make sure to save the work that you complete in this lesson.

Complete example
A completed example from this lesson can be found here:

GitHub: Node, Express, and Postgres—crud-with-knex branch

*** 35.7 Async and await in Express
Async and await in Express
1.5 hoursEstimated completion time
Learning Objective
By the end of this lesson, you will be able to convert route handlers to use the async and await pattern.

Overview
A senior member of the engineering team has done a code review on your work for the previous lesson and noticed the prevalent use of then() methods in the pull request. As a result, the engineer has tasked you with refactoring your route handlers to make use of async and await, where appropriate.

In a previous module, you learned that promises manage asynchronous code, and you learned how to use then() and catch() to handle values from promises. You also learned about one of the most powerful tools that involves promises: the async and await keywords.

As you've seen, relying on then() to handle asynchronous logic may quickly lead to messy and unreadable code. Using the async and await keywords instead simplifies the syntax for making asynchronous calls by allowing your asynchronous code to look more synchronous.

Starter code
This lesson continues using the project that you created in the previous lesson. If you need to, you can download that code here:

GitHub: Node, Express, and Postgres Starter
The async and await keywords in Express
Do this
Update the list() function in categories.controller.js
In categories.controller.js, modify your list() function as follows:

async function list(req, res) {
  const data = await categoriesService.list();
  res.json({ data });
}
Here's a breakdown of the syntax:

The categoriesService.list() function executes a Knex query, which is an asynchronous operation. Using the await keyword before categoriesService.list() forces the execution of the code to pause on that line until that asynchronous operation is finished. Once it is, the resolved response is stored in categories.

Because the list() function contains a function that uses await, you must add the async keyword in front of the list() function. Otherwise, your code won't work properly.

Go ahead and follow the rest of this guide to update your route handlers to use async and await, where appropriate.

Update the productExists() function products.controller.js
In products.controller.js, modify the productExists() function as follows:

async function productExists(req, res, next) {
  const product = await productsService.read(req.params.productId);
  if (product) {
    res.locals.product = product;
    return next();
  }
  next({ status: 404, message: `Product cannot be found.` });
}
Update the list() function in products.controller.js
In products.controller.js, modify the list() function as follows:

async function list(req, res, next) {
  const data = await productsService.list();
  res.json({ data });
}
Update suppliers.controller.js
In suppliers.controller.js, modify the supplierExists() function as follows:

async function supplierExists(req, res, next) {
  const supplier = await suppliersService.read(req.params.supplierId);
  if (supplier) {
    res.locals.supplier = supplier;
    return next();
  }
  next({ status: 404, message: `Supplier cannot be found.` });
}
Modify the create() function as follows:

async function create(req, res) {
  const data = await suppliersService.create(req.body.data);
  res.status(201).json({ data });
}
Modify the update() function as follows:

async function update(req, res) {
  const updatedSupplier = {
    ...req.body.data,
    supplier_id: res.locals.supplier.supplier_id,
  };
  const data = await suppliersService.update(updatedSupplier);
  res.json({ data });
}
Modify the destroy() function, as follows:

async function destroy(req, res) {
  const { supplier } = res.locals;
  await suppliersService.delete(supplier.supplier_id);
  res.sendStatus(204);
}
Use Postman to test your routes and make sure that your CRUD functionality is still working.

Error handling
If a promise is rejected inside async/await code, an error is thrown and needs to be handled.

Express was written before JavaScript contained promises. As a result, errors thrown by async handlers and middleware require a little work to make sure that you catch the error and pass it on to next(). There are three options when it comes handling errors in asynchronous code:

Use the then and catch methods of the promise (which is what you have done so far).

Use async and await and wrap the asynchronous code with try/catch.

Use async and await and pass the function to a higher-order function that handles errors for you.

All of these approaches are valid.

For example, you can add error handling to the list() function in categories.controller.js by using try...catch like the following code:

async function list(req, res, next) {
  try {
    const data = await categoriesService.list();
    res.json({ data });
  } catch (error) {
    next(error);
  }
}
The above code will catch any error and pass it to next() so that the request will fail.

One alternative to try/catch is to use a higher-order function to handle the async error for you. Rather than adding try/catch to the function, wrap the function in an error boundary when it is exported from the controller. The following code defines the asyncErrorBoundary() function.

Now, create src/errors/asyncErrorBoundary.js and add the following code:

function asyncErrorBoundary(delegate, defaultStatus) {
  return (request, response, next) => {
    Promise.resolve()
      .then(() => delegate(request, response, next))
      .catch((error = {}) => {
        const { status = defaultStatus, message = error } = error;
        next({
          status,
          message,
        });
      });
  };
}

module.exports = asyncErrorBoundary;
The asyncErrorBoundary() function takes two parameters:

delegate, which is an async/await handler or middleware function. This function will be called by the asyncErrorBoundary.

defaultStatus is an optional parameter that allows you to override the status code returned when delegate throws an error.

asyncErrorBoundary returns an Express handler or middleware function, which is eventually called by Express in place of the delegate function.

Here's a breakdown of the syntax:

Promise.resolve().then(() => delegate(request, response, next)) makes sure that the delegate function is called in a promise chain. Using Promise.resolve() to call delegate means that the value returned is guaranteed to have a catch() method, even if delegate isn't an async function.

The catch() method will default error to {} in the unlikely event that error is undefined (which will make sure that the destructuring in the next line doesn't fail).

Next, the error object is destructured to status and message variables. By defaulting message to error, error can be a String or Error object.

Finally, next() is called, passing in status and message.

Now, make use of the asyncErrorBoundary by wrapping the async functions when exported by the *.controller.js files:

In categories.controller.js, change the export to the following:

const asyncErrorBoundary = require("../errors/asyncErrorBoundary");

// ...

module.exports = {
  list: asyncErrorBoundary(list),
};
In products.controller.js, change the export to the following:

const asyncErrorBoundary = require("../errors/asyncErrorBoundary");

// ...

module.exports = {
  read: [asyncErrorBoundary(productExists), read],
  list: asyncErrorBoundary(list),
};
In suppliers.controller.js, change the export to the following:

const asyncErrorBoundary = require("../errors/asyncErrorBoundary");

// ...

module.exports = {
  create: [
    hasOnlyValidProperties,
    hasRequiredProperties,
    asyncErrorBoundary(create),
  ],
  update: [
    asyncErrorBoundary(supplierExists),
    hasOnlyValidProperties,
    hasRequiredProperties,
    asyncErrorBoundary(update),
  ],
  delete: [asyncErrorBoundary(supplierExists), asyncErrorBoundary(destroy)],
};
In the remaining lessons, you will be using only the async and await syntax to write asynchronous code. You will also wrap the async functions with asyncErrorBoundary to handle any possible errors thrown from the async functions.

You will be using the same repository in the next lesson, so make sure to save the work that you complete in this lesson.

Complete example
A completed example from this lesson can be found here:

GitHub: Node, Express, and Postgres—async-and-await-in-express-complete branch

*** 35.8 Aggregates with Knex and JavaScript
Aggregates with Knex and JavaScript
1.5 hoursEstimated completion time
Learning Objective
By the end of this lesson, you will be able to return aggregate data using standard Knex queries and JavaScript. You'll also be able to return aggregate data using Knex queries that make use of aggregate clauses.

Overview
When you're working with tables in the real world, you'll typically need to retrieve aggregate data from your tables. For example, you may need to retrieve counts, averages, and sums of the values of a given column or array of columns. In this lesson, you'll modify your routes to do just that.

At Thinkful Gifts, the inventory management department has stated that they will need to be able to access the following sets of inventory statistics via the API:

HTTP verb
Path
Description
GET
/products/out-of-stock-count
Returns a count of all out-of-stock products
GET
/products/price-summary
Returns the average, minimum, and maximum prices of products sourced from a supplier
GET
/products/total-weight-by-product
Returns the total weight (in lbs ) of each product type being held in the inventory. The total weight is calculated by taking the quantity of each product multiplied by the weight of each product.
In this lesson, your challenge is to modify your routes to meet the criteria above. The start project doesn't contain any code for above routes. You will need to make changes to multiple files, including the router, controller, and service files, in order to fully implement the the criteria above.

Starter code
This lesson continues using the project you created in the previous lesson. If you need to, you can download that code here:

GitHub: Node, Express, and Postgres Starter
Knex count() method
The Knex count() method performs a count on the specified column or array of columns. In PostgreSQL, count() returns a bigint type, which will be a string.

knex("products").count("product_id");
This translates to the following:

SELECT COUNT("product_id") FROM "products";
GET /products/out-of-stock-count endpoint
The API should return a count of all out-of-stock products. You will need to make use of the Knex count() method.

Do this
Add a new query builder function to products.service.js
In products.service.js, add the following listOutOfStockCount() method:

function listOutOfStockCount() {
  return knex("products")
    .select("product_quantity_in_stock as out_of_stock")
    .count("product_id")
    .where({ product_quantity_in_stock: 0 })
    .groupBy("out_of_stock");
}
The query above selects the product_quantity_in_stock column (aliased as out_of_stock) from the products table. It also selects a count all of the products where product_quantity_in_stock is set to 0. Finally, it groups the result by the out_of_stock column.

To export your method from the file, make sure to include the function in the module.exports object, as follows:

module.exports = {
  list,
  read,
+ listOutOfStockCount,
};
Add the listOutOfStockCount() handler to products.controller.js
In products.controller.js, add a listOutOfStockCount() handler, which calls the getOutOfStockCount() query builder method that you added to ProductsService in the previous step:

async function listOutOfStockCount(req, res, next) {
  res.json({ data: await productsService.listOutOfStockCount() });
}
Export your handler via module.exports:

module.exports = {
  read: [asyncErrorBoundary(productExists), read],
  list: asyncErrorBoundary(list),
+  listOutOfStockCount: asyncErrorBoundary(listOutOfStockCount),
};
Add the /products/out-of-stock-count route to products.router.js
On your own, make the necessary changes to products.router.js to handle GET requests to /products/out-of-stock-count.

Verify GET localhost:5000/products/out-of-stock-count
Now, either use Postman to send a GET request, or navigate to localhost:5000/products/out-of-stock-count in the browser. You will get back a result that looks like the following, showing that there are four products that are out of stock:

{
  data: [
    {
      out_of_stock: 0,
      count: "4",
    },
  ];
}
Knex min(), max(), and avg() methods
The min() method
The Knex min() method retrieves the minimum value for the specified column. It accepts the column name as a parameter. For example, consider this code:

knex("products").min("product_price");
This translates to the following:

SELECT MIN("product_price") FROM "products";
The max() method
The Knex max() method retrieves the maximum value for the specified column. It accepts the column name as a parameter. For example, look at the following code:

knex("products").max("product_price");
This translates to the following:

SELECT MAX("product_price") FROM "products";
The avg() method
The Knex avg() method retrieves the average value for the specified column. It accepts the column name as a parameter. For example, look at the following code:

knex("products").avg("product_price");
This translates to the following:

SELECT AVG("product_price") FROM "products";
GET /products/price-summary endpoint
The API should return the average, minimum, and maximum prices of products sourced from a supplier. You will need to make use of the Knex min(), max(), and avg() methods.

Do this
Add a new query builder function to products.service.js
In products.service.js, add the following listPriceSummary() method:

function listPriceSummary() {
  return knex("products")
    .select("supplier_id")
    .min("product_price")
    .max("product_price")
    .avg("product_price")
    .groupBy("supplier_id");
}
The query above selects the supplier_id column from the products table and returns the minimum, maximum, and average values of the product_price column, grouped by the supplier_id column.

To export your method from the file, make sure to include the function in the module.exports object.

Add the listPriceSummary() handler to products.controller.js
In products.controller.js, add a listPriceSummary() handler, which calls the getPriceSummary() query builder method that you added to ProductsService in the previous step:

async function listPriceSummary(req, res, next) {
  res.json({ data: await productsService.listPriceSummary() });
}
Export the handler function via module.exports:

module.exports = {
  read: [asyncErrorBoundary(productExists), read],
  list: asyncErrorBoundary(list),
  listOutOfStockCount: asyncErrorBoundary(listOutOfStockCount),
+ listPriceSummary: asyncErrorBoundary(listPriceSummary),
};
Add the /products/price-summary route to products.router.js
On your own, make the necessary changes to products.router.js to handle GET requests to /products/price-summary.

Verify GET localhost:5000/products/price-summary
Now, either use Postman to send a GET request, or navigate to localhost:5000/products/price-summary in the browser. You will get back a result that looks like the following:

{
  "data": [
      {
          "supplier_id": 4,
          "min": "3.99",
          "max": "699.99",
          "avg": "230.5940000000000000"
      },
...
The Knex raw() method
When a Knex method isn't available to perform a given query, you may need to use a raw SQL expression in a query. You can inject a raw query object pretty much anywhere you want using the raw() method, which accepts a raw query as a parameter. For example, if you want to find the sum of the result of multiplying two columns together, and then store the result in a new column called total, the syntax is as follows:

knex("products").select(raw("sum(column_1_value * column_2_value) as total"));
This translates to the following:

SELECT SUM(column_1_value * column_2_value) as total
FROM "products"
GET /products/total-weight-by-product endpoint
The API should return the total weight (in lbs) of each product being held in the inventory. The total weight is calculated by taking the quantity of each product multiplied by the weight of each product. You will need to use the knex.raw() to write a plain SQL query to perform the calculation.

Do this
Add a new query builder function to products.service.js
In products.service.js, add the following listTotalWeightByProduct() method:

function listTotalWeightByProduct() {
  return knex("products")
    .select(
      "product_sku",
      "product_title",
      knex.raw(
        "sum(product_weight_in_lbs * product_quantity_in_stock) as total_weight_in_lbs"
      )
    )
    .groupBy("product_title", "product_sku");
}
The query above selects the product_sku, product_title, and a third special column. This third column consists of the sum of multiplying the values from two columns (product_weight_in_lbs and product_quantity_in_stock) from the products table. The result is then grouped by product_title and product_sku.

To export your method from the file, make sure to include the function in the module.exports object.

Add the listTotalWeightByProduct() handler to products.controller.js
In products.controller.js, add a listTotalWeightByProduct() handler, which calls the listTotalWeightByProduct() query builder method that you added to productsService in the previous step:

async function listTotalWeightByProduct(req, res) {
  res.json({ data: await productsService.listTotalWeightByProduct() });
}
Export the handler via module.exports:

module.exports = {
  read: [asyncErrorBoundary(productExists), read],
  list: asyncErrorBoundary(list),
  listOutOfStockCount: asyncErrorBoundary(listOutOfStockCount),
  listPriceSummary: asyncErrorBoundary(listPriceSummary),
+ listTotalWeightByProduct: asyncErrorBoundary(listTotalWeightByProduct),
};
Add the /products/total-weight-by-product route to products.router.js
On your own, make the necessary changes to products.router.js to handle GET requests to /products/total-weight-by-product.

Verify GET localhost:5000/products/total-weight-by-product
Now, either use Postman to send a GET request, or navigate to localhost:5000/products/total-weight-by-product in the browser. You will get back a result that looks like the following:

{
  "data": [
      {
          "product_sku": "giUGNvnn8Q",
          "product_title": "Reserved for the Dog Cushion",
          "total_weight_in_lbs": "1100.00"
      },
      {
          "product_sku": "ay6srUUaht",
          "product_title": "Laudable Accomplishments of the Warrior: A Fate Forfeit",
          "total_weight_in_lbs": "898.01"
      },
...
You will be using the same repository in the next lesson, so make sure to save the work that you complete in this lesson.

Complete example
A completed example from this lesson can be found here:

GitHub: Node, Express, and Postgres—aggregates-with-knex-and-javascript branch
*** 35.9 Joins with Knex
Joins with Knex
1.5 hoursEstimated completion time
Learning Objective
By the end of this lesson, you will be able to return joined data with Knex.

Overview
In this lesson, you'll learn how to create queries in Knex to pull data from more than one table.

In the previous lesson, you wrote Knex queries to get aggregate data such as counts, minimums, maximums, averages, and sums from your tables. For example, in the previous lesson, you wrote a Knex query to return the minimum, maximum, and average prices of products sourced from each supplier from the products table. However, only the supplier_id is included in your result, because that's the foreign key reference in the products table.

What if you also wanted to include the supplier_name and supplier_email columns in your result? Because these columns are stored in the suppliers table, you'd have to join the products and suppliers tables.

If you need to retrieve data from multiple tables in your query, you can create queries in Knex that pull data from more than one table. The inventory management department has stated that they will need to be able to access the following sets of data via the API as well:

HTTP verb
Path
Description
GET
/products/:productId
Return a specific product, including all of its related category information
You will now get to work modifying the existing API endpoint above to return joined data.

Starter code
This lesson continues using the project you created in the previous lesson. If you need to, you can download that code here:

GitHub: Node, Express, and Postgres Starter
You will be using the same repository in the next lesson, so make sure to save the work that you complete in this lesson.

Knex joins
In the previous module, you learned how to write SQL queries to link or relate the data located in different tables. For example, to retrieve all columns from two tables (products and suppliers) with plain SQL, the join syntax looks like the following:

SELECT p.*, s.*
FROM products as p
JOIN suppliers as s
ON p.supplier_id = s.supplier_id;
A corresponding query in Knex would look like this:

knex("products as p")
  .join("suppliers as s", "p.supplier_id", "s.supplier_id")
  .select("p.*", "s.*");
The join() method above accepts the name of the join table as the first parameter. The next two parameters are the names of the columns that contain the values on which the join is based (products.supplier_id and suppliers.supplier_id in the example above). Similar to the SELECT statement in SQL, you also chain a select() method to determine the columns to include in the query result.

To link multiple tables, you can chain multiple join() methods. For example, to link the products, categories, and products_categories tables, you can chain join("products_categories as pc", "p.product_id", "pc.product_id") and join("categories as c", "pc.category_id", "c.category_id") to knex("products as p"), like this:

knex("products as p")
  .join("products_categories as pc", "p.product_id", "pc.product_id")
  .join("categories as c", "pc.category_id", "c.category_id");
Modify the GET /products/:productId endpoint
Your API should return a specific product, including all of its related category information. To get this information, you will need to use the productsCategoriesJoin base query, which relates the products, products_categories, and categories tables.

Do this
Add a new query builder function to products.service.js
In products.service.js, modify the existing read() query builder function as follows:

function read(product_id) {
  return knex("products as p")
    .join("products_categories as pc", "p.product_id", "pc.product_id")
    .join("categories as c", "pc.category_id", "c.category_id")
    .select("p.*", "c.*")
    .where({ "p.product_id": product_id })
    .first();
}
Now, either use Postman to send a GET request, or navigate to localhost:5000/products/1 in the browser. You will get back a result that looks like the following:

{
  "data": {
    "product_id": 1,
    "product_sku": "XLH4P7t3er",
    "product_title": "Vanilla Scented Candle",
    "product_description": "Vanilla-scented candle, perfect for your living room.",
    "product_price": "48.25",
    "product_quantity_in_stock": 0,
    "product_weight_in_lbs": "2.30",
    "supplier_id": 1,
    "created_at": "2020-12-01T20:37:09.550Z",
    "updated_at": "2020-12-01T20:37:09.550Z",
    "category_id": 2,
    "category_name": "candles",
    "category_description": "A category for gift candles, including both scented and non-scented candles, for the home."
  }
}
Notice that the JSON you are getting back from the server is represented in a "flat" way. However, a REST API would typically provide related objects, like category, as an object nested within the JSON.

Add category object
The nested data output for the /products/:productId endpoint should look like the following:

{
  "data": {
    "id": 1,
    "sku": "XLH4P7t3er",
    "title": "Vanilla Scented Candle",
    "description": "Vanilla-scented candle, perfect for your living room.",
    "price": "48.25",
    "quantity_in_stock": 0,
    "weight_in_lbs": "2.30",
    "supplier": {
      "id": 1
    },
    "created_at": "2020-12-01T20:37:09.550Z",
    "updated_at": "2020-12-01T20:37:09.550Z",
    "category": {
      "category_id": 2,
      "category_name": "candles",
      "category_description": "A category for gift candles, including both scented and non-scented candles, for the home."
    }
  }
}
Note the category property is a nested object that groups all of the related category properties.

Next you will write a function to group the category_id, category_name, and category_description columns into a single nested object:

Do this
Group category properties into a nested object
First, run npm install lodash to install lodash.

Then, create a new utils folder. Within it, create a file called map-properties.js.

Add the following code to map-properties.js:

const lodash = require("lodash");

function mapProperties(configuration) {
  return (data) => {
    if (data) {
      return Object.entries(data).reduce((accumulator, [key, value]) => {
        return lodash.set(accumulator, configuration[key] || key, value);
      }, {});
    }
    return data;
  };
}

module.exports = mapProperties;
The above code accepts a configuration parameter which is an object where the key specifies the original property name and the value specifies the new property name. The mapProperties() function returns a new function that can be used over and over to modify multiple data objects.

The following is the configuration to convert the category properties to a nested category object:

{
  category_id: "category.category_id",
  category_name: "category.category_name",
  category_description: "category.category_description",
}
In the above configuration, the values specify the "path" of the property, where . is the delimiter. . is used like / or \ for a path folder delimiter. If a portion of the path doesn't exist, it's created. Arrays are created for missing index properties while objects are created for all other missing properties. Any property that isn't in the configuration is left unchanged.

Then, in src/products/products.service.js, make the following changes:

// At the top of the file:
+ const mapProperties = require("../utils/map-properties");

+ const addCategory = mapProperties({
+   category_id: "category.category_id",
+   category_name: "category.category_name",
+   category_description: "category.category_description",
+ });

// Then modify the `read()` function
function read(product_id) {
   return knex("products as p")
     .join("products_categories as pc", "p.product_id", "pc.product_id")
     .join("categories as c", "pc.category_id", "c.category_id")
     .select("p.*", "c.*")
     .where({ "p.product_id": product_id })
     .first()
+    .then(addCategory);
}
Now, either use Postman to send a GET request, or navigate to localhost:5000/products/1 in the browser. You should get back a data object with a nested category object!

Complete example
A completed example from this lesson can be found here:

GitHub: Node, Express, and Postgres—joins-with-knex branch
*** 35.10 Assessment: Node, Express, and Postgres
** Backend Deployment - Module 36
*** 36.1 Overview: Backend deployment
Overview: Backend deployment
9 minutesEstimated completion time
Overview
It's time for you to learn how to put the servers that you've built onto the web. There are many different ways to do this, and some are easier than others. This module will begin by briefly discussing what it means to deploy a backend application that includes a database. Then, you'll learn concrete steps that you can take to put your servers onto the web.

In general, this module is focused on how to accomplish these tasks. Although there are many ways to accomplish them, you won't necessarily learn about the variety of deployment techniques here. Keep in mind that as you grow as a developer and work in different contexts during your career, you will learn additional ways to deploy to the web.

Do this
The purpose of the Do this sections in this module is to give you important hands-on experience. In these sections, you'll perform various tasks, like setting up a development environment or executing a command. Ultimately, these practice sections will help you successfully complete graded assessments, such as projects, mock interviews, and capstones.
*** 36.2 What is backend deployment?
What is backend deployment?
1.5 hoursEstimated completion time
Learning Objective
By the end of this lesson, you will be able to describe what deployment means.

Overview
Deployment is one of those technical terms that gets thrown around often without much explanation. In this lesson, you'll learn what deploying an application generally involves. You'll also learn about what kinds of services that process may include.

Sharing your work
Right now, your servers can only be hosted locally. That is, if someone wants to access the routes accessible on your server and the data available through it, they'll either need to do it on your computer or download your code and run it themselves.

To make your application accessible on the web, you'll need to deploy your application. Deploying a web application typically means making it accessible on the web through a URL.

Deployment is a way of sharing your work with the world by putting it on the web.

Development, staging, and production
When you deploy, you deploy to a certain environment. There are typically three environments:

The development environment references work done on your computer.

The staging environment references a deployed version of the application that is used to test the changes being made.

The production environment references a deployed version of the application that is your "completed" version of the application. This version is ready for users.

In this program, you'll have just two environments: development and production.

The deployment process
To deploy a server, you'll need to recreate your development environment's essential components on another machine. That machine will need to be configured to run your server and will be accessible to the internet.

You will also need to build a secure connection to your database. Typically, deploying the backend of an application involves both the server and the database. But because you have been working with a cloud database throughout this program, you will have little extra to do in order to "deploy" your database.

Manually deploying a server and setting up an environment is an entire topic in itself, often referred to as developer operations or DevOps. Thankfully, there are a number of modern tools and services that make deployment much easier.

Keep in mind that all of these services are often configuration heavy. That means that you need to follow a certain set of steps in a particular order. It's a good idea to take your own notes on the deployment process; this can be extremely useful for later on.

Common services
There are a number of common tools and services that you may hear about when it comes to deployment. Some of the most popular options are described below.

Amazon Web Services
As one of the most popular solutions out there, Amazon Web Services (AWS) runs the web. Working with AWS can be complicated and costly for new developers, but it's an industry standard for most large websites.

AWS allows you to host applications in all kinds of languages. They provide a number of tools to facilitate this process, including the popular Elastic Beanstalk tool. They also provide a variety of other services, like file and database hosting.

Microsoft Azure
Microsoft Azure is similar to AWS in that it provides a number of different services for hosting applications of all kinds. Azure is built for large websites and projects, and it can host applications in a variety of languages.

Heroku
Heroku is one of the more popular options, particularly among those who are just starting to code. Thanks to its easy-to-use command-line interface and excellent UI, Heroku can perform many of the same tasks that AWS and Azure can, with a bit less fuss.

CI/CD
The acronym CI/CD stands for continuous integration and continuous delivery (or continuous deployment). Continuous integration is a set of practices and the automated process of building, testing, and merging an application in a shared repository. CI allows for better software quality and collaboration. Tools such as Heroku CI and Travis CI are often used by developers to implement continuous integration in a project. After the changes made to an application are tested with a CI tool, the application must be deployed. And with continuous delivery, the new version of the application is automatically deployed into a testing or production environment. Some services, such as Heroku, will automatically deploy your application when changes are pushed into the project repository's main branch. In the following lesson, you will learn how to follow a continuous delivery process with Heroku.

*** 36.3 Deploying the server
Deploying the server
1.5 hoursEstimated completion time
Learning Objective
By the end of this lesson, you will be able to deploy an Express.js app on Heroku.

Overview
In this lesson, you are going to deploy an app using Heroku. As you learned in the Frontend deployment module, Heroku allows you to deploy web applications to the cloud for free using a couple of commands. In this lesson, you will use Heroku to deploy an Express.js app from scratch.

Setup
Go to Heroku's website and click the Sign Up button. You will receive an email when your account is created and authorized.

Next, install the heroku CLI tool. To install the correct npm package, run the command given on the download page, using the npm package manager option:

npm install -g heroku
Now, run heroku --version. It will output a version number to verify that the CLI tool has been installed.

Do this
Get set up with Heroku
If you already have an account with Heroku, you may ignore this step. If not, follow the instructions above to create an account with Heroku. Then, install the heroku CLI tool as described.

Command-line configuration
For Heroku to work as intended, you need to log in via the command line. You can log in by running this command:

heroku login
After entering the command, you will get something like this:

heroku: Press any key to open up the browser to login or q to exit:
Press any key (except q) to log in.

Do this
Log in to Heroku
Follow the instructions above to log in to Heroku. If you encounter any problems, reach out for help.

Deploying to Heroku
To deploy with the CLI, the command is heroku create. Note that Node.js is a supported framework on Heroku, so you can simply use heroku create to create the project for the backend.

heroku create
Heroku's build system will automatically detect the required buildpack, but if you are using an unsupported language or framework, you will be required to use a custom buildpack. Heroku provides a list of official and unofficial buildpacks.

Do this
Deploy an Express.js app to Heroku
Create a folder and create a server.js file inside the folder that you just created. Then copy the following snippet into your new server.js file:

const express = require('express')
const cors = require('cors')

const app = express()
const router = express.Router()
const PORT = process.env.PORT || 5000

router.get('/', cors(), (req, res) => {
  res.json({ message: 'Hello Heroku!' });
})

app.use('/', router);

app.listen(PORT, () => {
  console.log(`Server running on ${PORT} `);
})

module.exports = app
Create a package.json file and copy the following snippet into it.

{
  "name": "name-generator",
  "version": "1.0.0",
  "engines": {
    "node": "12.13.0"
  },
  "description": "",
  "main": "server.js",
  "scripts": {
    "start": "node server",
    "server": "nodemon server.js",
    "client": "npm start --prefix client"
  },
  "keywords": [],
  "dependencies": {
    "express": "^4.17.1",
    "cors": "^2.8.5"
  },
  "devDependencies": {
    "nodemon": "^2.0.3"
  }
}
Next, install and run your app to make sure that everything works as expected.

npm install
npm start
Then initialize a Git repository.

git init
Next, add a .gitignore file in the root folder of your repository to specify files that Git should ignore.

# Dependency directories
node_modules/
Run heroku create to create your project in Heroku.

heroku create
You will get something like this:

Creating app... done, ⬢ secure-beyond-60922
https://secure-beyond-60922.herokuapp.com/ | https://git.heroku.com/secure-beyond-60922.git
Now, add and commit your code.

git add .
git commit -m "first commit"
Then simply push your code to the Heroku repository.

git push heroku main
To see your deployed app, use heroku open.

heroku open
Redeploying
To redeploy your app, add, commit, and push your new changes to Heroku.

git add .
git commit -m "updated project"
git push heroku main

*** 36.4 Deploying a monorepo
Deploying a monorepo
1 hourEstimated completion time
Overview
In this lesson, you will use starter code to deploy a monorepo.

Key Terms
Monorepo
A repository that contains both the client and the backend projects
Starter code
This lesson requires you to have the following monorepo running on your local machine:

Starter Name Generator

Fork and clone the repository. Then, follow the instructions to get the app running.

Monorepo
A monorepo is a repository that contains both the client and the backend projects. Having both projects in a single repository means that you can open both projects in the same editor.

The table below describes the folders in the starter repository:

Folder/file path
Description
./backend
The backend project, which runs on localhost:5000 by default.
./client
The frontend project, which runs on localhost:3000 by default.
You will deploy both apps on Heroku.

Do this
Deploy a monorepo to Heroku
First, cd into the name-generator app (starter code).

cd starter-name-generator
Use heroku create to create the Heroku project for the server.

heroku create name-generator-backend
You will see something similar to this:

Creating ⬢ name-generator-backend... done
https://name-generator-backend.herokuapp.com/ | https://git.heroku.com/name-generator-backend.git
Tip
Heroku app names must be unique, so you will have to change the name of your app name-generator-backend. Rename it to whatever you like!

Now, use heroku create for the client application:

heroku create name-generator-client
You will see something similar to this:

Creating ⬢ name-generator-client... done
https://name-generator-client.herokuapp.com/ | https://git.heroku.com/name-generator-client.git
Great! You just used the heroku create command to create the URLs and repositories for your backend and client.

Now, use git remote add to add the Heroku Git repositories that you just created.

For example, if the previous commands generated https://git.heroku.com/name-generator-backend and https://git.heroku.com/name-generator-client, you would run the following commands:

git remote add heroku-backend https://git.heroku.com/name-generator-backend.git
git remote add heroku-client https://git.heroku.com/name-generator-client.git
Now, add and commit your code.

git add .
git commit -m "first commit"
Because you are using two different repositories in your project, you need to push your changes using the subtree command. The subtree command allows you to nest one or more repositories inside another as a subdirectory. In short, this command allows you to manage multiple repositories in one; it's commonly used in monorepo projects.

Use git subtree to push your code to the Heroku backend and client repos.

git subtree push --prefix backend heroku-backend main
git subtree push --prefix client heroku-client main
To see your deployed app, use heroku open with the -a flag to indicate the app that you want to open.

heroku open -a name-generator-client
Redeploying
To redeploy your app, add, commit, and push your new changes to Heroku.

As an example, go to client/App.js and change the fetch URL to the one that you just deployed.

useEffect(() => {
  fetch('https://name-generator-backend.herokuapp.com/generate')
    .then((res) => res.json())
    .then((nme) => setName([nme]))
}, [])
Then add, commit, and push your changes.

git add .
git commit -m "Fix fetch URL"
git subtree push --prefix backend heroku-backend main
git subtree push --prefix client heroku-client main
You can see your application's metrics, logs, deployment history, and settings in the application's dashboard.

*** 36.5 Database setup
Database setup
1 hourEstimated completion time
Learning Objective
By the end of this lesson, you will be able to run your migrations on a production server.

Overview
Because you are already working with a cloud database, connecting your database with your backend application will be relatively simple. It is important to connect them in order for your server to work properly.

Starter code
This lesson requires you to have the following repository running on your local machine.

GitHub: Backend deployment starter
Fork and clone the repository. Then, follow the instructions to get it to run.

Next, make sure that you have two different database URLs for this lesson. You will need a development database and a production database.

Database environments
Recall that you may have different types of environments—such as development, staging, and production—for your application. It is typical for all of these environments to have different databases to keep their data separate from one another.

Having separate databases is especially important for your development process. You want to be able to add new migrations and rerun your seed files without worrying about how that will affect your production database.

To make working with different databases manageable, it's typical to define a DATABASE_URL environment variable. Then, wherever you would make use of the URL to your database, you can use DATABASE_URL instead. Although you can set the DATABASE_URL from the command line, it's typical to use a tool like dotenv.

Do this
Find DATABASE_URL
In the starter-backend-deployment project, take a moment to find where DATABASE_URL is getting used. You should find it in the following files:

.env.sample

knexfile.js

Create a .env file
In the filename, .sample refers to the fact that this is an example of what the real file (which should be called .env) should look like. Run the following command from the command line to make a copy of it. Make sure that you are in the correct directory before running the command.

cp .env.sample .env
Inside that file, you'll notice that two new environment variables are being set:

DEVELOPMENT_DATABASE_URL=""
PRODUCTION_DATABASE_URL=""
One of these environment variables, DEVELOPMENT_DATABASE_URL, will be for your development database. The other, PRODUCTION_DATABASE_URL, will be for your production database.

Tip
You will be using two databases for this project. If you are using ElephantSQL, make sure that you create two databases: one for development and the other one for production.

Inside the quotations for DEVELOPMENT_DATABASE_URL, add a link to a database that you can use as a development database. Inside the quotations for PRODUCTION_DATABASE_URL, add a link to a database that you can use as a production database.

Next, take a look at the .gitignore file for this project. Notice that .env is included as a file to be ignored. This is important so that you do not push the URL to your database up to GitHub.

Inspect the knexfile.js
Now that you have your .env file set up, take a look at the knexfile.js file. At the top of the file, dotenv is being used to load the .env file:

require("dotenv").config();
This line is required if you want to make use of the .env file.

Below that, you'll see that a URL variable is set so that if the NODE_ENV environment variable is equal to the string "production", PRODUCTION_DATABASE_URL will be used. Otherwise, DEVELOPMENT_DATABASE_URL will be used.

const {
  NODE_ENV = "development",
  DEVELOPMENT_DATABASE_URL,
  PRODUCTION_DATABASE_URL,
} = process.env;
const URL =
  NODE_ENV === "production"
    ? PRODUCTION_DATABASE_URL
    : DEVELOPMENT_DATABASE_URL;
Run the migrations
To check that your database is now connected to your applications, you can run the migrations. From the command line, run the following:

npm run knex -- migrate:latest
Note that you're running migrations on your development database because NODE_ENV isn't set explicitly to "production".

If you encounter an issue, it could be one of the following problems:

You may encounter a problem that says that there are missing migrations if you're reusing an existing database. Make sure to use a new database or delete all of the tables in the existing database.

You may encounter a connection problem if your database URL is incorrect or if you haven't saved your .env file.

If the migrations run correctly, your local application can now be connected to your database.

Migrations on production
At this point, you now have a working local setup. To run migrations on your production database, you should run the migration command, but this time with NODE_ENV being equal to the "production" string, like this:

NODE_ENV=production npm run knex -- migrate:latest
Do this
Run your production migrations
Run the command above to run the migrations on your production database. You will see an output that's similar to what you saw when you ran the command locally—except this time, you will see that knex is using the production environment.

Using environment: production
Batch 1 run: 1 migrations
Deploy your application
Before deploying your application to Heroku, you need to create a Procfile. A Procfile is a file that tells Heroku what to do after installing an application. You can use the Procfile to specify any commands that are to be executed by the app on startup.

Create a file named Procfile, and copy the following line:

web: node ./api/index.js
The above code will tell Heroku to run index.js, which contains the server code. To read more about the Procfile, consult Heroku's Procfile documentation.

Now that you have created the Procfile, log in to Heroku.

heroku login
Then run the heroku create command.

heroku create
After running heroku create, you will get something like this.

Creating app... done, ⬢ cryptic-everglades-55740
https://cryptic-everglades-55740.herokuapp.com/ | https://git.heroku.com/cryptic-everglades-55740.git
Copy the Git URL from the previous command and add it as a remote.

git remote add heroku-backend https://git.heroku.com/cryptic-everglades-55740.git
Then, add, commit, and push your files to Heroku.

git add .
git commit -m "add files to heroku"
git push heroku-backend main
An additional step is needed for the app to work. You need to set the environment variables in Heroku with the heroku config:set command. Copy the PRODUCTION_DATABASE_URL that you set in your .env file and paste it after the command.

heroku config:set PRODUCTION_DATABASE_URL=postgres://phrawv:QTHOwQKmpVDvEwwG-Vpx9jZsG98EOS@batyr.db.elephantsql.com/phrawv --app cryptic-everglades-55740
You will get something like this:

Setting PRODUCTION_DATABASE_URL and restarting ⬢ cryptic-everglades-55740... done, v9
PRODUCTION_DATABASE_URL: postgres://phrawv:QTHOwQKmpVDvEwwG-Vpx9jZsG98EOS@batyr.db.elephantsql.com/phrawv
Once you have set the database URL, try visiting your deployed application at the /api/products route. You will see an empty response because you haven't seeded the production database.

This is fine; production databases aren't usually seeded with fake data. This is because they are meant to interact with real users. But if you do want to seed the production database with fake data, you can run the knex -- seed:run command on your production database. Then, if you try the /api/products route again, you'll get the following response.

{
  "data": [
    {
      "product_id": 2,
      "product_name": "Chocolate Sandwich Cookies",
      "aisle": "cookies cakes",
      "department": "snacks",
      "price": 31.06
    }
    // ...
  ]
}
Once you've completed all of the steps above, you may mark this lesson as complete.

*** 36.6 Troubleshooting
Troubleshooting
1 hourEstimated completion time
Learning Objective
By the end of this lesson, you will be able to debug some common problems that arise when deploying a backend server and database.

Overview
Unlike coding, debugging deployment can be tricky if you aren't sure what you're looking for. In this lesson, you'll be reminded of some of the most common problems that come up when deploying.

Tip
Make sure to reach out for assistance early if you encounter any problems with deployment. Often, big problems are caused by small mistakes.

Read your application's logs on Heroku
Click the View logs button in your application's dashboard and look for any errors.

View logs button in application dashboard in Heroku.
Troubleshoot the heroku command
If running the heroku command leads to a failure, carefully read through the error that is presented to understand what has gone wrong.

If you've forgotten to log in to your account, or have gotten logged out, you may need to log back in through the command line (by running heroku login) before you can deploy.

Check your database URL
A single wrong character in your database URL can lead to a failed connection to your database. You won't actually encounter this error until your server attempts to make a request.

For example, locally, you may see an UnhandledPromiseRejectionWarning with more information in the terminal tab where you're running your server. This is likely a sign that something is wrong with your connection. Also, check your application's error handling and investigate if any of your functions is missing a catch handler.

Try copying and pasting your database URL once again, assigning it to the appropriate environment variable.

Confirm environment variables
If you continue to have connection troubles, ensure that you have a .env file and that the environment variables set in that file are spelled correctly and used correctly throughout the application. Remember that you can always set environment variables via the command line to test out different environments.

Ensure that your migrations have run
It's possible that you may have run your migrations locally but not on your production database. You can always attempt to rerun your migrations with your NODE_ENV set to "production"—if they've already been run, nothing additional will happen.

Note: This isn't true for seeds. Seeds will delete everything in your database before inserting rows.
*** 36.7 Assessment: Backend deployment
** Connecting it all - Module 37
*** 37.1 Overview: Connecting it all
Overview: Connecting it all
9 minutesEstimated completion time
Overview
This module will begin by reviewing how the web works and how what you have learned so far fits into a larger internet ecosystem. Then, you'll learn how to add features, organize your workflow, and further optimize your applications.

It's time for you to take what you have learned and put it together into an effective full-stack workflow. You'll learn to take some concrete steps to add new features and capture information about what is happening in your applications that are running in production. There are many different ways to organize your workflow, and some are easier than others.

Do this
The purpose of the Do this sections in this module is to give you important hands-on experience. In these sections, you'll perform various tasks, like setting up a logging framework. Ultimately, these practice sections will help you successfully complete graded assessments, such as projects, mock interviews, and capstones.

*** 37.2 Review of how the web works
Review of how the web works
1.5 hoursEstimated completion time
Learning Objective
By the end of this lesson, you will be able to describe the request-response cycle and identify the purpose behind each part of a full-stack application.

Overview
So far, you have built multiple frontend applications, created simple and complex APIs, and used database servers to store data for APIs. No doubt your understanding of how the web works has become more complex. In this lesson, you will review the basics of how web requests and responses work.

Key Terms
Request-response cycle
The path that information takes from the client to the server and then back to the client
Three-tier architecture
A way of building a web application in three separate pieces: the frontend, the backend, and the database
Presentation tier
The frontend tier in a three-tier architecture, which consists of the user interface
Application tier
The backend tier in a three-tier architecture, which contains the functional business logic that drives an application's core capabilities
Data tier
The database tier in a three-tier architecture, which consists of a database
The request-response cycle
As you know, the web is a cycle of requests and responses that flow between clients and servers. The client sends a request to the server, which then processes this request and responds to the client. Often, the client will then do something with this information, such as render a web page. This process is called the request-response cycle. The request-response cycle refers to the path that information takes from the client to the server and then back to the client.

When you first learned about the request-response cycle, you probably thought of it as something like the following diagram:

Client sending a request (method and URL) and server sending a response (status code and body).
This process is repeated multiple times for a single view of a web page. For example, on this page, your browser made multiple requests for HTML, CSS, JavaScript, and image files.

As you have progressed through the modules and lessons, you have used this simple request-response cycle to build increasingly complex web applications. You have learned that a web application has several parts; specifically, you have learned about the frontend, the backend, and the database. Now, you know that the request-response cycle really looks more like the following diagram:

Client (browser), React app (frontend), Express API (backend), and PostgreSQL (database) sending a series of requests and responses to each other.
Notice that the application above consists of three separate pieces: the frontend, the backend, and the database. This specific way of building a web application is called a three-tier architecture.

What is a three-tier architecture?
A three-tier architecture is a type of software architecture that is composed of three tiers or layers. This type of architecture provides many benefits for production and development environments by modularizing the user-interface, business-logic, and data-storage layers. This modularization allows developers to update and deploy one layer of an application independently of the other layers.

This added flexibility can improve overall time to market. It can also decrease development cycle times by making it possible for developers to replace or upgrade one tier without affecting the other parts of the system. For example, a web application's user interface could be redesigned to change the color layout, without touching the underlying functional business and data-access logic underneath. A three-tier architecture is often used in software-as-a-service (SaaS) applications.

The presentation tier
The presentation tier, which is the frontend layer in a three-tier architecture, consists of the user interface. This tier is often built on web technologies, such as HTML5, JavaScript, CSS, or other popular web development frameworks. This layer generally communicates with other layers through API calls. When requests or data are processed in a three-tiered application, this process usually starts and ends in the presentation layer.

As you've learned, requests are usually in the form of objects; these objects represent data that your application is trying to retrieve or save. For example, you may want to set up an account for a new customer, or you may want to retrieve transaction history for an existing customer.

Validation also takes place in this layer. The validation helps ensure that all the data elements that are needed to process a given request are present. If any required data elements are missing, the request processing stops, and the user is notified about the missing data.

The application tier
The application tier contains the functional business logic that drives an application's core capabilities. This logic tier is also the only one that writes and reads data into the data tier.

This tier handles requests sent from the presentation tier and contains business logic to determine how to process the request. Examples of this include the following:

When creating a new user account, make sure that the email address entered isn't already associated with an existing user account.

When ordering something from an e-commerce site, make sure that there is sufficient inventory to fulfill the order.

When creating a new item for sale on an e-commerce site, make sure that the sale price is greater than the cost of the product.

Implement security features to prevent hackers from getting unauthorized access to the data.

The data tier
The data tier consists of a database, which can be hosted onsite or in the cloud. This tier manages read and write access to the database. Some popular database systems for managing read-write access include MySQL, PostgreSQL, Microsoft SQL Server, and MongoDB.

Advantages and disadvantages of a three-tier architecture
Advantages
A three-tier architecture reduces dependencies between tiers. As long as each tier follows the interface standards, this architecture allows different developers to work on each layer. As a result, the system can be developed much faster.

Each tier can be changed, redeveloped, or modernized without affecting other tiers in a three-tier architecture. This shortens time to market and reduces the cost of adding new features.

Only the application tier is allowed to access the data tier. This improves data security.

Disadvantages
It can reduce system performance. Most requests from the presentation tier access both the application tier and the data tier; this adds the overhead of multiple network requests and delays for each request.

It can result in cascading changes from the top down. If you need to add a feature in the presentation tier, you may need to add code in both the business-logic tier and the data-access tier in order to implement the feature fully.

*** 37.3 CORS
CORS
1.5 hoursEstimated completion time
Learning Objective
By the end of this lesson, you will be able to describe the same-origin policy and why it is important. You'll also be able to enable cross-origin resource sharing for a REST API.

Overview
This lesson covers some important concepts in web application security. In this lesson, you'll learn how to enable cross-origin resource sharing (CORS). CORS allows servers to specify not only which websites can access the API, but also which HTTP methods can be used, and which headers of the response can be accessed by the JavaScript running in the browser.

Key Terms
Same-origin policy
A policy in which a web browser allows scripts contained in one web page to access data at a different URL, but only if the web page and URL have the same origin
Cross-site request forgery
CSRF, an attack that tricks the victim into submitting a malicious request
Cross-origin resource sharing
CORS, a mechanism that allows a web page from one origin to access a resource at a different origin
Cookie
A small piece of data stored on the user's computer by the web browser, providing a way for websites to remember stateful information
Starter code
This lesson requires you to have the following repositories running on your local machine.

GitHub: Starter CORS backend

GitHub: Starter CORS frontend

Fork and clone the above repositories. Then, follow the instructions to get each to run.

The same-origin policy
The same-origin policy is an important concept in web application security. Under this policy, a web browser allows scripts contained in one web page to access data at a different URL, but only if the web page and URL have the same origin.

Key Term
Origin: The combination of the protocol, the domain, and the port of the URL used to access web content

As an example, if the origin is https://mybank.com, the code running on that page can access https://mybank.com/api and comply with the same-origin policy. The same-origin policy was added to browsers to close some cross-domain vulnerabilities, as explained below.

Note: The origin is the combination of protocol, domain, and port. This means that https://api.mydomain.com and https://mydomain.com are actually different origins. And http://localhost:3000 and http://localhost:5000 are also different origins. The path or query parameters are ignored when considering the origin.

The cross-domain vulnerability
Imagine that, after logging on to https://mybank.com, you later browse to https://fluffykittens.com, a malicious website, without first logging out of https://mybank.com. Without the same-origin policy, the JavaScript embedded in the https://fluffykittens.com website could transfer money out of your savings account by sending a POST request to https://mybank.com/savings. This request would succeed, because the cookie from https://mybank.com would prove that you are logged in, even though the money transfer request didn't come from https://mybank.com.

Before browsers had a same-origin policy, the browser would automatically attach any cookies tied to https://mybank.com to all HTTP requests to that domain, including embedded JavaScript requests from sites like https://fluffykittens.com, to https://mybank.com.

By restricting HTTP calls to the same origin (the same domain as the browser tab's domain), this closes some hacker backdoors such as cross-site request forgery (CSRF). Note that it doesn't close all backdoors; additional mechanisms like CSRF tokens are still necessary.

Cross-site request forgery is an attack that tricks the victim into submitting a malicious request. It inherits the identity and privileges of the victim to perform an undesired function on the victim's behalf. If the user is currently authenticated to the site, the site will have no way to distinguish between the forged request sent on behalf of the victim and a legitimate request sent by the victim.

As you can see, the same-origin policy improves the overall security of websites. However, it creates some additional challenges when you are building a three-tier application, because you typically deploy each tier to a different domain. For example, the presentation tier could be deployed to https://www.mybank.com, the application tier could be deployed to https://api.mybank.com, and the data tier could be deployed to https://data.mybank.com. In this case, you need a way to tell the browser that it is okay for pages at https://www.mybank.com to request resources from https://api.mybank.com. This is done by enabling cross-origin resource sharing, as explained below.

What is cross-origin resource sharing?
Cross-origin resource sharing (CORS) is a mechanism that allows a web page from one origin to access a resource at a different origin. In other words, it makes cross-domain requests possible. CORS provides a way for a website to relax the same-origin policy and share resources with specified origins only, thereby preserving the intended security provided by the same-origin policy.

How does CORS work?
If a request is cross-origin, the browser always adds an Origin header to the request. For example, if the page at https://mybank.com/accounts/savings makes a request to https://api.mybank.com/savings, the Origin header will be Origin: https://mybank.com. As you can see, the Origin header contains the exact origin (protocol, domain, and port), without a path, query parameters, or document fragment.

The server can inspect the Origin header. If it agrees to accept such a request, it will add a Access-Control-Allow-Origin header to the response. For example, to allow access from any origin, you could set the header as follows:

Access-Control-Allow-Origin: *
Allowing any origin is what you would want for a public API that anyone can access. For example, the random dog API at https://random.dog/woof.json includes an Access-Control-Allow-Origin: * header in every request, even if you visit that URL directly.

More frequently, the allowed origin is narrowed down to one specific origin, like this:

Access-Control-Allow-Origin: https://mybank.com
Note: The Access-Control-Allow-Origin header specification is very strict. It can only include an asterisk * or exactly one origin. It cannot include more than one origin, and it cannot include * in the origin. You will learn how to allow multiple origins later in this module.

When making cross-origin requests, the browser plays the role of a trusted mediator by doing the following:

Ensuring that the Origin header is sent with a cross-origin request.

Checking the Access-Control-Allow-Origin header in the response. If the request is allowed, the browser will then allow JavaScript to access the response. Otherwise, the request will fail with an error.

CORS is a bit complicated. Fortunately, the Express cors package handles most of the complexity for you. Next, you will enable CORS for a GET request.

Do this
Enable CORS for a GET request
Now, start the following starter projects:

GitHub: Starter CORS backend

GitHub: Starter CORS frontend

Start the backend project using npm run start:dev so that it will automatically restart the server when you make changes.

Then visit http://localhost:3000. Open Dev Tools by pressing Control+Shift+C (or Command+Option+C on a Mac). You will see something like the following:

CORS test in Dev Tools.
Next, click the Submit button. You will see an error in the console:

Access to fetch at 'http://localhost:5000/cors-enabled' from origin 'http://localhost:3000' has been blocked by CORS policy: No 'Access-Control-Allow-Origin' header is present on the requested resource. If an opaque response serves your needs, set the request's mode to 'no-cors' to fetch the resource with CORS disabled.
Error in Dev Tools.
Now, update the /cors-enabled GET endpoint in src/cors-enabled/cors-enabled.router.js of the backend project.

Change the code as follows:

const router = require("express").Router();
const controller = require("./cors-enabled.controller");
const methodNotAllowed = require("../errors/methodNotAllowed");
+const cors = require("cors");

router
  .route("/:corsId")
  .get(controller.read)
  .put(controller.update)
  .delete(controller.delete)
  .all(methodNotAllowed);

router
  .route("/")
- .get(controller.list)
+ .get(cors(), controller.list)
  .post(controller.create)
  .all(methodNotAllowed);

module.exports = router;
Tip
The default configuration for cors() middleware enables CORS for every origin. You should be more restrictive in a production application by allowing only origins that require access to the API.

Finally, submit the request again, and it will be successful:

Successful Get request submitted in Dev Tools.
What you just did enabled CORS for a simple request. There is also another type of request called a preflight request.

Understanding CORS request types
There are two types of CORS requests: simple requests and preflight requests. It's the browser that determines which is used. As the developer, you won't normally need to pay attention to this when you are constructing requests to be sent to a server. However, you may see the different types of requests appear in your network log. And because it may have a performance impact on your application, you will want to know why and when these requests are sent.

Simple requests and preflight requests
The browser deems a request to be a simple request when, among other things, the request is a GET or POST, and the Content-Type header is application/x-www-form-urlencoded, multipart/form-data, or text/plain.

Any other request is considered a preflight request. For example, a request with a PUT method is considered a preflight request. This also means that every request to an API server with a Content-Type of application/json is a preflight request.

A preflight request can use any HTTP method—not just GET and POST, but also PUT, PATCH, DELETE, and others.

Before CORS, no web page could submit a DELETE or other such request. So, there were many servers that would consider a nonstandard method (not GET or POST) to mean, "That isn't a browser." To avoid misunderstandings with any non-simple request (any request that couldn't be done in the early days of browsers), the browser doesn't make non-simple requests right away. First, it sends a preliminary, preflight request, asking for permission.

A preflight request uses the OPTIONS method, no body, and the following headers:

Access-Control-Request-Method header, which is set to the method of the non-simple request, such as DELETE

Access-Control-Request-Headers header, which is set to a comma-separated list of the header names that will be part of the non-simple request, such as Authorization,Content-Type

If the server agrees to serve the requests, then it will respond with a status 200, an empty body, and the following headers:

Access-Control-Allow-Origin, which must be set to either * or to the requesting origin, such as https://mybank.com, to allow it.

Access-Control-Allow-Methods, which must specify the desired request method.

Access-Control-Allow-Headers, which must have a list of allowed headers.

Access-Control-Max-Age header (optional), which must be set to a number of seconds to cache the permissions. This way, the browser won't have to send a preflight request for subsequent requests that satisfy given permissions.

Preflight requests enable the browser to ask for the server's permission before making requests with certain HTTP methods and headers. This permissions model puts the server in charge of how cross-origin requests behave.

Do this
Enable CORS for a preflight request
Start by visiting http://localhost:3000. Then follow these steps:

Open Dev Tools.

Change the remote URL to http://localhost:5000/cors-enabled/07e05d3fa75f44c0b3ee06530c01629e.

Change the method to DELETE.

Click Submit.

Now you will see something like the following:

New remote URL and Method changed to Delete.
Now update the /cors-enabled/:corsId DELETE endpoint in src/cors-enabled/cors-enabled.router.js of the backend project.

const router = require("express").Router();
const controller = require("./cors-enabled.controller");
const methodNotAllowed = require("../errors/methodNotAllowed");
const cors = require("cors");

router
  .route("/:corsId")
  .get(controller.read)
  .put(controller.update)
- .delete(controller.delete)
+ .delete(cors(), controller.delete)
  .all(methodNotAllowed);

router
  .route("/")
  .get(cors(), controller.list)
  .post(controller.create)
  .all(methodNotAllowed);

module.exports = router;
Back in the frontend, click Submit again, and you will see the same error. This is because the preflight request using OPTIONS failed.

Switch to the Network tab, and you will see the following. (Note that you may need to clear the network data and click Submit again.)

Delete error on Network tab in Dev Tools.
The order of these network requests may seem out of order, but they are in the correct order. You initiated the DELETE request by clicking the Submit button. The DELETE request then triggered the OPTIONS request.

The OPTIONS request failed because CORS isn't enabled for the OPTIONS method of that route. Click the OPTIONS request and look at the response headers. There is no Access-Control-Allow-Origin header, so the browser blocked access to the response.

Access to response blocked due to no Access-Control-Allow-Origin header.
For a preflight request, you must enable CORS for both the OPTIONS method and any methods that may be a preflight request.

Next, you will update the code to enable CORS for the OPTIONS method of the /cors-enabled/:corsId route.

const router = require("express").Router();
const controller = require("./cors-enabled.controller");
const methodNotAllowed = require("../errors/methodNotAllowed");
const cors = require("cors");

+ const corsDelete = cors({methods: "DELETE"});

router
  .route("/:corsId")
  .get(controller.read)
  .put(controller.update)
-  .delete(cors(), controller.delete)
+  .delete(corsDelete, controller.delete)
+ .options(corsDelete)
  .all(methodNotAllowed);

router
  .route("/")
  .get(cors(), controller.list)
  .post(controller.create)
  .all(methodNotAllowed);

module.exports = router;
Now submit the DELETE request again, and it will succeed.

Successful resubmit of Delete request.
Note: The OPTIONS method must also allow CORS requests for a CORS preflight request to work.

Notice that this time, you passed some configuration information to the cors() method. Specifically, you passed cors({methods: "DELETE"}), which enables CORS only for the DELETE method. Any CORS request using a method other than OPTIONS or DELETE will fail for this router.

Try a GET request to http://localhost:5000/cors-enabled/55b5a6f60ddc4b3cba36e2a1ec5c6990, and it will fail.

Get request failed in Dev Tools.
You will fix this error next, but rather than enabling CORS on each method, you will enable CORS for the entire route.

Enable CORS for a single route
Sometimes, you want to enable CORS for every method on a route. Rather than adding cors() as middleware to each method handler, you can add cors() to the entire route.

Do this
Enable CORS for the /:corsId route
Now, update the /cors-enabled/:corsId endpoint in src/cors-enabled/cors-enabled.router.js of the backend project.

const router = require("express").Router();
const controller = require("./cors-enabled.controller");
const methodNotAllowed = require("../errors/methodNotAllowed");
const cors = require("cors");

- const corsDelete = cors({methods: "DELETE"});

router
  .route("/:corsId")
+ .all(cors())
  .get(controller.read)
  .put(controller.update)
- .delete(corsDelete, controller.delete)
- .options(corsDelete)
+ .delete(controller.delete)
  .all(methodNotAllowed);

router
  .route("/")
  .get(cors(), controller.list)
  .post(controller.create)
  .all(methodNotAllowed);

module.exports = router;
Try the GET request to http://localhost:5000/cors-enabled/55b5a6f60ddc4b3cba36e2a1ec5c6990 again, and it will succeed.

Successful Get request in Dev Tools.
Next, make sure that the server still responds with 405 when you use a method that isn't handled. For example, a POST request to http://localhost:5000/cors-enabled/55b5a6f60ddc4b3cba36e2a1ec5c6990 should return 405.

Post request returning 405 error.
Now that you have CORS enabled for the entire /:corsId route, you will change the configuration again to enable CORS for the entire router.

Enable CORS for a router
Next, you will enable CORS for the entire router. Rather than adding all(cors()) to both routes, you can add cors() to the router.

First, make sure that CORS isn't enabled for a POST request to /cors-enabled. In the frontend, change the method to POST, and the body text area will appear.

Post the following to http://localhost:5000/cors-enabled:

{
  "data": {
    "message": "NEW CORS enabled data"
  }
}
Then you will see the following error:

Post errors in Dev Tools.
Do this
Enable CORS for the router
Now, make the following changes to the code in src/cors-enabled/cors-enabled.router.js of the backend project.

const router = require("express").Router();
const controller = require("./cors-enabled.controller");
const methodNotAllowed = require("../errors/methodNotAllowed");
const cors = require("cors");

+router.use(cors())

router
  .route("/:corsId")
- .all(cors())
  .get(controller.read)
  .put(controller.update)
  .delete(controller.delete)
  .all(methodNotAllowed);

router
  .route("/")
  .get(cors(), controller.list)
  .post(controller.create)
  .all(methodNotAllowed);

module.exports = router;
Try the POST request again, and it will succeed.

Successful Post request in Dev Tools.
Next, you will enable CORS for the entire server.

Enable all CORS requests
If you are building an API that is intended to be used from any website, you may want to enable CORS for the entire API.

Do this
Enable CORS for everything
Now make the following changes to the code in src/cors-enabled/cors-enabled.router.js of the backend project.

const router = require("express").Router();
const controller = require("./cors-enabled.controller");
const methodNotAllowed = require("../errors/methodNotAllowed");
const cors = require("cors");

-router.use(cors())

router
  .route("/:corsId")
  .get(controller.read)
  .put(controller.update)
  .delete(controller.delete)
  .all(methodNotAllowed);

router
  .route("/")
  .get(cors(), controller.list)
  .post(controller.create)
  .all(methodNotAllowed);

module.exports = router;
Then make the following changes to the code in src/app.js of the backend project.

const express = require("express");
+const cors = require("cors");
const app = express();

const corsEnabledRouter = require("./cors-enabled/cors-enabled.router");
const corsNotEnabledRouter = require("./cors-not-enabled/cors-not-enabled.router");
const notFound = require("./errors/notFound");
const errorHandler = require("./errors/errorHandler");

+app.use(cors());
app.use(express.json());

app.use("/cors-enabled", corsEnabledRouter);
app.use("/cors-not-enabled", corsNotEnabledRouter);

app.use(notFound);

app.use(errorHandler);

module.exports = app;
Now, CORS is enabled for the entire API.

Submit a GET request to http://localhost:5000/cors-not-enabled, and you will see the following:

Get request submitted to //localhost:5000/cors-not-enabled.
Unless otherwise required, you can enable CORS for the entire server for the APIs that you create in this program. However, if you are building a publicly available API, you should be more restrictive with the CORS configuration.

Additional CORS permissions
There are additional CORS features that also require special permissions. A common one is user credential support. By default, CORS doesn't attach user credentials, such as cookies, on requests.

Another is response headers support. The browser doesn't reveal all response headers to the JavaScript client code. If your server responds with an X-Powered-By response header, the JavaScript client code won't be able to read its value without permission.

As with all CORS permissions that you've learned about, the server is in charge of enabling them, and it does so by using HTTP headers.

Enabling additional CORS permissions is out of scope for this lesson. You can read more about these permissions on the MDN cross-origin resource sharing page.
*** 37.4 Full-stack application workflow
Full-stack application workflow
1.5 hoursEstimated completion time
Learning Objective
By the end of this lesson, you will be able to describe common full-stack workflows.

Overview
When you are adding a feature or fixing a bug in a full-stack three-tier application, you are faced with a decision: where do you start making changes? Do you change the backend repository first, or do you start with the frontend repository? Like everything in software development, each approach has its advantages and disadvantages.

Key Terms
Inside-out development flow
A full-stack workflow that involves making changes starting with the backend, and fully implementing the feature for the current layer before making any changes to the layer above it
Outside-in development flow
A full-stack workflow that involves making changes starting with the frontend, fully implementing a small part of the feature through each layer of the architecture
Full-stack application workflow
As mentioned above, anytime that you have a new feature to add to your application, or a defect to fix in the application, you need to choose where to start making changes. Do you start with the backend, in a workflow known as inside-out development? Or do you start with the frontend, in a workflow known as outside-in development?

Inside-out and outside-in are two common full-stack workflows. You will learn about both workflows in this lesson.

Inside-out development
Following an inside-out development workflow, you start by making the necessary changes to the data tier. Then you make the changes to the application tier. And finally, you make changes to the presentation tier so that the user can access the new feature.

When following an inside-out development flow, work is done in horizontal slices, as depicted in the diagram below.

Inside-out development workflow.
Each horizontal slice of work fully implements the feature for the current layer before making any changes to the layer above it. Then the changes to each architectural layer are combined before the feature is available to the user.

Inside-out development offers the following advantages:

Changes to the data tier are made early. Some developers believe that the data tier is the most valuable tier in the application.

Business requirements are clarified early. Some questions about the business requirements only emerge when working on the application tier.

The user interface is left to the end. The UI is notorious for frequent changes, so delaying the frontend work may result in less rework on the frontend.

But this workflow also has the following disadvantages:

It's easy to create accidental features. You may start thinking about all of the possibilities of the backend code and try to make the API as robust and generic as possible. As a result, you may create functionality that has no clear concrete need from the UI or the user.

It's easy to miss some UI needs. When you try to connect the UI, you may find that the backend doesn't exactly satisfy all of the needs of the frontend. This can result in usability compromises that make the customer unhappy.

Feedback is delayed. Users care about the UI. If the frontend is done after the backend, you will only get feedback when the whole feature is done, and that is too late.

Frontend development may be rushed. When the frontend is delayed, you may run out of time. Rushing the frontend implementation causes two main problems:

A bad user experience

Lower quality frontend, leading to difficult-to-maintain software

Outside-in development
Following an outside-in development workflow, you first implement the necessary changes to the presentation tier, then the application tier, and finally the data tier. That may sound counterintuitive, and it generally makes backend developers shake their heads.

But is it the wrong choice? Well, building the UI will give you quick feedback from users. If they don't like how the UI is working, you can change it easily without any backend impact, because there is no backend yet. Once everyone agrees on the UI, you will know the exact data that the backend will need to provide.

When following an outside-in development flow, work is done in small vertical slices, as depicted in the following diagram.

Outside-in development workflow.
Each vertical slice of work fully implements a small part of the feature through each layer of the architecture. It only implements exactly what is required by the layer above, and nothing more. Once the small feature is implemented, it can be made available to the user immediately.

Outside-in development offers the following advantages:

Feedback is provided early. The user only cares about the presentation layer, so getting feedback on the UI early gives you confidence while working on the application and data tiers.

It's unlikely that accidental features will be created. All changes are driven from the UI, so you are focused on minimal changes.

The focus is on user needs. Code is only written to satisfy a user need, so you build what really needs to be built and nothing more.

But this workflow also has the following disadvantages:

Backend development may be rushed. When the backend is delayed, you may run out of time. Fortunately, it is much easier to improve the backend implementation after release without impacting the user.

There's delayed clarification of some business requirements. Some questions about the business requirements only emerge when working on the application tier.

Working outside-in using small vertical slices means that you will do the following:

Write just enough code to implement the UI

Get value sooner

Tend to avoid accidentally building low-value changes

Get earlier, higher-quality feedback

Become more predictable in delivering features (because small working features become the measure of progress)

For the above reasons, you will follow the outside-in development workflow in the next lesson.

*** 37.5 Practice full-stack application workflow
Practice full-stack application workflow
1.5 hoursEstimated completion time
Learning Objective
By the end of this lesson, you will be able to make changes to a full-stack application that has a frontend repository and backend repository.

Overview
Your knowledge about outside-in development workflow helped get you hired as a full-stack developer at WeatherZen, an exciting new startup that is crowdsourcing local weather observations from their users. In this lesson, you'll practice implementing several user stories for WeatherZen.

Starter code
This lesson requires you to have the following repositories running on your local machine.

GitHub: Starter WeatherZen backend
GitHub: Starter WeatherZen frontend
Fork and clone the above repositories. Then, follow the instructions to get each to run.

Scenario
It's your first day as a full-stack developer at WeatherZen, and the product owner has several user stories queued up for you to implement. The first one is to allow a user to create a new weather observation.

New weather observation
User story
As a user, I would like to create a new weather observation, so that other people have accurate information about my local weather.

Acceptance criteria
Given that I am on the New Weather Observation page, when I submit a weather observation using the form, then I am redirected to the Weather Observations page and see the weather observation that I posted at the top of the list.

A weather observation has the following inputs:

Latitude

Must be between -90 and 90, inclusive

Longitude

Must be between -180 and 180, inclusive

Sky condition: User selects one of the following:

Cloudless (value = 100)

Some clouds (value = 101)

Cloud covered (value = 102)

Foggy (value = 103)

Raining (value = 104)

Snowing (value = 106)

Hailing (value = 108)

Thunderstorms (value = 109)

The API will return an observation as JSON similar to the following:

{
  "observation_id": 2,
  "latitude": 0,
  "longitude": 0,
  "sky_condition": 100,
  "created_at": "2020-12-10T08:30:32.326Z",
  "updated_at": "2020-12-10T08:30:32.326Z"
}
This user story provides a minimal weather observation. Additionally, this story is small enough to be implemented as a small vertical slice.

Note: Because this is the first feature involving weather observations, it might take a bit longer as you add new pages, API routes, and database tables to the application. However, each subsequent user story related to weather observations will be a bit easier.

Do this
Implement new weather observation
You will start in the frontend project by creating a new route and minimal UI.

First, make the following changes to src/layout/Layout.js:

 import Menu from "./Menu";
 import { Route, Switch } from "react-router-dom";
 import Home from "../home/Home";
+import ObservationCreate from "../observations/ObservationCreate";

 function Layout() {
   return (
       <Menu />
       <div className="container">
         <Switch>
+          <Route path="/observations/new">
+            <ObservationCreate />
+          </Route>
           <Route exact={true} path="/">
            <Home />
           </Route>
           <Route>
             <NotFound />
           </Route>
         </Switch>
       </div>
     </>
   );
 }

 export default Layout;
Create src/observations/ObservationCreate.js. Then add the following:

import React from "react";
import { useHistory } from "react-router-dom";

function ObservationCreate() {
  const history = useHistory();

  function cancelHandler() {
    history.push("/");
  }

  function submitHandler(event) {
    event.preventDefault();
    history.push("/");
  }

  return (
    <main>
      <h1>Create Observation</h1>
      <form onSubmit={submitHandler}>
        <p>Later, input fields will be added here.</p>
        <div>
          <button
            type="button"
            className="btn btn-secondary mr-2"
            onClick={cancelHandler}
          >
            Cancel
          </button>
          <button type="submit" className="btn btn-primary">
            Submit
          </button>
        </div>
      </form>
    </main>
  );
}

export default ObservationCreate;
Now you have a small implementation that will allow you to verify that the routing and the Cancel and Submit buttons are working.

Start the application and click the New Observation link on the right side of the menu bar. You will see something similar to the following screenshot.

Example view after clicking New Observation.
Next, click the Submit and Cancel buttons to make sure that you return to the home page.

Implement ObservationCreate component
Now, update src/observations/ObservationCreate.js to include the latitude, longitude, and sky_condition fields, and call createObservation() (which doesn't yet exist) when the form is submitted.

Edit src/observations/ObservationCreate.js as follows:

-import React from "react";
+import React, { useState } from "react";
 import { useHistory } from "react-router-dom";
+import { createObservation } from "../utils/api";

 function ObservationCreate() {
   const history = useHistory();

+  const [observation, setObservation] = useState({
+    latitude: "",
+    longitude: "",
+    sky_condition: "",
+  });
+
   function cancelHandler() {
     history.push("/");
   }

   function submitHandler(event) {
     event.preventDefault();
-    history.push("/");
+    createObservation(observation).then(() => {
+      history.push("/");
+    });
   }
+
+  function changeHandler({ target: { name, value } }) {
+    setObservation((previousObservation) => ({
+      ...previousObservation,
+      [name]: value,
+    }));
+   }

   return (
     <main>
-      <h1>Create Observation</h1>
-      <form onSubmit={submitHandler}>
-        <p>Later, input fields will be added here.</p>
+      <h1 className="mb-3">Create Observation</h1>
+      <form onSubmit={submitHandler} className="mb-4">
+        <div className="row mb-3">
+          <div className="col-6 form-group">
+            <label className="form-label" htmlFor="latitude">
+              Latitude
+            </label>
+            <input
+              className="form-control"
+              id="latitude"
+              name="latitude"
+              type="number"
+              max="90"
+              min="-90"
+              value={observation.latitude}
+              onChange={changeHandler}
+              required={true}
+            />
+            <small className="form-text text-muted">Enter a value between -90 and 90.</small>
+          </div>
+          <div className="col-6">
+            <label className="form-label" htmlFor="longitude">
+              Longitude
+            </label>
+            <input
+              className="form-control"
+              id="longitude"
+              name="longitude"
+              type="number"
+              max="180"
+              min="-180"
+              value={observation.longitude}
+              onChange={changeHandler}
+              required={true}
+            />
+            <small className="form-text text-muted">Enter a value between -180 and 180.</small>
+          </div>
+        </div>
+        <div className="mb-3">
+          <label className="form-label" htmlFor="cloudCover">
+            Sky conditions
+          </label>
+          <select
+            className="form-control"
+            id="sky_condition"
+            name="sky_condition"
+            value={observation.sky_condition}
+            onChange={changeHandler}
+            required={true}
+          >
+            <option value="">Select a sky condition option</option>
+            <option value="100">Cloudless</option>
+            <option value="101">Some clouds</option>
+            <option value="102">Cloud covered</option>
+            <option value="103">Foggy</option>
+            <option value="104">Raining</option>
+            <option value="106">Snowing</option>
+            <option value="108">Hailing</option>
+            <option value="109">Thunderstorms</option>
+          </select>
+        </div>
         <div>
           <button
             type="button"
   ...
   // Remaining code omitted for brevity
Now, the New Observation page will look something like the following:

Create Observation page.
Next, you will add a createObservation() function to src/utils/api.js. This function will eventually send a POST request to add the data to the backend API route /observations, but that route doesn't exist yet in the backend. For now, you will simulate that API call using a local array to store the observations.

Edit src/utils/api.js as follows:

// Existing code omitted for brevity

+const observations = [];
+
+function nextId() {
+  const uint32 = window.crypto.getRandomValues(new Uint32Array(1))[0];
+  return uint32.toString(16);
+}
+
+export async function createObservation(observation, signal) {
+  const now = new Date().toISOString();
+  const newObservation = {
+    ...observation,
+    observation_id: nextId(),
+    created_at: now,
+    updated_at: now,
+  };
+  observations.push(newObservation);
+  return newObservation;
+}
Now, when the user fills in the form and clicks the Submit button, the new observation will be saved. Note that any time that the page is reloaded, the observations will be lost. Additionally, there is no way for the user to see the saved observations, if there are any.

The next step is to update the home page to display a list of the saved observations.

Edit src/home/Home.js as follows:

+import { useEffect, useState } from "react";
+import { listObservations } from "../utils/api";
+import ErrorAlert from "../layout/ErrorAlert";
+
 function Home() {
+  const [observations, setObservations] = useState([]);
+  const [error, setError] = useState(null);
+
+  useEffect(() => {
+    const abortController = new AbortController();
+    listObservations(abortController.signal)
+      .then(setObservations)
+      .catch(setError);
+    return () => abortController.abort();
+  }, []);
+
+  const tableRows = observations.map((observation) => (
+    <tr key={observation.observation_id}>
+      <th scope="row">{observation.observation_id}</th>
+      <td>{observation.latitude}</td>
+      <td>{observation.longitude}</td>
+      <td>{observation.sky_condition}</td>
+      <td>{observation.created_at}</td>
+    </tr>
+  ));
+
   return (
     <main>
       <h1>Home</h1>
+      <ErrorAlert error={error} />
+      <table className="table">
+        <thead>
+        <tr>
+          <th scope="col">#</th>
+          <th scope="col">Latitude</th>
+          <th scope="col">Longitude</th>
+          <th scope="col">Sky Condition</th>
+          <th scope="col">Created</th>
+        </tr>
+        </thead>
+        <tbody>
+        {tableRows}
+        </tbody>
+      </table>
     </main>
   );
 }

export default Home;
Edit src/utils/api.js as follows:

// Existing code omitted for brevity

+export async function listObservations(signal) {
+  return observations;
+}
Now is a great time to demo the UI to the product owner, so go ahead and create a new observation.

Note: To get the coordinates of a place, start by opening Google Maps. (Make sure not to use Lite mode, indicated with a lightning bolt icon, because you won't be able to get the coordinates of a place in Lite mode.) Then right-click the place or area on the map. Select What's here? and you'll see a card with the coordinates.

When redirected to the home page, it should look something like the following:

WeatherZen Home page.
Once the product owner approves the UI, it's time to start calling the real API.

Implement POST /observations
Now that most of the work in the presentation layer is complete, you need to change the API project by creating a new route.

The first step is to start the GitHub: Starter WeatherZen frontend project. The next step is to update the createObservation() method to call the API.

Edit src/utils/api.js as follows:

-const observations = [];
-
-function nextId() {
-  const uint32 = window.crypto.getRandomValues(new Uint32Array(1))[0];
-  return uint32.toString(16);
-}
-
 export async function createObservation(observation, signal) {
-  const now = new Date().toISOString();
-  const newObservation = {
-    ...observation,
-    observation_id: nextId(),
-    created_at: now,
-    updated_at: now,
+  const url = `${API_BASE_URL}/observations`;
+  const options = {
+    method: "POST",
+    headers,
+    body: JSON.stringify({ data: observation }),
+    signal,
   };
-  observations.push(newObservation);
-  return newObservation;
+  return await fetchJson(url, options);
 }

 export async function listObservations(signal) {
-  return observations;
+  return [];
 }
Now go to the New Observation page and try to create a new observation. Because the API route doesn't exist yet, you expect to see an error displayed, but there is no error. If you look in the console, you will see the 404 error.

Now that you know that the error isn't handled, you will update the ObservationCreate component to handle the error.

In src/observations/ObservationCreate.js:

 import React, { useState } from "react";
 import { useHistory } from "react-router-dom";
 import { createObservation } from "../utils/api";
+import ErrorAlert from "../layout/ErrorAlert";

 function ObservationCreate() {

   // Code omitted for brevity

+  const [error, setError] = useState(null);

   // Code omitted for brevity

   function submitHandler(event) {
     event.preventDefault();
-    createObservation(observation).then(() => {
-      history.push("/");
-    });
+    createObservation(observation)
+      .then(() => {
+        history.push("/");
+      })
+      .catch(setError);
   }

   // Code omitted for brevity

   return (
     <main>
       <h1 className="mb-3">Create Observation</h1>
+      <ErrorAlert error={error} />
       <form onSubmit={submitHandler} className="mb-4">
         <div className="row mb-3">
           <div className="col-6 form-group">

   // Code omitted for brevity
Now the error displays as expected:

Error on Create Observation page.
The previous error is an example of the type of problem that is easy to catch when working outside-in, but very difficult to detect when working inside-out. If you build the API route first, then there is nothing to help ensure that you properly handle errors in the UI.

It is now time to open the backend project and add the POST /observations route.

Edit src/app.js as follows:

 const app = express();

 app.use(cors())
 app.use(express.json());

+let nextId = 1;
+app.post('/observations', ((req, res) => {
+  const newObservation = req.body.data;

+  newObservation.observation_id = nextId++
+
+  res.status(201).json({
+    data: newObservation,
+  });
+}))

 // Remaining code omitted for brevity
You may have been tempted to create the controller and router for observations. But by creating a simple inline handler for the POST request, you can get the API working in minutes and test the integration between the frontend and backend.

Switch back to the UI and create a new observation. If the new route is working, the POST request works, but the new observation doesn't show up on the home page.

Now is a good time to create the observations router and controller.

Edit src/app.js as follows:

 // Code omitted for brevity

+const observationsRouter = require('./observations/observations.router')
+
 const app = express();

 app.use(cors())
 app.use(express.json());

-let nextId = 1;
-app.post('/observations', ((req, res) => {
-  const newObservation = req.body.data;
-
-  newObservation.observation_id = nextId++
-
-  res.status(201).json({
-    data: newObservation,
-  });
-}))
+app.use("/observations", observationsRouter);

 app.use(notFound);
 app.use(errorHandler);
 // Code omitted for brevity
Edit src/observations/observations.router.js as follows:

const router = require("express").Router({ mergeParams: true });
const controller = require("./observations.controller");
const methodNotAllowed = require("../errors/methodNotAllowed");

router.route("/").post(controller.create).all(methodNotAllowed);

module.exports = router;
Edit src/observations/observations.controller.js as follows:

let nextId = 1;

async function create(req, res) {
  const newObservation = req.body.data;

  const now = new Date().toISOString();
  newObservation.observation_id = nextId++;
  newObservation.created_at = now;
  newObservation.updated_at = now;

  res.status(201).json({
    data: newObservation,
  });
}

module.exports = {
  create,
};
Again, switch back to the UI and create a new observation. It should work the same as it did before you created the controller and router.

Update the UI project to call the API to GET /observations.

Edit src/utils/api.js as follows:

 // Code omitted for brevity

 export async function listObservations(signal) {
-  return [];
+  const url = `${API_BASE_URL}/observations`;
+  const options = {
+    headers,
+    signal,
+  };
+  return await fetchJson(url, options);
 }
Again, switch back to the UI. You will now see an error displayed on the home page.

Error on home page.
Create validation
The UI requires a value for every field and makes sure that the values are valid. The API has to have exactly the same validation. As you know, there is nothing that prevents someone from using Postman or other tools to call your API, so it is necessary to implement the same validation in the API.

The validation for an observation is as follows:

Latitude is a number between -90 and 90.

Longitude is a number between -180 and 180.

Sky condition must be one of these values: 100, 101, 102, 103, 104, 106, 108, 109.

Edit src/observations/observations.controller.js as follows:

 let nextId = 1;
+
+const validSkyConditions = [100, 101, 102, 103, 104, 106, 108, 109]
+
+function hasData(req, res, next) {
+  if (req.body.data) {
+    return next()
+  }
+  next({status: 400, message: "body must have data property"})
+}
+
+function hasLatitude(req, res, next) {
+  const latitude = Number(req.body.data.latitude)
+  if (latitude >= -90 && latitude <= 90 ) {
+    return next()
+  }
+  next({status: 400, message: "latitude must be between -90 and 90"})
+}
+
+function hasLongitude(req, res, next) {
+  const longitude = Number(req.body.data.longitude)
+  if (longitude >= -180 && longitude <= 180 ) {
+    return next()
+  }
+  next({status: 400, message: "longitude must be between -180 and 180"})
+}
+
+function hasSkyCondition(req, res, next) {
+  const skyCondition = Number(req.body.data.sky_condition)
+
+  if (validSkyConditions.includes(skyCondition)) {
+    return next()
+  }
+  next({status: 400, message: `sky_condition must be one of: ${validSkyConditions}`})
+}
+

// Code omitted for brevity

 module.exports = {
-  create,
+  create: [hasData, hasLatitude, hasLongitude, hasSkyCondition, create],
 };
Then use Postman or another tool to verify that the validation is working as expected.

Next, you will implement GET /observations in the backend project.

Implement GET /observations
Edit src/observations/observations.router.js as follows:

 router.route("/")
   .post(controller.create)
+  .get(controller.list)
   .all(methodNotAllowed);

 module.exports = router;
Edit src/observations/observations.controller.js as follows:

 let nextId = 1;
+const observations = []
+
 async function create(req, res) {
   const newObservation = req.body.data;

   const now = new Date().toISOString();
   newObservation.observation_id = nextId++;
   newObservation.created_at = now;
   newObservation.updated_at = now;

+  observations.push(newObservation)
+
   res.status(201).json({
     data: newObservation,
   });
 }

+async function list(req, res) {
+  res.json({
+    data: observations,
+  });
+}
+
 module.exports = {
   create: [hasData, hasLatitude, hasLongitude, hasSkyCondition, create],
+  list
 };
Next, switch back to the UI. You will no longer see an error displayed on the home page.

Connect to the database
Now that the UI and API are properly connected, it is time to connect to the database.

Edit src/observations/observations.controller.js as follows:

-let nextId = 1;
-
+const service = require("./observations.service");
 const validSkyConditions = [100, 101, 102, 103, 104, 106, 108, 109];

 // Code omitted for brevity

 async function create(req, res) {
-  const newObservation = req.body.data;
-
-  const now = new Date().toISOString();
-  newObservation.observation_id = nextId++;
-  newObservation.created_at = now;
-  newObservation.updated_at = now;
-
-  observations.push(newObservation);
-
+  const newObservation = await service.create(req.body.data);
+
   res.status(201).json({
     data: newObservation,
   });
 }
   // Code omitted for brevity
Edit src/observations/observations.service.js as follows:

const knex = require("../db/connection");

function create(newObservation) {
  return knex("observations").insert(newObservation).returning("*");
}

module.exports = {
  create,
};
Now go back to the UI and create a new observation.

It didn't work, and no error is displayed. What is happening? If you look at the console output of the API server, you will see an error message like the following:

UnhandledPromiseRejectionWarning: error: insert into "observations" ("latitude", "longitude", "sky_condition") values ($1, $2, $3) returning \* - relation "observations" does not exist
This means that the error handling of the async code isn't working correctly. Luckily, you have the asyncErrorBoundary() function for this.

Edit src/observations/observations.controller.js as follows:

 const service = require("./observations.service");
+const asyncErrorBoundary = require("../errors/asyncErrorBoundary");

 // Code omitted for brevity

 module.exports = {
-  create: [hasData, hasLatitude, hasLongitude, hasSkyCondition, create],
+  create: [hasData, hasLatitude, hasLongitude, hasSkyCondition, asyncErrorBoundary(create)],
   list,
 };
Try to create another new observation, and you will now see the error displayed on the UI, like this:

Error on Create Observation page.
Now that the error handling is working, you can fix the error by creating the observations table.

npx knex migrate:make createObservationsTable
Edit src/db/migrations/##############_createObservationsTable.js as follows:

exports.up = function (knex) {
  return knex.schema.createTable("observations", (table) => {
    table.increments("observation_id").primary();
    table.decimal("latitude", null);
    table.decimal("longitude", null);
    table.integer("sky_condition");
    table.timestamps(true, true);
  });
};

exports.down = function (knex) {
  return knex.schema.dropTable("observations");
};
Then run the latest migration:

npx knex migrate:latest
Again, create a new observation—it works this time! Use DBeaver to check that the record was added to the database.

Now, return the observations in the database for GET /observations.

Edit src/observations/observations.controller.js as follows:


-const observations = [];

 // Code omitted for brevity

 async function list(req, res) {
+  const data = await service.list();
   res.json({
-    data: observations,
+    data,
   });
 }

 module.exports = {
  create: [hasData, hasLatitude, hasLongitude, hasSkyCondition, asyncErrorBoundary(create)],
-  list,
+  list: asyncErrorBoundary(list),
 };
Edit src/observations/observations.service.js as follows:

 // Code omitted for brevity

+async function list() {
+  return knex("observations").select("*");
+}
+
 module.exports = {
   create,
+  list,
 };
Return to the UI and refresh the home page to see a list of weather observations from the database.

List of weather observations from database.
You have successfully changed to a full-stack application that has a frontend repository and backend repository! You can skip to the end of this lesson if you like, or you can continue on your own and implement another user story or two.

Additional user stories
Edit an existing observation
User story
As a user, I would like to edit an existing observation, so that I can correct any data entry errors I may make.

Acceptance criteria
Given that I am on the Edit Weather Observation page, and I see the existing weather observation information filled in the form, when I make changes and submit the form, then I am redirected to the Weather Observations page and see the updated weather observation in the list.

All create validation applies to update as well.

Add air temperature and unit
User story
As a world-traveling user, I would like to create a new weather observation that includes air temperature and the temperature unit, so that other people have accurate temperature information about places where I travel.

Acceptance criteria
Given that I am on the New Weather Observation page, when I submit a weather observation using the form, then I am redirected to the Weather Observations page and see the weather observation that I posted at the top of the list.

A weather observation has the following inputs:

Air temperature

Celsius must be between -50 and 107 degrees.

Fahrenheit must be between -60 and 224 degrees.

Air temperature unit

Celsius (value = "C")

Fahrenheit (value = "F")

The API will return an observation as JSON similar to the following:

{
  "observation_id": 2,
  "latitude": 0,
  "longitude": 0,
  "air_temperature": 100,
  "air_temperature_unit": "C",
  "sky_condition": 100,
  "created_at": "2020-12-10T08:30:32.326Z",
  "updated_at": "2020-12-10T08:30:32.326Z"
}

*** 37.6 Tracing errors
Tracing errors
1.5 hoursEstimated completion time
Learning Objective
By the end of this lesson, you will be able to identify how logging can help you trace problems in your code.

Overview
As you have been building three-tier applications, you have learned that there are many places where something can go wrong and cause the application to not work as expected. Errors happen in every application, and this lesson will explore some of the most common errors. Ideally, error information tells you exactly where things went wrong and how to fix it, if necessary.

Key Terms
Log levels
A way to indicate the severity of various log messages
Logging
When you first start programming, it is common to only think about the happy path and write code that assumes that everything works all of the time. But as you have learned, there are many places in the code where things can go wrong.

The most common thing to do when handling an error is to log some information about the error so that you can use the information to prevent the error from happening in the future. As a result, logging can be one of the most important features of your application. Effective logging can help you discover a problem quickly. On the other hand, missing or ineffective logging can leave you searching for a problem for hours, days, or even weeks. Therefore, it's helpful to think about the logging needs for every feature that you add to your application.

So what is effective logging? Well, with logging, you never really know what information you will need in the future, so you should do your future self a favor and log everything! However, if you log everything using console.log(), the logs will be huge, and it will be very hard to find the specific information that you need. On a day-to-day basis, you don't want to know every little thing that the application did—you just want the major events. But if it starts malfunctioning, you may want to increase the granularity of the logs and zoom in on the problem with as much focused detail as possible.

Perhaps an analogy will be helpful. Imagine that you have an android and you ask it about what it did today.

"Well, I woke up, opened both eyes, turned my head fifty degrees to the right—"

"Whoa, that's too much information. Reduce the granularity from 10 to 4."

"I woke up, ate breakfast, got in my car ... "

Logging needs to work like this android. Usually, you'll only want to know if there is a problem or warning condition. But sometimes, you'll want to increase the information granularity for troubleshooting purposes.

Unfortunately, console.log() doesn't give you the ability to adjust the level of detail; it always prints everything. To adjust the granularity of the information in the logs, you need to make use of something called log levels.

Log levels
Log levels indicate the importance, or severity, of a log message. Some messages are informative, while others are critical to report for application processing.

Most logging frameworks use a standard set of log levels in order to separate log messages by their severity. These log levels include the following:

FATAL: This represents a truly catastrophic situation for your application. A fatal entry means that your application is about to abort in an attempt to prevent some kind of corruption or other serious problem.

ERROR: An error is a serious issue. It means that something important in your application has failed. Unlike FATAL, the application isn't going to abort; it can recover from the error and keep running.

WARN: A warning represents a problem and flags that you've detected an unusual situation. It's unexpected and uncommon, but it doesn't cause any real harm. It also isn't known whether the issue will persist or recur.

INFO: An information entry represents normal application behavior and milestones. During normal operations, you probably won't pay much attention to information entries. These entries provide a high-level overview of what happened; for example, they'll log if a service started or stopped, or if you added a new user to the database.

DEBUG: A debug entry represents more granular, diagnostic information. Here, you're probably furnishing more information than you'd want in normal production situations. You're providing detailed diagnostic information for developers, sysadmins, and so forth.

TRACE: A trace entry represents extremely fine-grained information—finer even than a debug entry. When you're at this level, you're capturing as many details as you can about the application's behavior. This is used for diagnostics, and it will generally swamp your resources in production.

SILENT: This is just what it sounds like: it won't log anything at all.

At this point, you may be wondering how log levels are used. Well, there are two parties participating in logging:

The logging framework, which has a configured log level at runtime

The application code that calls the logging framework

If the framework has a given log level enabled, then all requests at that level of severity or higher are written to the log. Everything else is ignored.

Consider the following function:

function sum(left, right) {
  logger.debug({ left, right });

  const result = left + right;

  logger.trace({ result });

  return result;
}
If you have the application log level set to TRACE, you would see two entries in the log: one for logger.debug({ left, right }) and one for logger.trace({ result }). However, if you change the log level to DEBUG, then you would only see the call for logger.debug({ left, right }) in the log. Finally, if you turned the log level all the way down to FATAL, you would see nothing in the log because there are no calls to logger.fatal().

Now imagine that you have lots of log statements, at different levels, peppered throughout your application. Things are looking good! Then someone calls and says that they are getting a 404 error when they call your API. There are lots of calls to your API, so how do you find the request that is returning 404?

In fact, there might be multiple requests returning 404. How do you follow the path through the code and multiple application tiers for one specific request? You need a way to correlate each request across each application tier and track its progress through your code. One way to track each request is to assign a unique ID to each request.

Request IDs
To make tracing requests easier, almost all deployment platforms automatically generate a unique request ID and set the X-Request-ID header to that value. Then you can use the X-Request-ID HTTP header to trace an HTTP request all the way through your API and back again by including the request ID in the logs.

Tip
Some deployment platforms might use a different header name for the same concept. Vercel uses X-VERCEL-ID, Netlify uses X-NF-REQUEST-ID, and others use X-CORRELATION-ID.

Of course, there are times when you are running locally or in some other context where there is no hosting platform setting the X-Request-ID header on each request. In this case, you can have your Express server set the header if it isn't already set.

Now you know the basic concepts of logging. In a future module, you will learn how to add detailed logging to a backend project so that you can trace errors when they happen.

*** 37.7 Practice tracing errors
Practice tracing errors
1.5 hoursEstimated completion time
Learning Objective
By the end of this lesson, you will be able to implement logging in an Express application.

Overview
Effective logging can help you discover a problem quickly. But what does effective logging look like in the code? You might be surprised to learn that effective logging adds a lot of log messages to the code.

Key Terms
Pretty printing
Reformatting text into a structure that is easier for people to read than the original format
Trace-level logging
A logging level that aims to capture as much information as possible
Debug-level logging
A logging level that aims to capture just enough information to debug a problem
Info-level logging
A logging level that aims to capture the significant events in your application, such as the status code of a request or new database record created
Starter code
This lesson requires you to have the following repository running on your local machine.

GitHub: Starter tracing backend

Fork and clone the above repository. Then, follow the instructions to get the repository to run.

Implementing effective logging
Do your future self a favor and consider the logging requirements every time that you add or change the code. In short, you will want to log as much information as possible, because you never know what you will need in the future.

Trace-level logging is the most granular, so think about this level of logging first. You can't really have too much trace-level logging. Trace messages will log the return value of every function call.

Debug-level logging is about capturing just enough information to debug a problem. Debug messages will log the entry into most function calls, along with any parameters passed to the function.

Info-level logging is about capturing the significant events in your application, such as the status code of a request or new database record created.

Do this
Set a request ID
You will start in the backend project by configuring pino-http, an Express middleware package, to assign a unique ID to each request.

First, install pino-http:

npm install pino-http
Then make the following changes to src/app.js:

+const pinoHttp = require('pino-http')

 const app = express();

+app.use(pinoHttp());
 app.use(cors());
 app.use(express.json());

 // Some code omitted for brevity
Now start the server and send a request to http://localhost:5000. You will see some JSON logged to the console.

Your output will be unformatted and hard to read. If you use an online JSON formatter to format the output, it will look something like the following. (But note that almost all of the details will be different in your log message.)

{
  "level": 30,
  "time": 1607828947331,
  "pid": 72593,
  "hostname": "<your-computer-name>",
  "req": {
    "id": 1,
    "method": "GET",
    "url": "/",
    "headers": {
      "host": "localhost:5000",
      "user-agent": "HTTPie/2.3.0",
      "accept-encoding": "gzip, deflate",
      "accept": "*/*",
      "connection": "keep-alive"
    },
    "remoteAddress": "::1",
    "remotePort": 59521
  },
  "res": {
    "statusCode": 404,
    "headers": {
      "x-powered-by": "Express",
      "access-control-allow-origin": "*",
      "content-type": "application/json; charset=utf-8",
      "content-length": "29",
      "etag": "W/\"1d-edkcVHA/drGEA7P6WQOdtTaE3WY\""
    }
  },
  "responseTime": 4,
  "msg": "request completed"
}
What a nice surprise—there is now a logging framework, named pino, which is automatically logging information about each request. By default, the log level of pino is set to INFO, so any log messages with a lower severity than INFO are ignored.

Take a moment to review the log message above. There is a lot of useful information there, and once you understand it, you can use it to monitor the performance of your application.

The table below explains the meaning of some of the more interesting properties of the log message:

Property
Description
level
The log level of this message
time
The time that the log message was created
req.id
The unique request ID assigned by pino-http
res.statusCode
The status code returned to the client
responseTime
The total amount of time it took, in milliseconds, to process the request from start to finish
The following excerpt from the pino source code shows the relationship between the log level name and its numeric value.

const levels = {
  trace: 10,
  debug: 20,
  info: 30,
  warn: 40,
  error: 50,
  fatal: 60
}
Notice that the request ID is an integer. This is okay for development, but once you go to a production deployment, an integer value like 1 for the request ID will be difficult to find when searching the logs. Rather than using the default generator, you will configure pino-http to use the nanoid package to generate long and unique string values for the request ID.

First, install nanoid:

npm install nanoid
Rather than configuring the Express logger in src/app.js, you will create a new file to configure the logger and then require it in src/app.js.

Next, add the following to src/config/logger.js:

const pinoHttp = require("pino-http");
const { nanoid } = require("nanoid");

const level = process.env.LOG_LEVEL || "info"

const logger = pinoHttp({
  genReqId: (request) => request.headers['x-request-id'] || nanoid(),
  level
});

module.exports = logger;
In the above code, if the request has an X-Request-ID header, the value of that header will be used as the request ID. Otherwise, nanoid() is called to generate a new ID.

Tip
Node converts all header keys to lowercase, so it doesn't matter what case is used in the original request. You will access the header using all lowercase characters in the key.

Also note that the above code is using the LOG_LEVEL environment variable to set the log level for the logger. For pino, the log level can be one of the following values: fatal, error, warn, info, debug, trace, or silent. If the LOG_LEVEL isn't set, the default will be info.

Next, make the following changes to src/app.js:

-const pinoHttp = require('pino-http')
+const logger = require("./config/logger");

 const app = express();

-app.use(pinoHttp());
+app.use(logger);
 app.use(cors());
 app.use(express.json());

 // Some code omitted for brevity
Then send another request to http://localhost:5000. You will see that the req.id is a long string value, such as qMryt28YVXSMugzAdbnyH.

Finally, use Postman to send another request to http://localhost:5000, but this time, set the X-REQUEST-ID to something like edroSf2GZu8ieWcMu5rKQ. Now when you look in the logs, the request will have the ID value from the X-REQUEST-ID header.

JSON is a great format for log files because it is easy for other software programs to parse and search the logs. However, writing the logs in JSON makes it harder for people to read.

Ideally, the logs would be in JSON format in production, but in a human-readable format in development. So, the next thing that you will do is use the NODE_ENV environment variable to enable pretty printing in development. Pretty printing is a general term used to describe reformatting text into a structure that is easier for people to read than the original format.

For example, consider the following standard pino log line:

{"level":30,"time":1522431328992,"msg":"hello world","pid":42,"hostname":"foo","v":1}
When you pretty-print it, the line above will look like this:

[1522431328992] INFO (42 on foo): hello world
Do this
Pretty print logs in development
First, install pino-pretty:

npm install pino-pretty
Next, make the following changes to src/config/logger.js:

 const pinoHttp = require("pino-http");
 const { nanoid } = require("nanoid");

 const level = process.env.LOG_LEVEL

+const nodeEnv = process.env.NODE_ENV || 'development'
+const prettyPrint = nodeEnv === "development"

 const logger = pinoHttp({
   genReqId: (request) => request.headers['x-request-id'] || nanoid(),
-  level
+  level,
+  prettyPrint
 });

module.exports = logger;
In the above code, if the NODE_ENV environment variable isn't set, or if it's set to development, the logger will automatically load pino-pretty and output the logs in a human-readable format.

Finally, send a request to http://localhost:5000. You will see a log message that looks something like the following:

[1607881392556] INFO     (81902 on <your-computer-name>): request completed
    res: {
      "statusCode": 404,
      "headers": {
        "x-powered-by": "Express",
        "access-control-allow-origin": "*",
        "content-type": "application/json; charset=utf-8",
        "content-length": "29",
        "etag": "W/\"1d-edkcVHA/drGEA7P6WQOdtTaE3WY\""
      }
    }
    responseTime: 1
    req: {
      "id": "AHh6bOL7lINVNoI6Vc3gg",
      "method": "GET",
      "url": "/",
      "headers": {
        "host": "localhost:5000",
        "user-agent": "HTTPie/2.3.0",
        "accept-encoding": "gzip, deflate",
        "accept": "*/*",
        "connection": "keep-alive"
      },
      "remoteAddress": "::1",
      "remotePort": 64921
    }
The output still contains some JSON, but it's certainly easier to read than the original format.

Now that you have readable log information in development, it is time to add some log messages to your code.

Do this
Add log statements
In this section, you will be making calls to the /articles route, which requires a database. So take a moment and make sure that you have followed the setup instructions in the project.

Next, make the following changes to src/articles/articles.controller.js:

 async function list(req, res) {
+  const methodName = "list";
+  req.log.debug({ __filename, methodName });
   const data = await service.list();
   res.json({
     data,
   });
+  req.log.trace({ __filename, methodName, return: true, data });
 }
In the above code, you added two log statements. The first calls debug(), and the second calls trace(). This means that nothing new will be logged with the standard log level of info.

So, change the LOG_LEVEL in your .env file to debug:

LOG_LEVEL=debug
Now, restart the server and send a request to http://localhost:5000/articles. You will see two log statements like the following:

[1607889735008] DEBUG    (86980 on <computer-name>):
    __filename: "<path-to>/src/articles/articles.controller.js"
    methodName: "list"
    req: {
      "id": "4_Yd2Nj4NCCr8MHcnmD_N",
      "method": "GET",
      "url": "/articles",
      "headers": {
        "host": "localhost:5000",
        "user-agent": "HTTPie/2.3.0",
        "accept-encoding": "gzip, deflate",
        "accept": "*/*",
        "connection": "keep-alive"
      },
      "remoteAddress": "::1",
      "remotePort": 49619
    }
[1607889735322] INFO     (86980 on <computer-name>): request completed
    res: {
      "statusCode": 200,
      "headers": {
        "x-powered-by": "Express",
        "access-control-allow-origin": "*",
        "content-type": "application/json; charset=utf-8",
        "content-length": "1491",
        "etag": "W/\"5d3-+mkO3ENcZKn0QW8953BTSxq56UQ\""
      }
    }
    responseTime: 315
    req: {
      "id": "4_Yd2Nj4NCCr8MHcnmD_N",
      "method": "GET",
      "url": "/articles",
      "headers": {
        "host": "localhost:5000",
        "user-agent": "HTTPie/2.3.0",
        "accept-encoding": "gzip, deflate",
        "accept": "*/*",
        "connection": "keep-alive"
      },
      "remoteAddress": "::1",
      "remotePort": 49619
    }
Note that the DEBUG message appeared, but the TRACE message didn't.

Next, change the LOG_LEVEL in your .env file to trace:

LOG_LEVEL=trace
Then restart the server and send a request to http://localhost:5000/articles. Now you will see the trace message.

As you can see, you can now adjust the granularity of the log messages without having to change your code. For the most part, the disabled log statements have no (or very little) impact on the performance of your application. However, enabled log messages do have an impact on performance.

Next, you will update the http://localhost:5000/articles controller to have detailed debug and trace logging when it receives a POST request to /articles.

First, make the following changes to src/articles/articles.controller.js:

 function hasData(req, res, next) {
+  const methodName = "hasData";
+  req.log.debug({ __filename, methodName, body: req.body });
   if (req.body.data) {
+    req.log.trace({ __filename, methodName, valid: true });
     return next();
   }
-  next({ status: 400, message: "body must have data property" });
+  const message = "body must have data property";
+  next({ status: 400, message: message });
+  req.log.trace({ __filename, methodName, valid: false }, message);
 }

 function dataHas(propertyName) {
-  return (request, response, next) => {
-    const { data = {} } = request.body;
+  const methodName = `dataHas('${propertyName}')`;
+  return (req, res, next) => {
+    req.log.debug({ __filename, methodName, body: req.body });
+    const { data = {} } = req.body;
     const value = data[propertyName];
     if (value) {
+      req.log.trace({ __filename, methodName, valid: true });
       return next();
     }
-    next({ status: 400, message: `Article must include a ${propertyName}` });
+    const message = `Article must include a ${propertyName}`;
+    next({ status: 400, message: message });
+    req.log.trace({ __filename, methodName, valid: false }, message);
   };
 }
Finally, send the following body in a POST request to http://localhost:5000/articles.

{
  "data": {
    "title": "Google News",
    "url": "news.google.com",
    "summary": "All the news from Google"
  }
}
Now you will see all of the trace messages. If an error ever happens in one of these methods, you can use the trace logs to determine exactly what happened.

Complete example
A completed example from this lesson can be found here:

GitHub: Starter tracing backend—practice-complete branch
*** 37.8 Assessment: Connecting it all

** Mock Interview: Backend - Module 38

** Node and Express - Module 39
