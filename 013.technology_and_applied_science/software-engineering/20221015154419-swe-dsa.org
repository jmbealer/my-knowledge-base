:PROPERTIES:
:ID:       d3374c8f-56bd-4dee-ad16-e9f4a8492999
:mtime:    20230206181547 20221015190500
:ctime:    20221015154419
:END:
#+title: swe-dsa

[[id:626859c7-4f90-41d5-a450-d37bf06f24fb][grokking-the-coding-interview]]

* Data Structures and Algorithms - Modules 41-44
** Module 41 - Introduction to data structures and algorithms
*** 41.1 - Overview: Intro to data structures and algorithms
*** 41.2 - What are algorithms?

What are algorithms?
1.5 hoursEstimated completion time
Learning Objective
By the end of this lesson, you will be able to describe an algorithm.

Overview
Whether you are aware of it or not, you have been using algorithms your whole life. For example, in elementary school, you learned a series of steps for multiplying two numbers. You learned that if you follow those steps precisely, you will always get the correct result. That series of steps is an algorithm, and that's what you'll focus on in this lesson.

Key Terms
Time complexity of an algorithm
The number of instructions (or steps) needed to execute the algorithm
Space complexity of an algorithm
The amount of memory used by the algorithm
The video below provides a brief introduction to algorithms. Start by watching the video, and then read through the rest of the lesson. This will give you a thorough understanding of this topic.


An algorithm is defined as a well-defined sequence of steps for solving a computational problem. Even though this definition is simple, it covers a few concepts that may be expanded.

Key Term
Algorithm: A well-defined sequence of steps for solving a computational problem

First, break down the idea of a computational problem. A computational problem is a problem that a computer might be able to solve. For example, the problem of sorting a sequence of numbers in ascending order is a computational problem.

Problems must be well specified. That is, the statement of the problem must specify the inputs, outputs, and the relationship between the inputs and outputs. For example, the search problem may be defined as follows: given a sequence of n numbers and a number x as input, output the index of the first occurrence of the number x in the sequence. Output -1 if the number x isn't in the sequence.

There may be more than one way to solve that problem. An algorithm, then, is any sequence of steps that, if followed precisely, will solve the problem.

Next, note that the definition above specifies that the sequence of steps must be well defined. In other words, the steps must be unambiguous and complete. Here, unambiguous means that the instruction for a particular step can only mean one thing. And complete means that no steps are left out.

The linear search algorithm
Here is an example of an algorithm that solves the search problem defined above.

function linearSearch(sequence, x) {
  for (let i = 0; i < sequence.length; i++) {
    if (sequence[i] === x) {
      return i;
    }
  }

  return -1;
}
This algorithm is complete and unambiguous. The statement let i=0 can only mean one thing: declare a variable named i and initialize it to the value 0. Similarly, each statement has a precise meaning.

You'll study this algorithm in more detail in a later lesson.

A note on notation
In the study of algorithms, the programming language does not matter. An algorithm may be implemented in any programming language. What is relevant is what steps are needed to complete the task.

For this reason, you can write an algorithm using pseudocode. As you learned earlier in this program, pseudocode is a form of structured English used for describing algorithms. It resembles programming code but isn't concerned with details such as semicolons. The exact syntax of pseudocode is not important as long as it is unambiguous and clear.

The linear search algorithm above is written in JavaScript. In pseudocode, that algorithm may look like this:

 function linearSearch
   input: sequence - an unsorted array of numbers
   input: x - a number

   Initialize a variable i to 0
   Iterate while i is less than the length of sequence
     if the ith element of sequence is equal to x, then return i

   if no element of sequence matches x, then return -1
Alternatively, an algorithm may be depicted in a flow diagram, like this:

Algorithm represented as a flow diagram.
In this module, JavaScript will be used to specify the algorithms. This is because of the following reasons:

You are already familiar with the syntax.

Programming code is by definition unambiguous.

Having the ability to directly run the code may help you to understand the algorithm.

Properties of algorithms
The following video provides an introduction to the properties of algorithms.


Imagine that you are the judge at a baking contest. You would make your decisions by comparing the properties or characteristics of various cakes. For example, you would look at the fluffiness, aroma, moistness, taste, and appearance of each cake. It is by comparing these properties that you know which cake is the best.

Similarly, algorithm design and analysis involves a disciplined approach to studying the properties of algorithms. It is by comparing these properties that you can know which algorithm may be most suitable for use in a particular scenario.

The properties of an algorithm are as follows:

Correctness: The output produced by the algorithm is correct for all valid input.

Efficiency: The algorithm minimizes the use of the available computing resources.

Determinism: The result of each step of the algorithm is determined only by the inputs and the results of the preceding steps.

Finiteness: The algorithm must stop. It may take many steps, but eventually, it must terminate.

Generality: The algorithm applies to a set of inputs.

Proving that an algorithm is correct for all valid inputs requires some rigorous mathematics that is beyond the scope of this module. Rather, you will take a more informal approach to determining the correctness of an algorithm.

In this module, you are going to focus your attention on the efficiency of algorithms.

Efficiency
Computing resources are limited. A good algorithm makes efficient use of these resources. Providing that an algorithm satisfies all the other properties, it is efficiency that differentiates various algorithms.

The resources that are most relevant are the memory of the computer and the CPU. An algorithm may be correct, but if it requires more RAM than is reasonably available, then it is useless. Similarly, if the algorithm requires more processing time than is available, it is useless.

The number of instructions (or steps) needed to execute an algorithm is called the time complexity of the algorithm. The amount of memory used by the algorithm is called the space complexity.

Algorithms are usually discussed in terms of their time and space complexity. Normally, the concern is with the time complexity rather than space complexity. This is because it is easier and cheaper to obtain space, and there are techniques for achieving space efficiency by spending more time.

*** 41.3 - Why study algorithms?
Why study algorithms?
1.5 hoursEstimated completion time
Learning Objective
By the end of this lesson, you will be able to describe why studying algorithms is useful.

Overview
Computers are getting faster every year, and memory is getting cheaper all the time. So is it still important to study algorithms? The answer is yes. In this lesson, you will examine some of the reasons why the study of algorithms is essential.

Some real-world algorithms
The video below provides a brief introduction to why algorithms are important. Start by watching the video, and then read through the rest of the lesson. This will give you a thorough understanding of this topic.


Algorithms are everywhere. They impact your life in meaningful ways, even if you do not directly use them yourself. Take a look at a few examples below.

Google's PageRank algorithm
PageRank is one of the algorithms that Google uses to order search results. If you have ever done a search on Google, you have used their PageRank algorithm. This algorithm attempts to measure the importance of a web page and its relevance to your search query.

Internet routing
The internet is made up of millions of nodes (computers and routers and other devices) that are all connected together in a vast array of connections. Moving data from one computer to another is an extremely complicated task. Yet, you do it every day with every web page that you visit, every email that you send, and every video call that you make.

To make this all work, there are a series of different routing algorithms that attempt to find the fastest route from your computer to any other computer that you are connected to on the internet.

The problem of routing is an area of active research. As internet access continues to grow cheaper, the number of nodes grows, and it becomes more and more critical to identify more efficient algorithms to enable routing.

Cryptography
The modern world would not be possible without encryption. Encryption is the process of obscuring a message so that third parties may not access the content of the message. Encryption is used to secure communication on the internet. If you have ever used online banking, done a credit card transaction, or even used an online email service like Gmail, then you have used an encryption algorithm.

Ride-sharing
The chances are good that you have used a ride-sharing service while hardly giving a thought to the technology behind such a service. The ride-sharing service wishes to optimize the use of its drivers and minimize idle time as well as wait times for its customers. Tracking numerous drivers on the move and numerous customers waiting for rides is a complex task that requires sophisticated algorithms.

GPS
Today, it is trivial to pinpoint your exact position on the surface of the planet with GPS. Your phone seems to know—with relative ease—that you visited your local Starbucks. But the technology behind this is far from trivial; it involves several complex systems all working together to find your position as you buy your coffee. GPS works by synchronizing atomic clocks on a constellation of satellites orbiting the Earth with a ground-based control system and your phone. The algorithms involved are as complex as anything that humankind has ever created.

Usefulness of algorithms
Just from the examples above, it should be clear that there are many useful algorithms already at work in the world today. But the story is far from complete. The hardware that makes up your computer implements many algorithms that enable the tasks that you do on your computer. The operating system is made up of a series of advanced algorithms for file management, CPU usage, networking, security, and a host of other services within the computer. Your browser uses many algorithms just to display a web page. And games that you play are possible because of sophisticated algorithms.

Generally, algorithms are a technology that has been used for commercial advantage and military benefit. For these reasons alone, the study of algorithms is a useful pursuit.

But computers are so fast!
The video below provides an example to illustrate why algorithms are important.


Computers are fast and getting faster. Memory is cheap and getting cheaper. So why study algorithms at all? If you wanted to solve some problem at a faster rate, you could just wait a few years, and computers would solve the problem faster—right? Well, the story isn't quite that simple. Below, explore this through a concrete example.

Suppose that you had an array of 10 million elements, and you needed to sort those elements. There are two programmers, and they both say that they can write a program to sort those numbers for you. The first programmer, Bob, claims that he has a computer that can perform 10 billion instructions per second. The second programmer, Alice, says that she has a computer that can perform only 10 million instructions per second. So Bob's computer is 1,000 times faster than Alice's. It would seem that if you use Bob's computer, you will get your sorted array 1,000 times faster than using Alice's computer.

Sketch of Bob and Alice with their computers.
Intuitively, you may understand that as the size of the array increases, the length of time needed to sort the array also increases. So sorting 10 million elements will take longer than sorting 1 million elements. The question is, how much longer?

Bob's sorting algorithm uses n² instructions to sort n elements. For example, to sort 5 elements, Bob's algorithm will use 5², or 25 instructions (5²=5*5=25). And to sort 6 elements, it will use 6², or 36 instructions (6²=6*6=36). This is known as the running time of the algorithm. You will be taking a deep look at running time in the next few lessons.

On the other hand, Alice's sorting algorithm takes n log n (that is, n times log to base 2 of n) instructions to sort n elements. Don't worry if the math is starting to look complicated; it won't get any more complex than this. In this case, sorting 5 elements would take 12 instructions (5*log₂5=5*2.322=11.6), and sorting 6 elements would take 16 instructions (6*log₂6=6*2.585=15.51). (Note that the numbers are rounded for simplicity.)

How long would it take to sort 10 million elements using Bob's algorithm on Bob's computer? And how long would it take for Alice's computer using her algorithm?

Calculation of sorting times for Bob and Alice with their respective computers and algorithms.
Incredibly, Alice can sort 10 million elements in an array in just 24 seconds—while Bob would take almost 3 hours. That is more than 400 times as fast!

This gap grows significantly as the size of the array grows. You can use your calculator to see how long it takes to sort 20 million elements or 100 million elements. You will see that to sort 100 million elements, Alice's algorithm completes in 265 seconds, while Bob's algorithm completes in one million seconds—almost 12 days! That is almost 4,000 times as fast!

Of course, this is just a made-up example, but it serves to illustrate that a well-designed algorithm can produce benefits far beyond that of a faster computer.

*** 41.4 - An efficiency example
An efficiency example
1.5 hoursEstimated completion time
Learning Objective
By the end of this lesson, you will be able to describe the efficiency of an algorithm in terms of its running time as a function of the size of input.

Overview
At this point, you can define an algorithm, you know some examples of famous algorithms, and you understand why the study of algorithms continues to be an important pursuit. In this lesson, you'll begin to take a closer look at what it means for an algorithm to be efficient. Efficiency, as you have already seen, is the main measured characteristic of an algorithm.

In a previous lesson, it was stated that an algorithm is a solution to a well-defined computational problem. It was also stated that there might be many algorithms that solve the same problem. So in this lesson, you'll define a problem and consider some possible solutions.

The problem
The video below provides a brief introduction to solving an algorithm using a naive approach. Start by watching the video, and then read through the rest of the lesson and complete the practice tasks. This will give you a thorough understanding of this topic.


Given an integer n, find the sum of all integers from 1 to n inclusively.

For example, if n=4, the solution involves the sum 1+2+3+4=10.

The first solution
To solve this problem, you could use a loop to count every integer from 1 to n and sum as you go. At the end of the loop, you will have the required sum.

Here is one way that the solution may be written:

function sumIntegers(n) {
  let sum = 0;
  for (let i = 1; i <= n; i++) {
    sum = sum + i;
  }
  return sum;
}
The question is now the following: how long does it take for this program to run? To find the answer, you can check the system time before you run the function and then check the system time again after the function is complete. Then you can find the difference to see how long it takes. There are some problems with this approach, as you will see, but it can give you a feel for the running time.

Measuring the performance
The Node environment has a built-in function process.hrtime.bigint(), which gives you the current system time in nanosecond precision. Here's an example:

const N = 100; // The input to the function
const start = process.hrtime.bigint(); // Note the start time

const answer = sumIntegers(N); // Call the function

const end = process.hrtime.bigint(); // Note the end time

console.log(`Summing ${N} numbers took ${end - start} nanoseconds`);
Do this
Measure the running time of the function
Create a new JavaScript file and copy the code above into it. Execute the file and observe the output. Run the same code several times. Notice that every time that you run the code, the result is slightly different. For example, after running the code three times, this was the output:

Summing 100 numbers took 26507 nanoseconds
Summing 100 numbers took 22287 nanoseconds
Summing 100 numbers took 22816 nanoseconds
Of course, the output on your machine will be different than the above output, because your machine may be faster or slower than the machine used to run the code for this example.

The important takeaway is that the actual time that it takes to run the program varies. This is because of several factors in the environment, such as what other processes may be running on the computer, how fast the CPU actually is, and how much memory may be available.

Getting an average time
You can try running the code several times and finding an average. As shown below, update the code to run the function 10 times and find the average time.

const NUMBER_OF_REPETITIONS = 10; // Number of times to repeat the test
const N = 100;

let sumOfRunningTime = 0n;

for (let k = 1; k <= NUMBER_OF_REPETITIONS; k++) {
  const start = process.hrtime.bigint();

  const answer = sumIntegers(N);

  const end = process.hrtime.bigint();

  sumOfRunningTime += end - start;
}

const averageTime = sumOfRunningTime / BigInt(NUMBER_OF_REPETITIONS);

console.log(
  `Summing ${N} numbers took an average of  ${averageTime} nanoseconds`
);
This way, you can feel more confident about the value.

As the input grows
The test above gives you a time for summing precisely 100 numbers. What if you had to sum more than that? How long would it take? It may be worthwhile to try the above program with some different numbers just to see what happens.

Do this
Repeat the experiment with larger numbers
In the above code, change the value for N and run the code again. Here's an example:

const N = 1000;
Do this several times, with N set to 100, 500, 1000, 2000, 5000, and 10000. Note the time that each takes.

What can you say about the way that the length of time that it takes to run the function changes as the size of the input changes?

Linear growth
You may have noticed that as the size of the input increases, the running time of the function increases correspondingly. Given this observation, you could extrapolate and guess the running time for even larger input sizes.

This is an important concept. Although it is useful to know the running time for a specific input size, it is actually more useful to have a formula that tells you how that running time changes as the input size changes.

If you plot the numbers on a graph, you may get something like this:

Growth rate of sum integer function
The blue line shows the linear function, and the red line shows how the running time of the sumInteger() function changes as n changes. Notice that it is roughly the same rate of growth.

Given that, you can say that the function that was written has a linear growth rate. That is, the length of the running time changes proportionally to the size of the input.

This finding should be easy to see. In the function, there is a loop that goes from 1 to n. As n increases, the function has correspondingly more work to do.

A second solution
The video below describes the process of solving an algorithm using a more optimal approach.


Is it possible to develop an algorithm that performs better than this first solution? And what exactly is meant by better here? In this case, better would mean a growth rate that is slower than the linear function. If there were another solution that had a running time that grew slower than the linear function as n increases, then you could say that you have a more efficient algorithm. That is, you can calculate larger values of n in less time.

Take a few moments to think about how you might approach this problem.

One way that it might be done was devised by the famous mathematician Carl Friedrich Gauss when he was seven years old. Don't worry if you didn't think of this solution; Gauss is considered one of the greatest mathematicians of the age, and he basically invented algebra as it is known today.

Gauss came up with the formula n(n+1)/2. It is easy to verify that this is correct. For example, when n=100, the expression is evaluated as follows:

100 * (100 + 1) / 2
=> 100 * 101 / 2
=> 10100 / 2
=> 5050
This means that you can write a function to implement this formula like this:

function sumIntegers2(n) {
  return (n * (n + 1)) / 2;
}
Do this
Measure this function
Using the same technique as before, run this new function several times with different input sizes. Note the running times as the input size increases.

What do you notice about the running time for this function as compared to the previous program?

Running time of the second solution
You should have noticed that the running time remains roughly the same, no matter how large n is. That is, the running time remains constant as n increases. If you plot this on a graph, it may look like this:

Graph of running time in nanoseconds vs size of n. The constant function always has a running time of 250 ns, while the growth function's running time varies slightly but stays near 250 ns.
This time, the blue line shows the constant function. That is, the value does not change as the size of n changes. The red line is the growth rate of the function. Notice that even though there are slight variations, the function closely mirrors the constant function line.

Again, it is easy to explain why this is so for this second algorithm. No matter the value of n, you simply perform one multiplication operation, one addition operation, and one division operation to calculate the result. The same amount of work is done regardless of the size of n.

When the running time of a function doesn't change with changes to the input size, you can say that it has a constant growth rate.

Comparing the growth rates
Now that it is known that the first algorithm has a linear growth rate and the second has a constant growth rate, which would you say is the more efficient algorithm? Plotting a constant function and a linear function on the same graph depicts the difference:

Graph of running time in nanoseconds vs size of n. The constant function always has a running time of 250 ns, while the linear function's running time increases as n increases.
In this graph, you can see that the linear function increases and the distance between the two lines grows as n increases. Therefore, a constant growth rate is more efficient than a linear growth rate.

Problems with this approach
The exercise that you did in this lesson is useful to illustrate that the same problem can be solved in different ways, and some ways may be more efficient than other ways.

It also illustrated what is meant by running time. However, there are some problems with this approach.

First, you need a working program in order to run such an experiment. What if you had an idea for an algorithm and wanted to analyze the algorithm before you implemented it?

Second, the program depends on several factors, such as the programming language and even the skill of the programmer. These functions were written in JavaScript and executed in a Node environment. What if they were written in C or Python or Java? Would they run faster or slower? What if, in the first program, a for each loop was used instead of the for loop? Would that change the running time? Also, more experienced programmers may know techniques for writing more efficient code even if the algorithm itself has a poor growth rate. How can you account for that?

Third, the computer used for this experiment had its own configuration. The CPU, operating system, version of Node, amount of memory, and many other factors affect how fast programs run on a computer.

Fourth, this was a fairly simple algorithm. It took a few nanoseconds to run. It was easy to run it many times to determine the growth rate. What if the algorithm was more complex and took an hour to run? It wouldn't be practical to run the function ten times over in that case.

And fifth, how would you compare the running time of this algorithm to the running time of other algorithms? Suppose that it was published that this algorithm executed in 250 nanoseconds. If another programmer published that their algorithm ran in 200 nanoseconds, is it because their algorithm is better, or is it because of one of the factors mentioned above?

Is there a better way?
There is a better way. What is needed is some way to analyze the running time of the algorithm independently of the programming language that it is written in or the computer hardware that it runs on.

In the next lesson, you will explore exactly such a technique.
*** 41.5 - Asymptotic analysis
Asymptotic analysis
1.5 hoursEstimated completion time
Learning Objective
By the end of this lesson, you will be able to use big O notation to represent the order of growth of the running time of an algorithm.

Overview
Analyzing an algorithm means determining the resources that the algorithm will consume during execution. Sometimes, the resource in question is the memory of the computer or the network bandwidth. However, the most important resource to measure is the computational time. That is, how long does it take to execute the algorithm on a given set of inputs?

Key Terms
Rate of growth
Also called the order of growth, the rate at which the running time of an algorithm increases as a function of the input size
Big O notation
A notation commonly used to describe the order of growth of an algorithm
A model of computation
Today, computers come in all shapes and sizes, from small wearable devices, to laptop and desktop machines, to massive servers and supercomputers running many CPUs in parallel. Even with one class of devices, there are so many differences in CPU technology that it would be impossible to list them all. Some CPUs have a single core, while many CPUs today come with multiple cores, allowing software to take advantage of asynchronous execution models. Many computers include additional processing units like GPUs (graphics processing units) where some of the work can be offloaded, freeing up the main CPU. And this doesn't even touch on the different memory models, cache pipelines, bus speeds, and other variations and techniques used to provide all the computing power that you may need.

With all this variation, it is very difficult to isolate the efficiency of the algorithm itself.

Rather than depend on the computer's implementation details, it is common to adopt a simplified model of the computer for the purpose of analysis. What is really being measured is the amount of work that is required to execute the algorithm. The fact that a faster computer will do the work faster does not change the fact that the same amount of work is being done.

The computational model that you'll adopt here has the following properties:

It has a single processor and runs the algorithm in a sequential manner. That is, the instructions for the algorithm are executed in the given order, one at a time.

It takes exactly one time unit to execute a standard instruction. A standard instruction is an operation such as addition, subtraction, multiplication, division, comparisons, assignments, and conditional control. No complex multistep instructions (such as sorting) can be done in a single time unit.

It takes the same amount of space to store each value (such as an integer).

It has an infinitely large memory. This assumption frees you from worrying about the space requirements.

Tip
A time unit is a generic amount of time. Because of variations in the speed of processors, some can execute an instruction in a few nanoseconds, while others may take a few milliseconds to execute the same instruction. Here, focus on how many of the time units are used, rather than the absolute time.

Counting steps
Recall the problem that was solved in the previous lesson:

Given an integer n, find the sum of all integers from 1 to n inclusively. For example, if n=4, the solution involves the sum 1+2+3+4=10.

And recall the first solution that was presented:

function sumIntegers(n) {
  let sum = 0;
  for (let i = 1; i <= n; i++) {
    sum = sum + i;
  }
  return sum;
}
How many steps would it take to execute this algorithm on the computer model described above? First, break up this program into its individual steps. A diagram might be helpful here:

Diagram of algorithm broken into steps.
In this flow chart, each statement to be executed is represented by a rectangle or diamond. A diamond shape represents a conditional statement.

Starting from the top, look at each statement in turn. Write down the number of times that statement will be executed when the algorithm runs.

For example, the first statement, sum=0, will only execute once. The value of n doesn't change this. The next statement, i=1 is the loop initialization step, and it is only executed once at the beginning of the loop.

The loop condition statement, i<=n, depends on the value of n. Consider a few small examples. Suppose that n is 1. This condition executes once when i is equal to 1 and one more time when i is equal to 2. When n is 1, this statement executes twice.

Suppose that n was 2. The loop condition executes once when i is equal to 1, once when i is equal to 2, and once when i is equal to 3. Therefore, when n is 2, this statement executes three times.

Generalizing that, you can say that this statement executed n+1 times. You should try a few more examples, say when n is equal to 4 and when n is equal to 5, to see if this conclusion holds up.

Diagram explaining what the statements do.
The next statement occurs in the body of the loop. Everything in the body of the loop depends on the number of times that the loop iterates, which in turn depends on n. Like the previous statement, you can use a few small examples to try to determine the pattern.

n
Number of executions
Explanation
1
Executes one time
When i is 1, i<=n is true. So the loop body executes, and i is then incremented to 2. That means that i<=n becomes false and the loop ends.
2
Executes two times
Similar to above, but when i is 2, i<=n is still true. So the loop body executes a second time. i is then incremented to 3 and i<=n becomes false.
You can do a few more of these to verify the pattern. Generally, it looks like the statement executes n times.

The next statement, the loop step, executes n times, by the same logic as the previous step. The final statement, return sum, executes just once.

Diagram showing number of times each statement executes.
To put it all together, sum the number of steps:

1 + 1 + n + 1 + n + n + 1 = 3n + 4
Therefore, you can say that the running time for this algorithm in terms of the input n is 3n+4.

Using this function, you can calculate the number of steps that this algorithm takes for any value of n. For example, if n is equal to 10, this algorithm will execute as follows:

3 * 10 + 4 = 34 steps
And if n is equal to 100, this algorithm will execute as follows:

3 * 100 + 4 = 304 steps
Order of growth
So far, some simplifying assumptions have been made to facilitate this analysis. For example, a simple computer model was assumed. Next, the actual time to execute an instruction was ignored. In reality, computers take slightly longer to execute multiplication than addition. Here, that difference is ignored.

The execution time derived above, 3n+4, contains more information than is needed, and further simplifications can be made.

It is the rate of growth that you are really interested in. The rate of growth, also called the order of growth, is the rate at which the running time of an algorithm increases as a function of the input size.

For this reason, it is only the highest-order term in the running time that matters. To illustrate this, consider the running time 3n+4. There are two terms in this expression: 3n and 4. As n grows, the 3n term dominates the value of the expression.

n
3n
4
3n+4
Contribution of 4 to the value
1
3
4
7
57.000%
10
30
4
34
11.760%
100
300
4
304
1.310%
1,000
3,000
4
3,004
0.013%
As you can see, as n increases to ever-larger values, the 4 in the expression has less effect on the final value. You could simply drop that from the expression, leaving only the 3n.

It is also possible to drop the leading term's constant coefficient, because constant factors are less significant than the growth rate. That just leaves n.

You can say that the algorithm has a growth rate of n. This is written as O(n), pronounced "big Oh of n."

Big O is a notation commonly used to describe the order of growth of an algorithm. Specifically, it describes the upper bound, or worst-case running time of the algorithm.

Big O
How can big O notation be interpreted? What exactly does O(n) mean?

O(f(n)) describes a set of functions that grow at most as fast as the function f(n). For example, O(n) refers to the function f(n)=n. You can plot that on a graph, as shown here:

f(n)=n plotted on a graph
The red line represents the function f(n)=n. It simply means that as n increases, the number of steps increases proportionally (at the same rate).

O(n) then means any function that grows slower than or at the same rate as the function f(n).

For example, in the following graph, the function g(n) grows as n increases. But it remains under the f(n) line.

The function g(n) growing as n increases
If g(n) were the running time of an algorithm, you could say that g(n) is O(n).

Notice also that O(f(n)) is true for sufficiently large values of n. It may be possible that for very small values, the running time is larger than the O(f(n)) function. As long as for large values of n, as n approaches infinity, it remains less than or equal to f(n), and it is still a valid definition. For this reason, you can call this an asymptotic analysis.

Because f(n)=n is a straight line, you can say that this is a linear function and O(n) is a linear growth rate.

Growth rate of the second solution
A second solution was provided for the sum of n numbers problem in the previous lesson. Review that solution below:

function sumIntegers2(n) {
  return (n * (n + 1)) / 2;
}
You can use a similar process as before to count the number of steps that it takes to complete this algorithm.

First, a flow chart of the function will help visualize the steps. Even though the entire function is written in a single expression, there are multiple operations happening. Remember that the model computer can execute only one operation at a time.

Flow chart of function and its steps.
For illustration purposes, only a temporary variable i was introduced to show the order of execution of the expression.

How many times does each of these statements execute? Does it depend on the value of n? It is clear that there are no loop structures in this program, so each statement executes exactly once—regardless of the value of n. That gives a running time of 4.

The order of growth of this solution is O(1), because 1 represents the constant function when all constants are factored out.

1 representing the constant function when all constants are factored out.
Once again, O(1) simply means that the actual running time of this algorithm grows at most as fast as the constant function.

This is called a constant growth rate.

The insert-into-a-sorted-array problem
The video below provides a brief introduction to solving an algorithm using an optimal approach.


Consider the following problem: You are given an array of n numbers. The first n-1 numbers in the array are sorted in ascending order, but the status of the last number is unknown. That is, the last number in the array may or may not be in the right position. Find the correct position for that last number and insert it into the array such that the array becomes fully sorted.

How might you solve such a problem?

Here is an instance of the problem:

Instance of problem.
In the first step, initialize a loop variable to n-2. This will keep track of the value that you are currently looking at in the array.

First step of solution.
Create a new variable named key to hold the value in the last position of the array.

Compare the current value to key. If the current value is greater than key, then copy the current value to position i+1.

Second and third steps of solution.
Repeat until the current value is not greater than key:

Fourth step of solution.
When you find a position that is less than key, copy key into the i+1 position of the array.

To take care of the situation where key is the smallest number in the array, if you get to the front of the array without inserting the key, simply copy the key into the first location of the array.

Those steps may be formalized into a function as follows:

function insertIntoSortedArray(sequence) {
  const key = sequence[sequence.length - 1];

  for (let i = sequence.length - 2; i >= 0; i--) {
    if (sequence[i] > key) {
      sequence[i + 1] = sequence[i];
    } else {
      sequence[i + 1] = key;
      return sequence;
    }
  }
  sequence[0] = key;
  return sequence;
}
Runtime of the insert-into-sorted-array solution

To count the number of steps that this algorithm takes, you'll follow a similar process as in the previous two examples. However, there is one significant difference.

The number of steps depends on both the size of the input array and the values in the array. For example, consider what happens if the input array is [3, 5, 7, 10, 12]. The loop will iterate only once before this algorithm ends. On the other hand, given the input [3, 5, 7, 10, 2], the loop will iterate at least n times.

So, how can you determine the running time of this algorithm?

There are three possible scenarios here. The first example above, where the loop iterates only once, is the best-case scenario. That is, the algorithm has little or no work to do. This case has a constant running time. Although this is a desirable situation, it isn't very interesting. It doesn't give you any information that is useful to the analysis.

The worst case is the scenario where the algorithm has to do the most amount of work. Like the second example above, if the key must be inserted at the beginning of the array, the only way to do that is to iterate the entire array. That gives you a running time of n.

The third possibility is the average case. If the algorithm is executed repeatedly with many different inputs, it will sometimes do a single iteration and sometimes do n iterations. It would be possible to say that on average, it will do n/2 iterations. Because constant coefficients are dropped in asymptotic analysis, this is the same as n.

Even though the average case can give you some meaningful data about the real-world complexity of the algorithm, it doesn't give you a growth rate that is different from the worst case. Think of it as a budgeting problem. To make an effective budget, you need to know the highest possible cost of everything that you need to purchase.

You can conclude that the growth rate of the running time of this algorithm is O(n).

Other running times
In the examples in this lesson, you saw two examples of growth rates: O(n) and O(1). It is possible to use any function in big O notation, but it is common practice to use a few well-defined functions. In the next lesson, some of these common functions will be explored in more detail.
*** 41.6 - Common functions
Common functions
1.5 hoursEstimated completion time
Learning Objective
By the end of this lesson, you will be able to define several well-known functions used in the analysis of algorithms.

Overview
You have already seen two functions, f(n)=1 and f(n)=n. The running time of an algorithm may be expressed with any valid mathematical function. However, it is common practice to use one of several well-known functions. The functions commonly used in the analysis of algorithms are presented in this lesson in order from "best" to "worst."

The constant function
The video below provides a brief introduction to constant functions.


The constant function is as follows:

f(n) = cf(n)=c
Here, c is some fixed constant. That is, for all values of n, the function f(n) remains equal to c. The most fundamental constant function is as follows:

f(n) = 1f(n)=1
This is typically the function used to denote an algorithm with a constant order of growth.

For example, any basic operation, like adding two numbers together or assigning a value to a variable, is done in a single step. Therefore, it is constant.

Recall the summation of integers from 1 to n from the previous lesson:

function sumIntegers2(n) {
  return (n * (n + 1)) / 2;
}
You saw that this algorithm had an order of growth of O(1). You also saw this growth rate depicted on a graph like this:

1 representing the constant function when all constants are factored out.
The meaning of O(1) is that the actual growth rate of this algorithm will be similar to the line f(n)=1 on the graph for all n.

The logarithmic function
The video below provides a brief introduction to logarithmic functions.


Your friend says to you, "Let's play a game. I am thinking of a number between 1 and 10. What number am I thinking of?" How can you guess the number with the fewest possible guesses? Suppose you guess 1, and your friend says, "No, higher." Then you guess 8, and your friend says, "No, lower," and so on.

Random guessing will, of course, have a worst case of 10 guesses. But because your friend is telling you higher or lower after each guess, you can take advantage of that and split the problem in half.

For example, imagine that you guess 5. Now, if your friend says "higher," you only have the numbers 6-10 to work with. And if your friend says "lower," you only have the numbers 1-4 to work with.

Given this approach, what is the maximum number of guesses needed? Here is a decision tree that shows all the possible guesses:

Decision tree showing all possible guesses.
With this approach, the worst possible case will take 4 guesses.

Now, your friend says, "I am thinking of a number between 1 and 20. What number am I thinking of?"

The problem is now twice as big. Once again, random guesses will result in a worst-case scenario of 20 guesses before getting the right answer. That is, it will take twice as long because the problem is twice as big.

But what is the worst case using the splitting technique? Is it twice as big as guessing from 10 numbers? Would it take you up to 8 guesses? Once again, you can use a decision tree to illustrate the guesses.

Decision tree showing all possible guesses using splitting technique.
The maximum number of guesses needed is only 5. The size of the problem was doubled, but the amount of work needed only increased by 1.

Similarly, if you double the problem to 40 numbers, the maximum number of guesses needed is only 6. And this goes on, no matter how big it gets. Try a few examples yourself to verify.

This growth rate, where the number of steps needed increases by 1 every time that the input size doubles, can be described by a logarithmic function. The logarithmic function is denoted as follows:

f(n) = \log nf(n)=logn
Graphically, it looks like this:

Graph of logarithmic function f(n)=log n.
The red constant line was left in the diagram for comparison.

As you can see, the logarithmic curve continues to increase, but the curve gets flatter and flatter without ever being totally flat.

Algorithms like the guessing game above, which tend to split the problem in half at each iteration, generally have a logarithmic growth rate. In later modules, you will encounter several practical algorithms that have logarithmic growth rates.

The linear function
The video below provides a brief introduction to linear functions.


The linear function is denoted as follows:

f(n) = nf(n)=n
This describes a growth rate that is proportional to the size of the input. That is, as the size of the data input to the problem increases, the number of steps correspondingly increases.

You have already encountered several algorithms with a growth rate of O(n). Generally, algorithms that involve a loop through each item in an array take linear time.

Graph showing that algorithms that involve a loop through each item take linear time.
The log-linear function
The log-linear function is denoted as follows:

f(n) = n \log nf(n)=nlogn
This describes a growth rate that grows slightly faster than linear.

Graph showing a growth rate that grows slightly faster than linear.
Later in the program, you will encounter sorting algorithms with a growth rate of O(n log n).

The quadratic function
The quadratic function is denoted as follows:

f(n) = n^2f(n)=n
2

This describes a growth rate that grows at the square of the size of the input. This is significantly faster than any of the other functions so far.

Graph showing a growth rate that grows at the square of the size of the input.
Typically, algorithms with a nested loop will yield a growth rate of O(n²).

Here is an example: Suppose that you are given the prices for a particular stock each day for a number of days. You are allowed to make one purchase on any of the given days and then sell the stock on any subsequent day. Find the best day to buy and the best day to sell to maximize your profit.

The prices are in an array like this:

const prices = [
  113,
  126,
  123,
  98,
  118,
  115,
  99,
  76,
  94,
  114,
  107,
  119,
  114,
  92,
  107,
  103,
  110,
];
One option is to use a brute-force approach and compare the price on every day to every other day, like this:

function maxSubArray(prices) {
  let bestBuy = -1;
  let bestSell = -1;
  let bestProfit = Number.NEGATIVE_INFINITY;
  for (let i = 0; i < prices.length - 1; i++) {
    for (let j = i + 1; j < prices.length; j++) {
      const profit = prices[j] - prices[i];
      if (profit > bestProfit) {
        bestBuy = i;
        bestSell = j;
        bestProfit = profit;
      }
    }
  }
  return [bestBuy, bestSell];
}
To analyze the efficiency of this algorithm, you have to count the steps the same way as was done previously. The outer loop is straightforward to count.

Diagram showing how to analyze the efficiency of the algorithm.
The inner loop, depicted in blue on the diagram, isn't so clear. On each iteration of the outer loop, the inner loop runs a different number of times. How can you determine the actual running time of those steps?

In the first iteration of the outer loop, the inner loop runs from 1 to n. That is, it runs n times. In the second iteration, it runs from 2 to n, or n-1 times. In the third, it runs from 3 to n, or n-2 times. If the array was of length 5, for example, the inner loop would run 5+4+3+2+1 times.

Luckily, you already saw that the sum of all integers from 1 to n is given by the formula n(n+1)/2. Therefore, you can say that each statement in the inner loop runs n(n+1)/2 times.

You can rearrange this to remove those parentheses as follows:

Formula rearranged to remove parentheses.
Don't be alarmed if the math here looks a bit complicated. In this program, you won't be required to do such math yourself. Just try to follow why the conclusions are drawn.

For the purposes of this analysis, you can then take it that each instruction in the inner loop takes this number of steps to complete. That gives you the following:

1 + 1 + 1 + 1 + n + n-1 + n-1 + 1 + \frac{1}{2}n^2 + \frac{1}{2}n + \frac{1}{2}n^2 + \frac{1}{2}n + \frac{1}{2}n^2 + \frac{1}{2}n + \frac{1}{2}n^2 + \frac{1}{2}n + \frac{1}{2}n^2 + \frac{1}{2}n + \frac{1}{2}n^2 + \frac{1}{2}n + \frac{1}{2}n^2 + \frac{1}{2}n1+1+1+1+n+n−1+n−1+1+
2
1
​
 n
2
 +
2
1
​
 n+
2
1
​
 n
2
 +
2
1
​
 n+
2
1
​
 n
2
 +
2
1
​
 n+
2
1
​
 n
2
 +
2
1
​
 n+
2
1
​
 n
2
 +
2
1
​
 n+
2
1
​
 n
2
 +
2
1
​
 n+
2
1
​
 n
2
 +
2
1
​
 n
So bring all the like terms together:

\frac{1}{2}n^2 + \frac{1}{2}n^2 + \frac{1}{2}n^2 + \frac{1}{2}n^2 + \frac{1}{2}n^2 + \frac{1}{2}n^2 + \frac{1}{2}n^2 + n + n + n + \frac{1}{2}n + \frac{1}{2}n + \frac{1}{2}n + \frac{1}{2}n + \frac{1}{2}n + \frac{1}{2}n + \frac{1}{2}n + 1 + 1 + 1 + 1 + 1 - 1 - 1
2
1
​
 n
2
 +
2
1
​
 n
2
 +
2
1
​
 n
2
 +
2
1
​
 n
2
 +
2
1
​
 n
2
 +
2
1
​
 n
2
 +
2
1
​
 n
2
 +n+n+n+
2
1
​
 n+
2
1
​
 n+
2
1
​
 n+
2
1
​
 n+
2
1
​
 n+
2
1
​
 n+
2
1
​
 n+1+1+1+1+1−1−1
This can be simplified to the following:

\frac{7}{2}n^2 + \frac{10}{2}n + 3
2
7
​
 n
2
 +
2
10
​
 n+3
But, you already saw that only the highest-order term really counts in this analysis. That means that you can drop the n term and the constant term, giving the following:

\frac{7}{2}n^2
2
7
​
 n
2

And the constant coefficient doesn't matter, so you end up with this:

n^2n
2

Therefore, the order of growth of the running time of this algorithm is O(n²).

Even though the above analysis was done in painful detail, that isn't always necessary. This was done to illustrate that even though the inner loop was running a different number of times on each iteration of the outer loop, the running time of the algorithm was still O(n²).

For simple algorithms like this, you can look at the nested loop and assume that the running time is O(n²)—without going through all the steps.

Cubic and higher-order polynomials
If a nested loop results in a running time of O(n²), what is the running time of triple nested loops? This running time can be defined by the cubic function:

f(n) = n^3f(n)=n
3

On the graph, this function grows faster than anything that you have seen so far.

Graph showing the function grows faster than anything you have seen so far.
Consider the following problem:

A Pythagorean triplet is a set of three numbers {a, b, c} that satisfies the following equation:

a^2 + b^2 = c^2a
2
 +b
2
 =c
2

Find all Pythagorean triplets up to n.

To do this, you can once again use a brute-force technique that requires nested loops:

function triplets(n) {
  const result = [];

  for (let a = 1; a <= n - 2; a++) {
    for (let b = a + 1; b <= n - 1; b++) {
      for (let c = b + 1; c <= n; c++) {
        if (a * a + b * b === c * c) {
          result.push([a, b, c]);
        }
      }
    }
  }

  return result;
}
Without going through all the steps, it should be easy to spot that this algorithm has a growth rate of O(n³).

Polynomials
A polynomial is a fancy mathematical term for a class of functions that involve multiple terms joined by addition, subtraction, multiplication, and integer exponents.

You have already seen several polynomial functions in this lesson.

For example, because n⁰=1, f(n)=1 is a polynomial expression. Also, because n¹=n, f(n)=n is also a polynomial.

Similarly, f(n)=n² and f(n)=n³ are polynomials.

It is rare that you will see algorithms with a running time as bad as O(n³), O(n⁴), or higher—but they do exist. This program will lump them all into the category of higher-order polynomials and consider them so bad that algorithms with such running times aren't practical.

Exponential functions
Suppose you wish to write a program that will break a password by guessing every possible combination. For simplicity, suppose that you know that the password only consists of numbers. If the password is of length 2, how many combinations will you need to check?

00
01
02
03
...
10
11
12
13
...
96
97
98
99
That is 10², or 100 possible combinations. What if the password is of length 3? By similar logic, you can see that you need to try 10³, or 1,000 possible combinations.

In general, breaking a password of length n requires 10ⁿ tries. This gives the brute-force password-breaking algorithm a running time of O(10ⁿ).

Functions of the form shown below, where c is some constant, are called exponential functions.

f(n) = c^nf(n)=c
n

This running time is so bad that the exponential line looks almost parallel to the y-axis if it is plotted on a graph.

Other functions
There are many other functions that may be used in the analysis of algorithms, but the ones covered here are the most commonly seen. In this module, you can assume that the running time of any algorithm provided falls into one of these categories.

Summary
In summary, the functions discussed, in order of best to worst, are as follows:

Name
Function
Use with big O
Constant
f(n) = 1
O(1)
Logarithmic
f(n) = log n
O(log n)
Linear
f(n) = n
O(n)
Log linear
f(n) = n log n
O(n log n)
Quadratic
f(n) = n²
O(n²)
Cubic
f(n) = n³
O(n³)
Higher-order polynomials
f(n) = nᵏ, where k > 3
O(nᵏ)
Exponential
f(n) = cⁿ, where c > 1 is a constant
O(cⁿ)

*** 41.7 - Space complexity
Space complexity
1.5 hoursEstimated completion time
Learning Objective
By the end of this lesson, you will be able to determine the space complexity of a code sample.

Overview
So far, you have only learned about the time complexity of the algorithms. That is, you've learned about the worst-case (big O) amount of time that it takes for the algorithm to complete the task. However, it's also important to pay attention to the amount of space that the algorithm takes to complete the task. The space complexity of an algorithm relates to how much memory the algorithm uses.

Key Terms
Auxiliary space
The temporary or extra space used by the algorithm while it is being executed
Understanding space complexity
Space complexity is a measure of the amount of working storage that an algorithm needs. It answers the following question: in the worst case, how much memory is needed at any point in the algorithm?

Space complexity includes both auxiliary space and the space used by the input. Auxiliary space is the temporary or extra space used by the algorithm while it is being executed.

As with time complexity, you are mostly concerned with how the space needs grow, in big O terms, as the size of the input grows. Below are a few examples of expressing space complexity using big O notation, starting from slowest space growth (best) to fastest (worst):

O(1): Constant complexity means that the algorithm takes the same amount of space regardless of the input size.

O(log n): Logarithmic complexity means that the algorithm takes space proportional to the log of the input size.

O(n): Linear complexity means that the algorithm takes space directly proportional to the input size.

O(n log n) Log-linear or quasilinear complexity (also called linearithmic) means that the space complexity grows proportionally to the input size and a logarithmic factor.

O(n²): Quadratic complexity means that the space complexity grows proportionally to the square of the input size.

The space complexity analysis works similarly to time complexity.

Constant space (O(1))
To get warmed up, consider a simple function that sums two numbers:

function sum(left, right) {
  return left + right;
}
In this particular function, three variables are used and allocated in memory:

The first parameter, left

The second parameter, right

The return value

In JavaScript, a single number occupies eight bytes of memory. In the above function, there are three variables assumed to be numbers. Therefore, this algorithm always takes 24 bytes of memory to complete (3 * 8 bytes). As a result, the space complexity is constant, so it can be expressed in big O notation as O(1).

Logarithmic space (O(log n))
Logarithmic space complexity (O(log n)) is the next best thing after constant space. Although logarithmic space complexity algorithms do require more space with larger inputs, space usage increases slowly. For example, imagine that you have a function, quickSort(), which takes 128 bytes to process an input of size 10. When you increase the input by 10 times, to 100, the space required only grows to 256 bytes. When you increase the input size to 1,000, the space only grows to 384 bytes.

It is also characteristic of logarithmic algorithms that they cut the problem size in half each round through.

As you may have guessed, quicksort uses O(log n) space. You will learn more about quicksort in a later lesson.

Linear space (O(n))
Algorithms with linear space complexity (O(n)) use an amount of space directly proportional to the size of the input. Every time that you double the size of the input, the algorithms will require twice as much memory.

For example, look at the space complexity of a function that sums all of the elements in an array:

function sum(numbers) {
  let total = 0;
  for (const number of numbers) {
    total += number;
  }
  return total;
}
Again, list all variables present in the above code:

numbers: The space taken by the array is equal to 8n bytes, where n is the length of the array

number: An 8-byte number

total : An 8-byte number

The total space needed for this algorithm is 8n+8+8 bytes. The highest order of n in this equation is just n. Thus, the space complexity of the above is O(n).

Space-time complexity tradeoff
An algorithm's efficiency is a combination of its time and space complexity. So an efficient algorithm is one that is fast and that takes the least amount of memory possible.

Space and time complexity are often linked. Usually, increasing speed leads to greater memory consumption, and vice versa. However, this isn't always the case; sometimes, space and time complexity aren't correlated. For example, bubble sort is a slow algorithm that occupies minimal space. On the other hand, merge sort (which you will learn about later) is an extremely fast algorithm that requires a lot of memory.

There are also some balanced sorting algorithms. In these, the speed and space usage aren't the best, but they are acceptable in most cases.

*** 41.8 - Assessment: Intro to data structures and algorithms
** Module 42 - Searching and Sorting
*** 42.1 Overview: Searching and sorting

In this module, you will learn how to implement and use common searching and sorting algorithms for arrays.

Key Terms:
+ Searching algorithms: Algorithms that iterate through data to retrieve specific data
+ Sorting algorithms: Algorithms that bring order to data, making it easier to search, display, and understand the data

Searching and sorting are fundamental algorithms that are used by any application that deals with data. Searching refers to iterating through the data to retrieve specific data, and sorting refers to putting the data in order.

Searching algorithms are essential for a wide variety of tasks. For example, the IRS uses searching to look up your tax records. And more broadly, you use searching algorithms anytime that you use a search engine like Google. Likewise, sorting is important to bring order to data; this makes it easier to search, display, and understand the data. For example, search engine results are sorted such that the most relevant sites for a given search term are listed first. And the contacts on your phone are ordered alphabetically by name so that it is easier to find a specific person.

Searching algorithms and sorting algorithms are different for different data structures. In this module, you'll focus on searching and sorting for arrays.

*** 42.2 Linear search

Overview
Finding a specific element in an array of data is one of the most common
activities that you will do in programming.
In this lesson, you will learn about linear search, which is the most basic
algorithm for searching through an array of data.

Key Terms
- Linear search algorithm: Also known as the sequential search algorithm, an
algorithm that checks every element in an array, starting from the leftmost
element, and continues until the desired element is found
- Brute-force algorithm: Any algorithm that doesn't use any logic to try to do its
job quickly or somehow reduce the number of elements that it searches for a
matching value

Understanding linear search
The linear search algorithm, also known as the sequential search algorithm,
checks every element in an array until the desired element is found. The
algorithm starts at the leftmost element. Then it continues searching until it
either finds the desired element or goes through all of the elements in the
array. Once the first element with the correct value is found, the position (or
index) for that element is returned. If the desired element isn't in the array,
-1 is returned.

Consider the following example, where the linear search algorithm is searching
an array of numbers to find the value 37.

Example of linear search algorithm searching an array of numbers to find the value 37.
In the above example, 11—the index of 37—is returned.

Linear search in JavaScript
The classic example of a linear search algorithm is the indexOf() function,
which searches for a particular value within an array. Take a look at the
traditional implementation of indexOf():

function indexOf(value, elements) {
  for (let index = 0; index < elements.length; index++) {
    if (elements[index] === value) {
      return index;
    }
  }
  return -1;
}
The indexOf() function above accepts two parameters: the desired value and the
  array to which the linear search algorithm is applied.

The algorithm iterates through the array, checking each value until it finds an
element equal to the supplied value. At that point, the index—the first position
of the value in the array—is returned. If the algorithm reaches the end of the
array without finding the value, it returns -1, indicating that the value wasn't
found.

Consider the following code:

const index = indexOf(5, [1, 3, 5, 7, 9]);
console.log(index);
When you run this code, 2 will be printed because the value 5 is at array index 2.

But what happens if you have an array of objects like the following?

const people = [
  {
    id: 1,
    first_name: "Monah",
    last_name: "Yarnall",
    age: 17,
  },
  {
    id: 2,
    first_name: "Daphne",
    last_name: "McGaugey",
    age: 81,
  },
  {
    id: 3,
    first_name: "Walker",
    last_name: "Bucknell",
    age: 81,
  },
];
How do you use the above indexOf() function to find the first person whose age
is 81? As you can see, the problem with the traditional implementation is that
it must exactly match the value of an element in the array; there is no way to
match on a property of an object in the element. If you want to match in a
different way, you have to write an entirely new function.

A better solution is to separate the matching logic from the linear search
algorithm. To accomplish this, you can have the caller pass in a function that
implements the matching logic (returning true when a matching element is found).
Then the linear search algorithm can call that function to determine whether or
not the current element is a match.

The following video provides an overview of this topic. Watch the video, and then complete the practice task below.


Do this
Implement a linear search
Now you will implement a linear search algorithm as a higher-order function that takes the following two parameters:

isMatch() is a function that takes the current element, current index, and original array as parameters. It returns true when a matching element is found.

elements is the array to which the linear search algorithm is applied.

In a new directory, create a file named indexOf.js. In indexOf.js, add the following code:

function indexOf(isMatch, elements) {
  if (Array.isArray(elements)) {
    for (let index = 0, length = elements.length; index < length; index++) {
      if (isMatch(elements[index], index, elements)) {
        return index;
      }
    }
  }
  return -1;
}

module.exports = indexOf;
In this implementation, the algorithm goes through all of the elements in the array and calls isMatch(), passing in the element, index, and array. The variable index keeps track of where it is in the array. If isMatch() returns true, it will return the current value for index. But in cases where isMatch() always returns false, -1 is returned after the loop; this indicates that the function didn't find the desired element.

Now that you have an implementation of indexOf(), you will use it to search for matching elements.

Next, create a file named linearSearch.js. In linearSearch.js, add the following code:

const indexOf = require("./indexOf");

const people = [
  {
    id: 1,
    first_name: "Monah",
    last_name: "Yarnall",
    age: 17,
  },
  {
    id: 2,
    first_name: "Daphne",
    last_name: "McGaugey",
    age: 81,
  },
  {
    id: 3,
    first_name: "Walker",
    last_name: "Bucknell",
    age: 81,
  },
];

function personIs81(person) {
  return person.age === 81;
}

console.log(indexOf(personIs81, people));

function numberIs5(value) {
  return value === 5;
}

console.log(indexOf(numberIs5, [1, 3, 5, 7, 9]));
Then run the code, using the command node linearSearch.js.

As you can see, passing in the isMatch() function isolates the search algorithm from the matching criteria. Now the search algorithm doesn't need to change, no matter what the matching criteria happen to be.

The efficiency of linear search
Linear search is a classic example of a brute-force algorithm. In other words,
it doesn't use any logic to try to do its job quickly or somehow reduce the
number of elements that it searches for a matching value. As a rule, the larger
the array is, the longer that linear search will take to find a matching
element.

As shown in the table below, the worst-case time complexity of linear search is
O(n), where n is the number of elements in the array being searched. With linear
search (as well as most other search algorithms), the worst-case scenario occurs
when the element doesn't exist in the array. If the element isn't in the array,
the algorithm would still need to iterate through all n elements to determine
that the element isn't there.

The best-case time complexity of linear search is O(1). This happens when the
element that you are looking for is in the first slot of the array. The
algorithm wouldn't really iterate n times; it would find the element on the
first try.

Linear search is by far the simplest search algorithm; it's one that doesn't
focus on speed.

Description Notation Explanation
Worst case O(n) There is no matching element in the array.
Average case O(n) The matching element is in the middle of the array.
Best case O(1) The element is in the first slot of the array.

*** 42.3 Binary search

Overview
Searching a sorted collection is a common task. If the list to be searched
contains more than a few items, a binary search will require far fewer
comparisons than a linear search. However, binary searches impose the
requirement that the list be sorted.

Key Terms
Binary search algorithm - Also known as a half-interval search algorithm, an
algorithm to find a specific element located in a sorted array
Understanding binary search - The video below provides a brief introduction to
binary search. Start by watching the video, and then read through the rest of
the lesson and complete any practice tasks.


Binary search, also known as a half-interval search, is a search algorithm to
find a specific element located in a sorted array. It only works with sorted
arrays. Binary search is advantageous over linear search because it searches
more quickly and efficiently. It does this by halving the number of elements to
check with each iteration.

As you learned in the previous lesson, a linear search algorithm searches the
array starting at index 0, then 1, 2, 3, 4, and so on. In contrast, a binary
search divides the array in half each time that it looks for a target value.

In a binary search, you define an index to refer to the element in the middle of
the array. Then you test whether the target value is less than, greater than, or
equal to the value at the index. If the target value is less than or greater
than the index, the algorithm removes either the left or right half of the array
from the search, respectively. If the element is found, the position (or index)
of the element is returned. If the desired element isn't in the array, -1 is
returned.

This can be confusing, so take a look at the following visualization of this
process. In this example, the algorithm is searching an array of increasing
numbers to search for the value 37.

Example of algorithm searching an array of increasing numbers for the value 37.
Binary search algorithms work as follows:

You need to keep track of three things to perform a binary search: lowerIndex,
index, and upperIndex.

lowerIndex will always start at 0: let lowerIndex = 0;

upperIndex is calculated using array.length: let upperIndex = array.length—1;

index is calculated by summing lowerIndex and upperIndex, and then dividing
by 2. Math.floor() is used to round down in case of an odd number of elements:
const index = Math.floor((lowerIndex+upperIndex)/2).

The while loop will then repeat until it ends. In this case, the loop is as
follows: while(lowerIndex <= upperIndex).

The algorithm compares the number at index to the target number to determine if
it is greater than, less than, or equal to the target value.

If the value at index is equal to the target value, return index.

If the value at index is less than the target value, then the target value will
be somewhere to the right of the index. The algorithm will then assign the
lowerIndex to index+1, thus ignoring the left half of the array.

If the value at index is greater than the target value, then the target value
will be somewhere to the left of the index. The algorithm will then assign
upperIndex to index-1, thus ignoring the right half of the array.

If the target value isn't found, the value of index is recalculated to be in the
middle of the new values for lowerIndex and upperIndex (const index =
Math.floor((lowerIndex+upperIndex)/2)). Then the loop continues at the previous
step. The loop may iterate once, or it may iterate dozens of times, depending on
the array size and the target number.

Put this all together, and you have a binary search!

Binary search in JavaScript
Just as you did with linear search, you will separate the matching logic from
the search algorithm. To accomplish this, the caller will supply a compare
function. This function tells the binary search whether to search to the left,
search to the right, or return index because the current element is a match.

Watch the following videos for an overview of this topic. Then practice applying
your new binary search skills by completing the practice task below.



Do this
Implement a binary search
Now you will implement a binary search algorithm as a higher-order function that
takes the following two parameters:

compare: A function that takes the current element, current index, and original
array as parameters. It returns one of the following:

0 if the current element is equal to the desired value

A positive value if the current element is greater than desired value by the
ordering criterion

A negative value if the current element is less than desired value by the
ordering criterion

sortedElements: The sorted array to which the binary search algorithm is
applied.

Next, create a file named binaryIndexOf.js. In binaryIndexOf.js, add the
following code:

function binaryIndexOf(compare, sortedElements) {
  if (Array.isArray(sortedElements)) {
    let lowerIndex = 0;
    let upperIndex = sortedElements.length - 1;

    while (lowerIndex <= upperIndex) {
      const index = Math.floor((upperIndex + lowerIndex) / 2);

      const comparison = compare(sortedElements[index], index, sortedElements);

      if (comparison > 0) {
        lowerIndex = index + 1;
      }

      if (comparison === 0) {
        return index;
      }

      if (comparison < 0) {
        upperIndex = index - 1;
      }
    }
  }
  return -1;
}

module.exports = binaryIndexOf;
As you can see, lowerIndex always starts at zero (let lowerIndex = 0;), and
upperIndex starts at the largest index for the array (let upperIndex =
sortedElements.length-1;). If the array is empty, lowerIndex === 0 and upperIndex
=== -1, which means that the while loop never executes. If lowerIndex <=
upperIndex, then the while loop does execute.

In the while loop, the first step is to calculate the value of index. There may
be an odd number of elements, so you will use Math.floor() to round down to the
nearest integer. Now, index points to the middle of the array.

Next, call the compare() function, passing in sortedElements[index], index, and
sortedElements. The return value from compare will tell the algorithm what to do
next.

If compare() returns 0, the target value is found, so index is returned.

If compare() returns a value greater than 0, the target value is greater than
the value at index, so lowerIndex is moved to index+1. In other words, the
algorithm ignores the elements to the left of index.

If compare() returns a value less than 0, the target value is less than the
value at index, so upperIndex is moved to index-1. In other words, the algorithm
ignores the elements to the right of index.

This process then repeats, moving lowerIndex and upperIndex closer and closer
together. This continues until compare() returns 0, or until lowerIndex is
greater than upperIndex and -1 is returned.

Now that you have an implementation of binaryIndexOf(), you will use it to
search for matching elements. Create a new file named binarySearch.js. In
binarySearch.js, add the following code:

const search = require("./binaryIndexOf");

const elements = [1, 2, 4, 7, 13, 24, 44, 81, 149, 274, 504, 927, 1705, 3136];

function forNumber(target) {
  return (element, index) => {
    console.log("compare", target, "to", element, "at index", index);

    if (element === target) {
      return 0;
    }
    if (element < target) {
      return 1;
    }
    if (element > target) {
      return -1;
    }
  };
}

console.log(search(forNumber(274), elements));
Then run the code, using the command node binarySearch.js. You will see the following:

compare 274 to 44 at index 6
compare 274 to 504 at index 10
compare 274 to 149 at index 8
compare 274 to 274 at index 9
9
In this case, binary search required only four steps to find the target value. A
linear search would have required nine steps.

As you can see, you can use the compare function passed into binaryIndexOf() to
observe exactly what the algorithm is doing—without changing the code in
binaryIndexOf.js. This can be very helpful when debugging the search algorithm.

The efficiency of binary search
Binary search is a classic example of dividing a problem into a number of
smaller problems. With each iteration of the search, the number of elements
being considered gets cut in half. As a rule, a linear search will be faster for
fewer than 10 elements. However, for larger arrays, a binary search will be more
efficient.

As you can see in the table below, the worst-case time complexity of binary
search is O(log n), where n is the number of elements in the array being
searched. As with linear search, the worst case for binary search occurs when
the element doesn't exist in the array. When the element isn't in the array, the
algorithm needs to go through log(n) elements to determine that the element
isn't there.

The best-case time complexity of binary search is O(1). This happens when the
matching element is in the middle of the array (first comparison).

The average-case complexity is O(log n). This happens when the matching element
is somewhere in the array but not in the middle.

Description Notation Explanation
Worst case O(log n) There is no matching element in the array.
Average case O(log n) The matching element is somewhere in the array but not in the middle.
Best case O(1) The matching element is in the middle of the array.

*** 42.4 Recursion


Overview
You know that a function can call any other function, but did you know that a
function can also call itself? A function that calls itself is called a
recursive function, and that's exactly what you'll learn about in this lesson.

Key Terms
Recursion - A problem-solving method that involves a function calling itself
Top-down recursive function - A function that calculates the solutions to each subproblem in the forward phase, passing the results of the calculation to the recursive call
Bottom-up recursive function - A function that breaks down the problem into increasingly smaller problems until the base case is encountered, and then combines the solutions in the backward phase
Stack overflow error - A runtime error where the call stack gets too big and runs out of space
Forward phase - Also called the recursive phase, the phase that happens when the function is calling itself and continues until the base case is satisfied
Backward phase - Also called the back-out phase, the phase that starts when a function call satisfies the base case, then returns a value to the function that called it, and continues until a value is returned to the initial function call

Recursion
In computer science, recursion is a problem-solving method that involves a function calling itself. In each call, it breaks down the problem into smaller and smaller subproblems until it reaches a problem small enough that it can be solved trivially.

Think of recursion as a process of defining the solution to a problem in terms of a simpler version of itself. Recursion may be applied to several data structures and algorithms to solve problems, but recursion itself isn't a data structure or an algorithm—it is a concept.

A recursive algorithm is broken down into two parts:

The base case (or base cases), which indicates when to stop

The recursive case (or recursive cases), which is where the function calls itself

Key Term
Base case: The solution to the "simplest" possible problem; this case provides a terminating condition for the recursive case

Note that the base case doesn't require calling itself. It ensures that you don't infinitely recurse: it provides a terminating condition for the recursive case.

Key Term
Recursive case: Where you call the same function to solve increasingly simple versions of the problem

A recursive algorithm may have more than one base case or recursive case. In other words, it may involve calling the same function with different arguments for each recursive case.

To better understand this concept, consider the following example. For simplicity, this example consists of a single base case and a single recursive case.

Factorial
The factorial for any positive integer n is written as n!. It is defined to be the product of all integers between 1 and n, inclusive:

n!= n * (n−1) * (n−2) * \ldots * 1n!=n∗(n−1)∗(n−2)∗…∗1
So if you were to calculate 4!, the result would be 24:

4! turns into 4*3*2*1=24.

And 5! is 120:

5! turns into 5*4*3*2*1=120, which is 5*4!.

As you can see, each factorial calculation for n! is n*(n-1)!. But what is the base case? Because n must be a positive integer, once n is less than or equal to 1, there is no simpler version of n!. Any n less than or equal to 1 is the base case for the factorial function. This means that the function should stop calling itself when n <= 1.

Do this
Implement a recursive factorial
Now, you will implement a recursive factorial function.

First, create a file named factorial.js. In factorial.js, add the following code:

function factorial(number) {
  console.log("Forward phase", number);

  // Base case
  if (number <= 1) {
    console.log("base case", number);
    return 1;
  }

  // Recursive case
  const numberMinusOneFactorial = factorial(number - 1);

  console.log("Backward phase", number, "*", numberMinusOneFactorial);

  return number * numberMinusOneFactorial;
}

console.log(factorial(5));
In the above function, note the following:

The console.log() statements record the phases of the recursive function. More on this later.

The if statement is the base case. It checks to see if the value of number is less than or equal to one. If so, the return value is always 1.

In the recursive case, the function calls itself to calculate factorial(number-1). It is breaking down the problem into a set of smaller problems like the formula above n! = n * (n-1)!.

After the recursive case, the function can calculate the return value for number!.

Now, run the code, using the command node factorial.js. You will see something like the following:

node factorial.js

Forward phase 5
Forward phase 4
Forward phase 3
Forward phase 2
Forward phase 1
base case 1
Backward phase 2 * 1
Backward phase 3 * 2
Backward phase 4 * 6
Backward phase 5 * 24
120
As you can see, there are two distinct phases in a recursive function: the forward phase and the backward phase.

The forward phase of recursion happens when the function is calling itself. The forward, or recursive phase, continues until the base case is satisfied. The backward phase (also called the back-out phase) starts when a function call satisfies the base case. The call then returns a value to the function that called it. This continues until a value is returned to the initial function call.

As you can see from the output above, the first call to factorial(5) doesn't return until all recursive calls have returned.

The animation below shows each of the recursive calls that are happening within the forward phase of the call to factorial(5):

Example showing each recursive call within the forward phase of the call to factorial(5).
Once the base case is encountered, the call to factorial(1) returns 1 to the caller, which in turn returns 2*1 to its caller. This pattern continues until control is returned to the caller of factorial(5). This is the backward phase.

In short, the forward phase is breaking down the problem into increasingly smaller problems until the base case is encountered. Then the recursive function switches to the backward phase, which combines the base case with the previous problem to ultimately provide a solution for the original problem. This is called a bottom-up recursion because there is no work done until the base case is encountered.

Bottom-up recursion
A bottom-up recursive function, like the example above, breaks down the problem into increasingly smaller problems until the base case is encountered. Then it combines the solutions in the backward phase. In other words, there are no solutions or calculations until the base case is reached. This is a very common approach in recursive functions.

As you might have guessed, there is also a top-down approach, which you will read about next.

Top-down recursion
A top-down recursive function calculates the solutions to each subproblem in the forward phase, passing the results of the calculation to the recursive call. In other words, the solution is incrementally calculated, and by the time the base case is reached, the full solution is calculated.

Next, you will write a top-down recursive factorial function.

Do this
Implement a top-down recursive factorial
Now, implement a top-down recursive factorial function.

First, create a file named topDownFactorial.js. In topDownFactorial.js, add the following code:

function factorial(number, total = 1) {
  console.log("Forward phase", number, "*", total);

  // Base case stays the same
  if (number <= 1) {
    console.log("base case", number);
    return total;
  }

  total = factorial(number - 1, total * number);

  console.log("Backward phase", number, total);
  return total;
}

console.log(factorial(5));
In the above function, note the following:

There is a new parameter, total, used to pass information to the recursive function call. The total parameter starts at 1 so that it can be multiplied by each number in the subproblems.

The if statement for the base case is still the same, but rather than return 1, it always returns the current total.

In the recursive case, the function calls itself, passing in total * number as the total parameter, thus calculating the total in the forward phase.

After the recursive case, no further calculation is required, so this function returns the value from the recursive call.

When you run the code using node topDownFactorial.js, you will see something like the following:

node topDownFactorial.js

Forward phase 5 * 1
Forward phase 4 * 5
Forward phase 3 * 20
Forward phase 2 * 60
Forward phase 1 * 120
base case 1
Backward phase 2 120
Backward phase 3 120
Backward phase 4 120
Backward phase 5 120
120
In looking at the output above for five factorial, the total is the result of the previous calculation. For example, the first line of output shows the number 5, times the total, 1. The second line of output uses the product of 5 and 1 to get the value 5. The third line of output uses the product of 4 and 5 to get the value 20, and so on. Each recursive call is passed the total derived from the previous calculation for use in its own calculation.

As you can see, there are still two distinct phases in this recursive function: the forward phase and the backward phase. However, the solution is calculated in the forward phase.

Top-down and bottom-up recursive functions can provide simple solutions to complicated problems, but you might run into an error if the number of subproblems gets too big.

Stack overflow errors
Recursion has a problem: it builds up a call stack of size n, where n is the number of subproblems. This makes it vulnerable to a stack overflow error, where the call stack gets too big and runs out of space. For example, if you call factorial(5000000), you will get a Maximum call stack size exceeded error during the forward phase.

To avoid the stack space limitation, you can rewrite the recursive function to use iteration instead. The following is an iterative factorial() function:

function factorial(number) {
  // Base case
  if (number <= 1) {
    return 1;
  }

  // Penultimate means second to the last in a series.
  let penultimate = 1;
  let total = 0;

  // Calculate the factorial from 1 to `number`
  for (let ultimate = 1; ultimate <= number; ultimate++) {
    console.log(ultimate, "*", penultimate);
    total = ultimate * penultimate;
    penultimate = total;
  }
  return total;
}
All recursive functions can be rewritten to be iterative functions. Sometimes, the recursive function is easier to understand than the iterative version. For example, many programmers would say that the recursive factorial() function is easier to understand than the iterative version.

Principles of recursive problem-solving
To think recursively, you ask the question, "What could I do if I had the solution to a smaller version of this same problem?"

For example, recall the sum of the first n integers problem that was introduced in a previous lesson. The problem is as follows: given an integer n, find the sum of all integers from 1-n, inclusive. For example, if n=4, the solution involves the sum 1+2+3+4=10.

How would you go about solving this recursively?

Step 1: The smaller subproblem
The first step is to think about the smaller problem. If you need to find the sum of all numbers from 1-5, you may ask, "Is there a smaller version of this same problem?" In this case, finding the sum of all integers from 1-4 is a smaller problem than finding the sum of all integers from 1-5.

You know that this smaller subproblem would help if you can take the solution to the smaller subproblem and use it to solve the original problem. For example, suppose that you know that the sum of all integers from 1-4 is 10. How can that help you find the sum of all integers from 1-5?

In this case, you take the solution to the subproblem and add 5, giving you 10+5=15.

Step 2: The recursion
The question now is, "How did you find the solution to the sum of all numbers from 1-4?" Can you apply the same steps that you applied above?

The problem is to find the sum of all integers from 1-4. The smaller problem is to find the sum of all integers from 1-3. Again, supposing that you did have the solution to that smaller problem, what would you do? Given that the sum of all numbers from 1-3 is 6, you would simply add 4 to that solution—giving 6+4=10.

Notice that solving the sum from 1-4 problem involved the same steps as solving the sum from 1-5 problem. You could even go another step; think about solving the sum from 1-3 problem and see if the same steps apply.

At this point, you know the following:

To solve for n, you need to get the solution for the n-1 subproblem.

If you have the solution for the n-1 subproblem, you just add n to get the final solution.

Step 3: The base case
If you keep reducing the size of the problem as you did in the step above, eventually the problem will get small enough that the answer is trivial. In this case, you are reducing the problem by 1 each time. Eventually, you will get to the sum of all integers from 1-1 problem.

At this point, there really is no calculation necessary. The sum of all integers from 1-1 is just 1. This is the base case.

You now have all the pieces to put together a complete solution.

Step 4: Putting it all together
It is now time to put the pieces together into a solution to the problem. You have the following pieces:

The base case: When n is 1, the answer is 1.

The recursive case: When n is greater than 1, find the solution to n-1.

The summation: The answer is to add n to the solution to n-1.

In pseudocode, this algorithm may be written as follows:

function sum(n):
  // Accepts n - find the sum from 1 to n
  // First, check the base case
  if n is equal to 1 then:
    return 1

  // Otherwise, find the solution to the subproblem
  // by making a recursive call
  subproblemSum = sum(n - 1)

  // Summation
  return n + subproblemSum;
Do this
Implement the sum to n function
Create a file named sumToN.js, and add the following implementation.

function sum(n) {
  // Check the base case
  if (n === 1) {
    return 1;
  }

  return n + sum(n - 1);
}
Test that this function provides the correct output.

String splitter
The two examples above involved numbers, but recursion may be applied to many types of problems. Consider splitting a string according to some given separator.

Given a string and a separator character, split the string by the separator and return an array of the substrings created. You may recognize this as JavaScript's built-in string split() function. You are going to use recursion to implement your own version of this function.

For example, given the string "Be generous in prosperity and thankful in adversity" and the separator " ", the algorithm should return the array ["Be", "generous", "in", "prosperity", "and", "thankful", "in", "adversity"].

The subproblem
What would a smaller version of this problem look like? What affects the complexity of the problem? In this case, it is the number of occurrences of the separator in the string.

Take as an example the string "4-8-2023" and the separator "-". A smaller version of this problem is a string with one less separator, say "8-2023". What if you already had the solution to that subproblem? You can see that the solution would be ["8", "2023"].

The final solution would be inserting the substring "4" into the array, giving ["4", "8", "2023"].

The recursion
In a similar manner, to find the solution to "8-2023", you can find the solution to the subproblem "2023" and simply insert the string "8" in the resulting array.

The base case
If you keep reducing the string by the number of separators found in the string, eventually you end up with a string with no separators. If the string contains no separators, what is the solution?

The solution is simply an array containing the given string. Given the string "2023" and the separator " ", the solution is the array ["2023"].

Putting it all together
You now have the necessary pieces of the solution.

The base case: For a string containing no separators, return an array containing the string.

The recursive case: Find the first separator, extract the substring from the first character to the first separator, and recursively solve the rest of the string from the first separator onward.

Final solution: Insert the substring into the array returned from the recursive step.

In pseudocode, this may be written as follows:

function split(text, separator):
  // Inputs: text - the string to be split
             separator - the character to be used to split the text

  search for the first occurrence of the separator in the text
  // Base case
  if no separator is found then:
     return [text]

  // Recursive case
  extract substring from character 0 to index of first occurrence of separator
  extract rest of string from first occurrence of separator + 1 to end of string
  recursively solve the rest of string
  insert the substring in the resulting array
  return the array
Do this
Implement the string split function
Create a file named split.js, and add the following implementation.

function split(text, separator) {
  // Find the index of the first occurrence of separator
  let index = text.indexOf(separator);

  // Base case
  if (index === -1) {
    return [text];
  }

  // Find the substrings
  let start = text.substring(0, index);
  let rest = text.substring(index + 1);

  // Recursive call
  let restSolution = split(rest, separator);

  // Insert the first substring
  restSolution.unshift(start);

  return restSolution;
}
Test the function to ensure that it works as expected.

The efficiency of recursive functions
In JavaScript, a loop is essentially always more efficient than a recursive function call. Allocating memory for the next function call takes time and memory that isn't required in a loop. However, recursion is almost never used for performance reasons; it's used to make the problem simpler and the code easier to understand.

*** 42.5 Bubble sort

Bubble sort
1.5 hoursEstimated completion time
Learning Objective
By the end of this lesson, you will be able to implement bubble sort and give the runtime.

Overview
Sorting items in an array is among the most common activities in programming. In this lesson, you'll learn about bubble sort, a simple sorting algorithm that compares two adjacent elements and swaps their positions if they are out of order.

Key Terms
Bubble sort algorithm
Also called sinking sort, a comparison sorting algorithm that repeatedly steps through the elements in an array, compares two adjacent elements, and swaps them if they are in the wrong order
Understanding bubble sort
The video below provides a brief introduction to bubble sort. Start by watching the video, and then read through the rest of the lesson and complete any practice tasks.


Bubble sort (or sinking sort) is a simple comparison sorting algorithm. It repeatedly steps through the elements in an array, compares two adjacent elements, and swaps them if they are in the wrong order. The pass through the array repeats until the array is sorted. The algorithm is named for the way that smaller or larger elements "bubble" to the top of the array.

The idea behind bubble sort is that, after each pass through the array, the elements furthest to the right are ordered correctly. To see how this works, consider how a bubble sort algorithm sorts the array [7, 5, 9, 3, 1]:

The first pass through the array will start by comparing the first pair of values, 7 and 5. 5 is smaller than 7, so the algorithm will swap the two values.

Then it will move on to compare the next pair of values, 7 and 9. 9 is greater than 5, so it moves on to the next pair, 9 and 3.

3 is smaller than 9, so the algorithm will swap the two values.

It then compares the next pair, 9 and 1. 1 is smaller than 9, so the algorithm will swap the two values.

After the first pass through the array, the elements are in the following order: [5, 7, 3, 1, 9]. 9 is in the correct position.

Now, the algorithm needs to iterate through the array again because it wasn't in order. The algorithm will continue to iterate through the entire array over and over, swapping elements until it can make one full pass through the array without swapping any elements. Only then is the array sorted.

Here's a visualization of this process:

Visualization of bubble sort process.
Bubble sort in JavaScript
Just like other algorithms, you will separate the comparison logic from the sorting algorithm. To accomplish this, the caller will supply a compare function that tells bubble sort whether or not two adjacent elements are in order.

Watch the following video for an overview of this topic. Then practice applying your new bubble sort skills by completing the practice task below.


Do this
Implement a bubble sort
Now you will implement a bubble sort algorithm as a higher-order function that takes the following two parameters:

compare: A function that compares two elements. It takes two parameters: left and right. It returns one of the following:

0 if the left element is equal to the right element

A positive value if the left element is greater than the right element by the ordering criterion

A negative value if the left element is less than the right element by the ordering criterion

elements: The array to which the bubble sort algorithm is applied.

Next, create a file named bubbleSort.js. In bubbleSort.js, add the following code:

function bubbleSort(compare, elements) {
  if (Array.isArray(elements)) {
    let inOrder;

    do {
      inOrder = true; // Assume that the array is in order

      for (
        let index = 0, length = elements.length - 1;
        index < length;
        index++
      ) {
        const leftElement = elements[index];
        const rightElement = elements[index + 1];

        if (compare(leftElement, rightElement) > 0) {
          elements[index] = rightElement;
          elements[index + 1] = leftElement;
          inOrder = false; // The array wasn't in order, so swap elements and then check it again.
        }
      }
    } while (inOrder === false);
  }
  return elements;
}

module.exports = bubbleSort;
Now that you have an implementation of bubbleSort(), you will use it to sort an array.

Next, create a new file named useBubbleSort.js. In useBubbleSort.js, add the following code:

const sort = require("./bubbleSort");

const elements = [4685, 471, 880, 808];

function compare(left, right) {
  console.log("compare", left, "to", right);
  return left - right;
}

console.log(sort(compare, elements));
Then run the code, using the node useBubbleSort.js command. You will see the following:

compare 4685 to 471
compare 4685 to 880
compare 4685 to 808
compare 471 to 880
compare 880 to 808
compare 880 to 4685
compare 471 to 808
compare 808 to 880
compare 880 to 4685
[ 471, 808, 880, 4685 ]
As you can see in the output above, you can use the compare function passed into bubbleSort() to observe exactly what the algorithm is doing—without changing the code in bubbleSort(). This can be very helpful when debugging the sort algorithm.

In this case, bubble sort had to do 9 comparisons to sort 4 numbers. It might take 20 comparisons to sort 5 numbers, and so on. In the worst case, the number of comparisons goes up polynomially to the number of elements in the array.

The efficiency of bubble sort
Bubble sort is the classic "terrible" sorting algorithm. This algorithm isn't suitable for large datasets because its average-case complexity is O(n²). This simple algorithm doesn't perform well in real-world use; it's used primarily as an educational tool to introduce the concept of sorting.

The best-case time complexity of bubble sort is O(n). This happens when the list is already sorted in ascending order. In this case, none of the elements are moved but the algorithm still makes n comparisons making the time complexity O(n).

Description Notation Explanation
Worst case O(n²) Every element is out of order.
Average case O(n²) Some elements are out of order.
Best case O(n) The list is already sorted.

*** 42.6 Merge sort

Merge sort
1.5 hoursEstimated completion time
Learning Objective
By the end of this lesson, you will be able to implement merge sort and give the runtime.

Overview
Merge sort takes a divide-and-conquer approach to sorting. As you'll see in this lesson, it breaks down the array into continually smaller chunks, then merges them back together in the correct order.

Key Terms
Merge sort algorithm
A divide-and-conquer algorithm that continuously splits arrays in half until every element is alone in its own array, then merges each subarray in order
Understanding merge sort
The video below provides a brief introduction to merge sort. Start by watching the video, and then read through the rest of the lesson and complete the practice exercises. This will give you a thorough understanding of this topic.


Similar to binary search, merge sort is a divide-and-conquer algorithm. The goal is to break down the problem into subproblems until you have a lot of simple problems that can be easily put back together.

Merge sort starts by taking the entire array and breaking it into many subarrays. To do this, it continuously splits every array in half, until every element is alone in its own array. Then, during the return phase, each subarray is merged in order. Merging two single-element arrays in order is rather easy: look at the first element of each array, select the smaller element, and then add the other element.

This merge technique works for sorted arrays of any length: look at the first element of each array, slice off the smaller of the two elements, and repeat as long as there are elements in both arrays. Then add on any left-over elements.

For example, take the following array: [5, 2, 8, 15, 3]. This array gets divided and merged as follows:

Diagram showing how array gets divided and merged.
Once merge sort breaks down part of the array into single elements, it starts merging them. Because both arrays are already sorted, it can easily compare which number in each is smaller and put both in the right place.

Thus, in the final merge of the [2, 5] and [3, 8, 15] arrays, the algorithm compares the first element of each array (2 versus 3), and slices off the smaller element, 2. It then compares the first element of the remaining arrays, which is 5 versus 3 now, and slices off 3. Then it compares 5 and 8, and slices off 8. It continues until each element is in the correct order in the new array.

Merge sort in JavaScript
Just like other algorithms, you will separate the comparison logic from the sorting algorithm. To accomplish this, the caller will supply a compare function that tells merge sort whether or not two adjacent elements are in order.

Do this
Implement merge sort
Now you will implement a merge sort algorithm as a higher-order function that takes the following two parameters:

compare: A function that compares two elements, so it takes two parameters: left and right. It returns one of the following:

0 if the left element is equal to the right element

A positive value if the left element is greater than the right element by the ordering criterion

A negative value if the left element is less than the right element by the ordering criterion

elements: The array to which the merge sort algorithm is applied.

Next, create a file named mergeSort.js. In mergeSort.js, add the following code:

function mergeSort(compare, elements) {
  if (Array.isArray(elements)) {
    if (elements.length <= 1) {
      return elements;
    }

    const middle = Math.floor(elements.length / 2);

    const leftElements = elements.slice(0, middle);
    const rightElements = elements.slice(middle);

    const leftElementsSorted = mergeSort(compare, leftElements);
    const rightElementsSorted = mergeSort(compare, rightElements);

    return merge(compare, leftElementsSorted, rightElementsSorted);
  }
  return elements;
}

/**
 * Merges two sorted arrays
 *
 * @param compare
 *  Function to compare elements of the array
 * @param left
 *  The left sorted array
 * @param right
 *  The right sorted array
 * @returns {*[]}
 *  The left and right sorted arrays merged in sorted order
 */

function merge(compare, left, right) {
  const sorted = [];
  let leftIndex = 0;
  let rightIndex = 0;

  while (leftIndex < left.length && rightIndex < right.length) {
    const comparison = compare(left[leftIndex], right[rightIndex]);

    if (comparison < 0) {
      sorted.push(left[leftIndex]);
      leftIndex++;
    } else {
      sorted.push(right[rightIndex]);
      rightIndex++;
    }
  }

  return sorted.concat(
    leftIndex < left.length ? left.slice(leftIndex) : right.slice(rightIndex)
  );
}

module.exports = mergeSort;
This is how the above mergeSort() function works:

The first step is to handle the recursive base case, which is when the array has one or fewer elements. In this case, just return the array.

Next, calculate the value of middle. There may be an odd number of elements, so use Math.floor() to round down to the nearest integer. Now, middle points to the middle of the array.

Next, create two new arrays from the original array, splitting the arrays at index middle.

Then, make two recursive calls to mergeSort(). Each recursive call is passed one of the new arrays.

Finally, when the recursive calls return, merge the two sorted arrays by calling the merge() function. Within the merge(), you initialize an empty array, sorted, to hold the final result. You also initialize two pointers, leftIndex and rightIndex, which point at the first element in the left and right arrays respectively. During each iteration of the loop, the elements at leftIndex and rightIndex are compared; the smaller of the two is pushed into the sorted array, and its corresponding pointer index is incremented by one. This loop continues until one of the pointers exceeds the length of the array. At the end, any remaining elements are concatenated with the sorted array.

Now that you have an implementation of mergeSort(), you will use it to sort an array.

Next, create a new file named usemergeSort.js. In usemergeSort.js, add the following code:

const sort = require("./mergeSort");

const elements = [260, 926, 954, 208, 669, 183];

function compare(left, right) {
  console.log("compare", left, "to", right);
  return left - right;
}

console.log(sort(compare, elements));
Then run the code, using the command node usemergeSort.js. You will see the following:

compare 926 to 954
compare 260 to 926
compare 669 to 183
compare 208 to 183
compare 208 to 669
compare 260 to 183
compare 260 to 208
compare 260 to 669
compare 926 to 669
[ 183, 208, 260, 669, 926, 954 ]
As you can see in the output above, you can use the compare function passed into mergeSort() to observe exactly what the algorithm is doing without changing the code in mergeSort; this can be very helpful when debugging the sort algorithm.

In this case, merge sort had to make 9 comparisons to sort 6 numbers. It would take 11 comparisons to sort 7 numbers, and so on. In the worst case, the number of comparisons goes up logarithmically to the number of elements in the array.

The efficiency of merge sort
Merge sort is the best sorting algorithm that you have learned so far, and it is significantly better than bubble sort's O(n²). Keep in mind that it also works better with larger amounts of data.

In merge sort, the time complexity of the divide step is O(log n) because the subarray split is done evenly. The merge step takes O(n) time per level, making merge sort's average-case and worst-case time complexity O(n log n).

Description Notation Explanation
Worst case O(n log n) Every element is out of order.
Average case O(n log n) Some elements are out of order.
Best case O(n log n) All the elements in the first array are either smaller or larger than all the elements in the second array.
Note: Some implementations of merge sort can have a best-case time complexity of O(n).

*** 42.7 Quicksort

Quicksort
1.5 hoursEstimated completion time
Learning Objective
By the end of this lesson, you will be able to implement quicksort and give the runtime.

Overview
Like merge sort, quicksort takes a divide-and-conquer approach to sorting. Quicksort can sort an array in place, so in many circumstances it's faster than other common sorting algorithms.

Key Terms
Quicksort algorithm
Also called a partition exchange sort, an algorithm that works by partitioning an array into two parts, then sorting the parts independently
Understanding quicksort
The video below provides a brief introduction to quicksort. Start by watching the video, and then read through the rest of the lesson and complete the practice exercises. This will give you a thorough understanding of this topic.


Quicksort works by partitioning an array into two parts, then sorting the parts independently. It is one of the most efficient sorting algorithms, and it's based on splitting (partitioning) an array into smaller ones and swapping (exchanging) based on the pivot element selected. Because of this, quicksort is also called a partition exchange sort.

Here's the idea behind quicksort: for each pass through the array, the algorithm selects a pivot value and places the pivot into the correct location in the array. Specifically, the elements on the left side of the pivot are smaller than the pivot, and the elements on the right side of the pivot are larger than the pivot. Then the two sides are partitioned again and again until all elements are in the correct order.

In the partition phase, the algorithm compares each element to the pivot. If the value is greater than the pivot, it doesn't make any changes to the array. If the value is less than the pivot, the algorithm swaps the current element with the previous element that was greater than the pivot.

The partition function will have the following signature:

function partition (compare, elements, lowerIndex, upperIndex) {
...
}
compare is a function that returns a value greater than 0 if the pivot is greater than the current element by the ordering criterion.

elements is the array of data being sorted.

lowerIndex is the first index being sorted.

upperIndex is the last index being sorted.

Quicksort walk-through
The partition process can be challenging to understand from just a verbal summary. So to better understand the approach, take a look below at a more detailed example.

Given the array [50, 23, 9, 18, 61, 32], you will use quicksort to sort the array.

Visualization of quicksort process.
The first step is to pick a pivot. The pivot can be any element of the array, and it doesn't have to be the middle of the array. In fact, some partition algorithms select a random pivot. To keep things simple, here you will always select the last element in the array (32, in this case) as the pivot.

Next, partition the array on the basis of the pivot value. Partitioning rearranges the array in such a way that the pivot (32) eventually comes to its correct position of the sorted array. This means that any elements to the left of the pivot are less than the pivot, and any elements to the right are greater than the pivot. During this process, there is a partitionIndex variable that starts at lowerIndex. This variable is used to keep track of the index that will eventually be swapped with the pivot.

For the array given above, this process goes through the following steps:

Start from the first element and compare it with the pivot. Because 50 is greater than 32, you don't make any changes. Move on to the next element.

Compare the next element with the pivot. Because 23 is less than 32, you swap 50 and 23. Then increment partitionIndex by 1. The array becomes [23, 50, 9, 18, 61, 32].

Compare the next element with the pivot. 9 is again less than 32, so you swap 50 and 9. Then increment partitionIndex by 1. The array becomes [23, 9, 50, 18, 61, 32].

Compare the next element with the pivot. 18 is less than 32, so you swap 50 and 18. Then increment partitionIndex by 1. The array becomes [23, 9, 18, 50, 61, 32].

Compare the next element with the pivot. 61 is greater than 32, so there are no changes.

Next, swap the pivot elements[5] with elements[partitionIndex] (partitionIndex === 3) so that 32 is in the correct position. The array becomes [23, 9, 18, 32, 61, 50]. At this point, 32 is in the correct position in the array because all elements to the left of the pivot are less than 32, and all of the elements to the right are greater than 32.

Return partitionIndex (3) so that it can be used in the recursive calls. Here, index (3) can be ignored in future calls because it is in the correct position in the array.

Next, recursively call quickSort() with indexes 0 and partitionIndex - 1 and then partitionIndex+1 through 5.

Partition 0 through partitionIndex-1 (2). The new pivot is 18 because it is the element at index 2 in [23, 9, 18, 32, 61, 50].

Compare 18 to 23: 23 is greater than 18, so no changes are made.

Compare 18 to 9: 9 is smaller than 18, so 9 and 23 are swapped. The array becomes [9, 23, 18, 32, 61, 50]. Then increment partitionIndex by 1.

Next, swap the pivot elements[2] with elements[partitionIndex] (partitionIndex === 1). The array becomes [9, 18, 23, 32, 61, 50]. Now both 18 and 32 are in the correct position in the array, as are 9 and 23.

Partition partitionIndex+1 (4) through 5. The new pivot is 50 because it is the element at index 5 in [9, 18, 23, 32, 61, 50].

Compare 61 to 50: Because 61 is greater than 50, no change is made.

Next, swap the pivot elements[5] with elements[partitionIndex] (partitionIndex === 4). The array becomes [9, 18, 23, 32, 50, 61].

Quicksort in JavaScript
As with other algorithms, you will separate the comparison logic from the sorting algorithm. To accomplish this, the caller will supply a compare function that tells quicksort whether or not the pivot is greater than, less than, or equal to a given element.

Watch the following video for an overview of this topic. Then practice applying your new quicksort skills by completing the practice task below.


Do this
Implement quicksort
Now you will implement a quicksort algorithm as a higher-order function that takes the following two parameters:

compare: A function that compares two elements, so it takes two parameters: left and right. It returns one of the following:

0 if the left element is equal to the right element

A positive value if the left element is greater than the right element by the ordering criterion

A negative value if the left element is less than the right element by the ordering criterion

elements: The array to which the quicksort algorithm is applied.

Next, create a file named quickSort.js. In quickSort.js, add the following:

function quickSort(
  compare,
  elements = [],
  lowerIndex = 0,
  upperIndex = elements.length - 1
) {
  if (lowerIndex < upperIndex) {
    const index = partition(compare, elements, lowerIndex, upperIndex);
    quickSort(compare, elements, lowerIndex, index - 1);
    quickSort(compare, elements, index + 1, upperIndex);
  }
  return elements;
}

function partition(compare, elements, lowerIndex, upperIndex) {
  const pivotValue = elements[upperIndex];
  let partitionIndex = lowerIndex;

  for (let index = lowerIndex; index < upperIndex; index++) {
    const comparison = compare(pivotValue, elements[index]);

    if (comparison > 0) {
      if (partitionIndex !== index) {
        [elements[index], elements[partitionIndex]] = [
          elements[partitionIndex],
          elements[index],
        ];
      }
      partitionIndex++;
    }
  }

  [elements[partitionIndex], elements[upperIndex]] = [
    elements[upperIndex],
    elements[partitionIndex],
  ];
  return partitionIndex;
}

module.exports = quickSort;
The above quickSort() function starts by handling the recursive base case, which is when lowerIndex >= upperIndex. In this case, it just returns the array. Next, it calls partition() to update the array so that the first pivot is in the correct location and it returns the index of the pivot. Then, it recursively calls quickSort() with the indexes above and below the previous pivot index. This recursion continues until the base case is reached. Note that no new arrays are created, and only four variables are needed for additional storage.

Now, create a new file named useQuickSort.js. In useQuickSort.js, add the following code:

const sort = require("./quickSort");

function compare(left, right) {
  console.log("compare", left, right);
  return left - right;
}

console.log(sort(compare, [50, 23, 9, 18, 61, 32]));
Then run the code, using the command node useQuickSort.js. You'll see the following:

compare 32 50
compare 32 23
compare 32 9
compare 32 18
compare 32 61
compare 18 23
compare 18 9
compare 50 61
[ 9, 18, 23, 32, 50, 61 ]
As you can see in the output above, you can use the compare function passed into quickSort() to observe exactly what the algorithm is doing, without changing the code in quickSort(). This can be very helpful when debugging the sort algorithm.

In this case, quickSort() had to do eight comparisons to sort six numbers. In the worst case, the number of comparisons goes up exponentially to the number of elements in the array.

The efficiency of quicksort
Quicksort is very efficient at sorting smaller sets of data. But keep in mind that with very large datasets that won't fit in your computer's memory, merge sort works better than quicksort.

Quicksort's average-case and best-case time complexity is O(n log n). This occurs when the array is rearranged into two same-length subarrays in every partition step (similar to merge sort).

Description Notation Explanation
Worst case O(n²) The pivot element lies in an extreme end of the sorted array. One subarray is always empty, and another subarray contains n-1 elements.
Average case O(n log n) The depth of the recursion is O(log n). At each level, the partitions do O(n) operations. O(log n) times O(n) is equal to O(n log n)
Best case O(n log n) The pivot happens to be in the middle.
Note: Some implementations of quicksort can have a best-case time complexity of O(n).
*** 42.8 Comparison of sorting algorithms
Comparison of sorting algorithms
1.5 hoursEstimated completion time
Learning Objective
By the end of this lesson, you will be able to compare different sorting algorithms based on runtime, space complexity, and stability.

Overview
As you've learned in this module, sorting algorithms play an important role in data management. Often, the best course of action is to use the built-in sort function. But for situations when the built-in sort function is insufficient, there are a variety of specialized sorting algorithms to choose from. Each algorithm has its own advantages and disadvantages, and you should choose the sorting algorithm that is best suited for your particular context.

Key Terms
Runtime complexity
The algorithmic efficiency as the size of the input grows larger
Space complexity
The storage space that the algorithm requires
Stability
Whether or not a sort preserves the order of elements with equal values
Criteria for comparing sorting algorithms
When you're choosing which sorting algorithm to use, consider three main criteria: runtime complexity, space complexity, and stability.

Runtime complexity refers to the algorithmic efficiency as the size of the input grows larger. This usually depends on the number of elements to sort. You've learned about three common sorting algorithms in this module, and they all have a runtime complexity of either O(n²) or O(n log n).

Note: In this module, you learned about comparison sorts. However, not every sorting algorithm is based on comparing the items being sorted. There are even a few special-case noncomparison sort algorithms that can sort some datasets faster than O(n log n).

Even algorithms that have the same runtime complexity don't necessarily have the same speed for a particular input. It's important to consider the average-case, best-case, and worst-case efficiency for each algorithm. Some algorithms, such as quicksort, perform exceptionally well for some inputs, but are very slow for others. And other algorithms, such as merge sort, have the same runtime regardless of the input data's order.

Space complexity involves the storage space that the algorithm requires. Can the array be sorted in place, without requiring additional memory for more than a few variables, or does it need additional storage space? As you'll see, some algorithms never require extra space, but other algorithms are easier to understand when they're implemented with extra space.

Stability refers to whether or not the sort preserves the order of elements with equal values. Stability isn't an issue if all values are different; to check for stability, focus on elements with equal values. If these elements appear in the same order both in the sorted output and in the input array to be sorted, then the algorithm is stable. Most simple sorts are stable, but some sorts are not.

If you have an unstable sorting algorithm, you can make it stable by changing the comparison operation so that it considers position as a factor for two equal values.

Comparison chart
The following chart compares several sorting algorithms on the criteria outlined above. You can use this chart as a rough guide to help you pick between sorts of the same efficiency. Note that this chart lists more sorting algorithms than were covered in this module, but it isn't an exhaustive list of all existing sorting algorithms.

Sort
Average time
Best time
Worst time
Space
Stability
Remarks
Bubble
O(n²)
O(n)
O(n²)
Constant
Stable
Stops after reaching a sorted array.
Selection
O(n²)
O(n²)
O(n²)
Constant
Stable
Even a perfectly sorted input requires scanning the entire array.
Insertion
O(n²)
O(n)
O(n²)
Constant
Stable
In the best case (already sorted), every insert requires constant time.
Heap
O(n log n)
O(n log n)
O(n log n)
Constant
Not stable
By using an input array as storage for the heap, it is possible to achieve constant space.
Merge
O(n log n)
O(n log n)
O(n log n)
Depends
Stable
On arrays, merge sort requires O(n) space. But on linked lists, merge sort requires constant space.
Quick
O(n log n)
O(n log n)
O(n²)
Constant
Not stable
To avoid worst-case scenarios, it can help to pick a pivot value at random or shuffle the array before running the quicksort algorithm.
The ideal sorting algorithm would have the following properties:

Stable, meaning that elements with equal values aren't reordered.

Operates in place, requiring O(1) extra space

O(n log n) comparisons in the worst case

O(n) swaps in the worst case

Adaptive, with speeds up to O(n) when data is nearly sorted or when there are few unique values.

No algorithm has all of the above properties, so the choice of sorting algorithm depends heavily on the details of the application and the algorithm implementation. For example, asymptotic complexity analysis (big O) lets you distinguish between O(n²) and O(n log n) algorithms, but it doesn't help you distinguish between algorithms with the same asymptotic complexity. Asymptotic analysis also doesn't say anything about which algorithm is best for sorting small lists. For answers to these questions, you must test and compare the different algorithms using real application data.

*** 42.9 Assessment: Searching and sorting
** Module 43 - Data Structures
*** 43.1 Overview: Data structures
*** 43.2 Arrays
Arrays
1.5 hoursEstimated completion time
Learning Objective
By the end of this lesson, you will be able to define and describe arrays. You'll also be able to identify the running time of common array methods.

Overview
Arrays are probably the most common data structure in use, and you've already used them many times in your programming journey.

Key Terms
Data structure
A way to organize data in a computer's memory, facilitating efficient access to and modification of the data
Data structures
A data structure is a way to organize data in a computer's memory, facilitating efficient access to and modification of the data.

Imagine for a moment that you are responsible for a very large warehouse. Suppose that when new products arrive, you simply place them in some unused place on the floor of the warehouse, without any particular organization. Then, imagine that someone eventually asks how many packets of blueberries are in stock. You would find yourself searching the warehouse from top to bottom to answer the question.

But there are various ways to organize the products in the warehouse. You can install shelves and organize the products by name, type, category, or even how soon the product will be needed. Or you can get bins and place products in various bins.

Each decision that you make changes the way that you put new products into the warehouse, or how long it takes you to find a product, or what happens when you remove a product from the warehouse.

Similarly, if you had a large quantity of data to store in the memory of a computer, like a list of products that a customer wants to buy, you would want to organize that data by the way in which you plan to access and use the data. The data structures that you are about to study will provide you with these kinds of options.

Understanding arrays
An array is an object that contains a sequence of elements numbered 0, 1, 2, ... . The numbers are called index numbers. Elements of an array can be accessed by their index number.

In the memory of the computer, the elements of the array are stored next to each other. You can imagine the computer's memory as being made up of a large sequence of small boxes, where each box can store exactly one value. Each box is numbered for convenience. When you declare an array, you reserve a number of those boxes for your data.

For example, the following snippet initializes an array with some values.

const names = [
  "Aegnor",
  "Elenwe",
  "Cirdan",
  "Luthien",
  "Amerie",
  "Galion",
  "Daeron",
  "Orophin",
];
Eight memory locations must be reserved to store these values.

Diagram showing the name values stored in eight reserved memory locations.
These locations are numbered from 0 through 7. To access a specific value in the array, you can use the index of that element.

names[0]; // Refers to "Aegnor"
names[2]; // Refers to "Cirdan"
The array reference—names, in this example—points to the location of the first element of the array. That is, the first element of the array is in some arbitrary box in the memory of the computer, and the variable names refers to that box. All the elements of the array may then be found by adding their respective index to that first location.

Because the elements of the array can be accessed directly by their index, it is said that an array is a random-access structure. If the index of an element is known, then accessing that element takes O(1) time.

Common methods
The push() method
The push() method of the array adds one or more elements to the end of the array. Because you have direct access to the end of the array in constant time, and you are simply inserting a new element without touching the other elements, this operation can be done in O(1) time.

names[0]; // Refers to "Aegnor"
names[2]; // Refers to "Cirdan"
Example of push() method adding one element to end of array.
The pop() method
Similarly, the pop() method removes an element from the end of the array without touching the other elements of the array. For this reason, it can be done in O(1) time.

person = names.pop();
Example of pop() method removing one element from end of array.
The unshift() method
The unshift() method adds one or more elements to the beginning of an array. Unlike the push() method, this does affect every other element in the array. Remember that the first element of the array is found in a specific memory location, and all other elements of the array are found in the locations that come immediately after that first one.

To make room for this new element at the beginning of the array, every other element in the array needs to be moved.

names.unshift("Glorfindel");
Example of unshift() method adding one element to beginning of array.
Moving each element takes O(n) time.

The shift() method
The shift() method removes the first element of the array. And similarly to the unshift(), it needs to move each element over by one. This once again results in a running time of O(n).

Example of shift() method removing one element from beginning of array.
The splice() method
The splice() method is used to delete or insert elements at arbitrary positions in an array. Inserting an element at some point in the array requires moving all elements following that position in order to make room for the new element.

Similarly, removing an element from some position in the array requires moving the elements that follow to fill the gap.

The worst-case running time occurs when the element removed is the first element of the array or when an element is inserted in the first position in the array. This running time is O(n).

The map(), filter(), and reduce() methods
You have already used various array iteration methods, such as map(), filter(), and reduce(), as well as several others. In each case, these methods perform some operation for each element of the array. Despite the conciseness of these methods, the running time remains O(n).

The efficiency of array methods
Arrays are most efficient when accessing the elements in random order or modifying only at the end of the array. Any modification to the middle of the array results in O(n) running time.

Description
Notation
Explanation
push()
O(1)
Inserting at the end of the array
pop()
O(1)
Removing an element from the end of the array
unshift()
O(n)
Inserting at the beginning of the array
splice()
O(n)
Inserting or deleting at some arbitrary position of the array
map(), filter(), reduce()
O(n)
Requires iterating over all of the array
Arrays
1.5 hoursEstimated completion time
Learning Objective
By the end of this lesson, you will be able to define and describe arrays. You'll also be able to identify the running time of common array methods.

Overview
Arrays are probably the most common data structure in use, and you've already used them many times in your programming journey.

Key Terms
Data structure
A way to organize data in a computer's memory, facilitating efficient access to and modification of the data
Data structures
A data structure is a way to organize data in a computer's memory, facilitating efficient access to and modification of the data.

Imagine for a moment that you are responsible for a very large warehouse. Suppose that when new products arrive, you simply place them in some unused place on the floor of the warehouse, without any particular organization. Then, imagine that someone eventually asks how many packets of blueberries are in stock. You would find yourself searching the warehouse from top to bottom to answer the question.

But there are various ways to organize the products in the warehouse. You can install shelves and organize the products by name, type, category, or even how soon the product will be needed. Or you can get bins and place products in various bins.

Each decision that you make changes the way that you put new products into the warehouse, or how long it takes you to find a product, or what happens when you remove a product from the warehouse.

Similarly, if you had a large quantity of data to store in the memory of a computer, like a list of products that a customer wants to buy, you would want to organize that data by the way in which you plan to access and use the data. The data structures that you are about to study will provide you with these kinds of options.

Understanding arrays
An array is an object that contains a sequence of elements numbered 0, 1, 2, ... . The numbers are called index numbers. Elements of an array can be accessed by their index number.

In the memory of the computer, the elements of the array are stored next to each other. You can imagine the computer's memory as being made up of a large sequence of small boxes, where each box can store exactly one value. Each box is numbered for convenience. When you declare an array, you reserve a number of those boxes for your data.

For example, the following snippet initializes an array with some values.

const names = [
  "Aegnor",
  "Elenwe",
  "Cirdan",
  "Luthien",
  "Amerie",
  "Galion",
  "Daeron",
  "Orophin",
];
Eight memory locations must be reserved to store these values.

Diagram showing the name values stored in eight reserved memory locations.
These locations are numbered from 0 through 7. To access a specific value in the array, you can use the index of that element.

names[0]; // Refers to "Aegnor"
names[2]; // Refers to "Cirdan"
The array reference—names, in this example—points to the location of the first element of the array. That is, the first element of the array is in some arbitrary box in the memory of the computer, and the variable names refers to that box. All the elements of the array may then be found by adding their respective index to that first location.

Because the elements of the array can be accessed directly by their index, it is said that an array is a random-access structure. If the index of an element is known, then accessing that element takes O(1) time.

Common methods
The push() method
The push() method of the array adds one or more elements to the end of the array. Because you have direct access to the end of the array in constant time, and you are simply inserting a new element without touching the other elements, this operation can be done in O(1) time.

names[0]; // Refers to "Aegnor"
names[2]; // Refers to "Cirdan"
Example of push() method adding one element to end of array.
The pop() method
Similarly, the pop() method removes an element from the end of the array without touching the other elements of the array. For this reason, it can be done in O(1) time.

person = names.pop();
Example of pop() method removing one element from end of array.
The unshift() method
The unshift() method adds one or more elements to the beginning of an array. Unlike the push() method, this does affect every other element in the array. Remember that the first element of the array is found in a specific memory location, and all other elements of the array are found in the locations that come immediately after that first one.

To make room for this new element at the beginning of the array, every other element in the array needs to be moved.

names.unshift("Glorfindel");
Example of unshift() method adding one element to beginning of array.
Moving each element takes O(n) time.

The shift() method
The shift() method removes the first element of the array. And similarly to the unshift(), it needs to move each element over by one. This once again results in a running time of O(n).

Example of shift() method removing one element from beginning of array.
The splice() method
The splice() method is used to delete or insert elements at arbitrary positions in an array. Inserting an element at some point in the array requires moving all elements following that position in order to make room for the new element.

Similarly, removing an element from some position in the array requires moving the elements that follow to fill the gap.

The worst-case running time occurs when the element removed is the first element of the array or when an element is inserted in the first position in the array. This running time is O(n).

The map(), filter(), and reduce() methods
You have already used various array iteration methods, such as map(), filter(), and reduce(), as well as several others. In each case, these methods perform some operation for each element of the array. Despite the conciseness of these methods, the running time remains O(n).

The efficiency of array methods
Arrays are most efficient when accessing the elements in random order or modifying only at the end of the array. Any modification to the middle of the array results in O(n) running time.

Description
Notation
Explanation
push()
O(1)
Inserting at the end of the array
pop()
O(1)
Removing an element from the end of the array
unshift()
O(n)
Inserting at the beginning of the array
splice()
O(n)
Inserting or deleting at some arbitrary position of the array
map(), filter(), reduce()
O(n)
Requires iterating over all of the array
Arrays
1.5 hoursEstimated completion time
Learning Objective
By the end of this lesson, you will be able to define and describe arrays. You'll also be able to identify the running time of common array methods.

Overview
Arrays are probably the most common data structure in use, and you've already used them many times in your programming journey.

Key Terms
Data structure
A way to organize data in a computer's memory, facilitating efficient access to and modification of the data
Data structures
A data structure is a way to organize data in a computer's memory, facilitating efficient access to and modification of the data.

Imagine for a moment that you are responsible for a very large warehouse. Suppose that when new products arrive, you simply place them in some unused place on the floor of the warehouse, without any particular organization. Then, imagine that someone eventually asks how many packets of blueberries are in stock. You would find yourself searching the warehouse from top to bottom to answer the question.

But there are various ways to organize the products in the warehouse. You can install shelves and organize the products by name, type, category, or even how soon the product will be needed. Or you can get bins and place products in various bins.

Each decision that you make changes the way that you put new products into the warehouse, or how long it takes you to find a product, or what happens when you remove a product from the warehouse.

Similarly, if you had a large quantity of data to store in the memory of a computer, like a list of products that a customer wants to buy, you would want to organize that data by the way in which you plan to access and use the data. The data structures that you are about to study will provide you with these kinds of options.

Understanding arrays
An array is an object that contains a sequence of elements numbered 0, 1, 2, ... . The numbers are called index numbers. Elements of an array can be accessed by their index number.

In the memory of the computer, the elements of the array are stored next to each other. You can imagine the computer's memory as being made up of a large sequence of small boxes, where each box can store exactly one value. Each box is numbered for convenience. When you declare an array, you reserve a number of those boxes for your data.

For example, the following snippet initializes an array with some values.

const names = [
  "Aegnor",
  "Elenwe",
  "Cirdan",
  "Luthien",
  "Amerie",
  "Galion",
  "Daeron",
  "Orophin",
];
Eight memory locations must be reserved to store these values.

Diagram showing the name values stored in eight reserved memory locations.
These locations are numbered from 0 through 7. To access a specific value in the array, you can use the index of that element.

names[0]; // Refers to "Aegnor"
names[2]; // Refers to "Cirdan"
The array reference—names, in this example—points to the location of the first element of the array. That is, the first element of the array is in some arbitrary box in the memory of the computer, and the variable names refers to that box. All the elements of the array may then be found by adding their respective index to that first location.

Because the elements of the array can be accessed directly by their index, it is said that an array is a random-access structure. If the index of an element is known, then accessing that element takes O(1) time.

Common methods
The push() method
The push() method of the array adds one or more elements to the end of the array. Because you have direct access to the end of the array in constant time, and you are simply inserting a new element without touching the other elements, this operation can be done in O(1) time.

names[0]; // Refers to "Aegnor"
names[2]; // Refers to "Cirdan"
Example of push() method adding one element to end of array.
The pop() method
Similarly, the pop() method removes an element from the end of the array without touching the other elements of the array. For this reason, it can be done in O(1) time.

person = names.pop();
Example of pop() method removing one element from end of array.
The unshift() method
The unshift() method adds one or more elements to the beginning of an array. Unlike the push() method, this does affect every other element in the array. Remember that the first element of the array is found in a specific memory location, and all other elements of the array are found in the locations that come immediately after that first one.

To make room for this new element at the beginning of the array, every other element in the array needs to be moved.

names.unshift("Glorfindel");
Example of unshift() method adding one element to beginning of array.
Moving each element takes O(n) time.

The shift() method
The shift() method removes the first element of the array. And similarly to the unshift(), it needs to move each element over by one. This once again results in a running time of O(n).

Example of shift() method removing one element from beginning of array.
The splice() method
The splice() method is used to delete or insert elements at arbitrary positions in an array. Inserting an element at some point in the array requires moving all elements following that position in order to make room for the new element.

Similarly, removing an element from some position in the array requires moving the elements that follow to fill the gap.

The worst-case running time occurs when the element removed is the first element of the array or when an element is inserted in the first position in the array. This running time is O(n).

The map(), filter(), and reduce() methods
You have already used various array iteration methods, such as map(), filter(), and reduce(), as well as several others. In each case, these methods perform some operation for each element of the array. Despite the conciseness of these methods, the running time remains O(n).

The efficiency of array methods
Arrays are most efficient when accessing the elements in random order or modifying only at the end of the array. Any modification to the middle of the array results in O(n) running time.

Description
Notation
Explanation
push()
O(1)
Inserting at the end of the array
pop()
O(1)
Removing an element from the end of the array
unshift()
O(n)
Inserting at the beginning of the array
splice()
O(n)
Inserting or deleting at some arbitrary position of the array
map(), filter(), reduce()
O(n)
Requires iterating over all of the array
Arrays
1.5 hoursEstimated completion time
Learning Objective
By the end of this lesson, you will be able to define and describe arrays. You'll also be able to identify the running time of common array methods.

Overview
Arrays are probably the most common data structure in use, and you've already used them many times in your programming journey.

Key Terms
Data structure
A way to organize data in a computer's memory, facilitating efficient access to and modification of the data
Data structures
A data structure is a way to organize data in a computer's memory, facilitating efficient access to and modification of the data.

Imagine for a moment that you are responsible for a very large warehouse. Suppose that when new products arrive, you simply place them in some unused place on the floor of the warehouse, without any particular organization. Then, imagine that someone eventually asks how many packets of blueberries are in stock. You would find yourself searching the warehouse from top to bottom to answer the question.

But there are various ways to organize the products in the warehouse. You can install shelves and organize the products by name, type, category, or even how soon the product will be needed. Or you can get bins and place products in various bins.

Each decision that you make changes the way that you put new products into the warehouse, or how long it takes you to find a product, or what happens when you remove a product from the warehouse.

Similarly, if you had a large quantity of data to store in the memory of a computer, like a list of products that a customer wants to buy, you would want to organize that data by the way in which you plan to access and use the data. The data structures that you are about to study will provide you with these kinds of options.

Understanding arrays
An array is an object that contains a sequence of elements numbered 0, 1, 2, ... . The numbers are called index numbers. Elements of an array can be accessed by their index number.

In the memory of the computer, the elements of the array are stored next to each other. You can imagine the computer's memory as being made up of a large sequence of small boxes, where each box can store exactly one value. Each box is numbered for convenience. When you declare an array, you reserve a number of those boxes for your data.

For example, the following snippet initializes an array with some values.

const names = [
  "Aegnor",
  "Elenwe",
  "Cirdan",
  "Luthien",
  "Amerie",
  "Galion",
  "Daeron",
  "Orophin",
];
Eight memory locations must be reserved to store these values.

Diagram showing the name values stored in eight reserved memory locations.
These locations are numbered from 0 through 7. To access a specific value in the array, you can use the index of that element.

names[0]; // Refers to "Aegnor"
names[2]; // Refers to "Cirdan"
The array reference—names, in this example—points to the location of the first element of the array. That is, the first element of the array is in some arbitrary box in the memory of the computer, and the variable names refers to that box. All the elements of the array may then be found by adding their respective index to that first location.

Because the elements of the array can be accessed directly by their index, it is said that an array is a random-access structure. If the index of an element is known, then accessing that element takes O(1) time.

Common methods
The push() method
The push() method of the array adds one or more elements to the end of the array. Because you have direct access to the end of the array in constant time, and you are simply inserting a new element without touching the other elements, this operation can be done in O(1) time.

names[0]; // Refers to "Aegnor"
names[2]; // Refers to "Cirdan"
Example of push() method adding one element to end of array.
The pop() method
Similarly, the pop() method removes an element from the end of the array without touching the other elements of the array. For this reason, it can be done in O(1) time.

person = names.pop();
Example of pop() method removing one element from end of array.
The unshift() method
The unshift() method adds one or more elements to the beginning of an array. Unlike the push() method, this does affect every other element in the array. Remember that the first element of the array is found in a specific memory location, and all other elements of the array are found in the locations that come immediately after that first one.

To make room for this new element at the beginning of the array, every other element in the array needs to be moved.

names.unshift("Glorfindel");
Example of unshift() method adding one element to beginning of array.
Moving each element takes O(n) time.

The shift() method
The shift() method removes the first element of the array. And similarly to the unshift(), it needs to move each element over by one. This once again results in a running time of O(n).

Example of shift() method removing one element from beginning of array.
The splice() method
The splice() method is used to delete or insert elements at arbitrary positions in an array. Inserting an element at some point in the array requires moving all elements following that position in order to make room for the new element.

Similarly, removing an element from some position in the array requires moving the elements that follow to fill the gap.

The worst-case running time occurs when the element removed is the first element of the array or when an element is inserted in the first position in the array. This running time is O(n).

The map(), filter(), and reduce() methods
You have already used various array iteration methods, such as map(), filter(), and reduce(), as well as several others. In each case, these methods perform some operation for each element of the array. Despite the conciseness of these methods, the running time remains O(n).

The efficiency of array methods
Arrays are most efficient when accessing the elements in random order or modifying only at the end of the array. Any modification to the middle of the array results in O(n) running time.

Description
Notation
Explanation
push()
O(1)
Inserting at the end of the array
pop()
O(1)
Removing an element from the end of the array
unshift()
O(n)
Inserting at the beginning of the array
splice()
O(n)
Inserting or deleting at some arbitrary position of the array
map(), filter(), reduce()
O(n)
Requires iterating over all of the array
Arrays
1.5 hoursEstimated completion time
Learning Objective
By the end of this lesson, you will be able to define and describe arrays. You'll also be able to identify the running time of common array methods.

Overview
Arrays are probably the most common data structure in use, and you've already used them many times in your programming journey.

Key Terms
Data structure
A way to organize data in a computer's memory, facilitating efficient access to and modification of the data
Data structures
A data structure is a way to organize data in a computer's memory, facilitating efficient access to and modification of the data.

Imagine for a moment that you are responsible for a very large warehouse. Suppose that when new products arrive, you simply place them in some unused place on the floor of the warehouse, without any particular organization. Then, imagine that someone eventually asks how many packets of blueberries are in stock. You would find yourself searching the warehouse from top to bottom to answer the question.

But there are various ways to organize the products in the warehouse. You can install shelves and organize the products by name, type, category, or even how soon the product will be needed. Or you can get bins and place products in various bins.

Each decision that you make changes the way that you put new products into the warehouse, or how long it takes you to find a product, or what happens when you remove a product from the warehouse.

Similarly, if you had a large quantity of data to store in the memory of a computer, like a list of products that a customer wants to buy, you would want to organize that data by the way in which you plan to access and use the data. The data structures that you are about to study will provide you with these kinds of options.

Understanding arrays
An array is an object that contains a sequence of elements numbered 0, 1, 2, ... . The numbers are called index numbers. Elements of an array can be accessed by their index number.

In the memory of the computer, the elements of the array are stored next to each other. You can imagine the computer's memory as being made up of a large sequence of small boxes, where each box can store exactly one value. Each box is numbered for convenience. When you declare an array, you reserve a number of those boxes for your data.

For example, the following snippet initializes an array with some values.

const names = [
  "Aegnor",
  "Elenwe",
  "Cirdan",
  "Luthien",
  "Amerie",
  "Galion",
  "Daeron",
  "Orophin",
];
Eight memory locations must be reserved to store these values.

Diagram showing the name values stored in eight reserved memory locations.
These locations are numbered from 0 through 7. To access a specific value in the array, you can use the index of that element.

names[0]; // Refers to "Aegnor"
names[2]; // Refers to "Cirdan"
The array reference—names, in this example—points to the location of the first element of the array. That is, the first element of the array is in some arbitrary box in the memory of the computer, and the variable names refers to that box. All the elements of the array may then be found by adding their respective index to that first location.

Because the elements of the array can be accessed directly by their index, it is said that an array is a random-access structure. If the index of an element is known, then accessing that element takes O(1) time.

Common methods
The push() method
The push() method of the array adds one or more elements to the end of the array. Because you have direct access to the end of the array in constant time, and you are simply inserting a new element without touching the other elements, this operation can be done in O(1) time.

names[0]; // Refers to "Aegnor"
names[2]; // Refers to "Cirdan"
Example of push() method adding one element to end of array.
The pop() method
Similarly, the pop() method removes an element from the end of the array without touching the other elements of the array. For this reason, it can be done in O(1) time.

person = names.pop();
Example of pop() method removing one element from end of array.
The unshift() method
The unshift() method adds one or more elements to the beginning of an array. Unlike the push() method, this does affect every other element in the array. Remember that the first element of the array is found in a specific memory location, and all other elements of the array are found in the locations that come immediately after that first one.

To make room for this new element at the beginning of the array, every other element in the array needs to be moved.

names.unshift("Glorfindel");
Example of unshift() method adding one element to beginning of array.
Moving each element takes O(n) time.

The shift() method
The shift() method removes the first element of the array. And similarly to the unshift(), it needs to move each element over by one. This once again results in a running time of O(n).

Example of shift() method removing one element from beginning of array.
The splice() method
The splice() method is used to delete or insert elements at arbitrary positions in an array. Inserting an element at some point in the array requires moving all elements following that position in order to make room for the new element.

Similarly, removing an element from some position in the array requires moving the elements that follow to fill the gap.

The worst-case running time occurs when the element removed is the first element of the array or when an element is inserted in the first position in the array. This running time is O(n).

The map(), filter(), and reduce() methods
You have already used various array iteration methods, such as map(), filter(), and reduce(), as well as several others. In each case, these methods perform some operation for each element of the array. Despite the conciseness of these methods, the running time remains O(n).

The efficiency of array methods
Arrays are most efficient when accessing the elements in random order or modifying only at the end of the array. Any modification to the middle of the array results in O(n) running time.

Description
Notation
Explanation
push()
O(1)
Inserting at the end of the array
pop()
O(1)
Removing an element from the end of the array
unshift()
O(n)
Inserting at the beginning of the array
splice()
O(n)
Inserting or deleting at some arbitrary position of the array
map(), filter(), reduce()
O(n)
Requires iterating over all of the array
Arrays
1.5 hoursEstimated completion time
Learning Objective
By the end of this lesson, you will be able to define and describe arrays. You'll also be able to identify the running time of common array methods.

Overview
Arrays are probably the most common data structure in use, and you've already used them many times in your programming journey.

Key Terms
Data structure
A way to organize data in a computer's memory, facilitating efficient access to and modification of the data
Data structures
A data structure is a way to organize data in a computer's memory, facilitating efficient access to and modification of the data.

Imagine for a moment that you are responsible for a very large warehouse. Suppose that when new products arrive, you simply place them in some unused place on the floor of the warehouse, without any particular organization. Then, imagine that someone eventually asks how many packets of blueberries are in stock. You would find yourself searching the warehouse from top to bottom to answer the question.

But there are various ways to organize the products in the warehouse. You can install shelves and organize the products by name, type, category, or even how soon the product will be needed. Or you can get bins and place products in various bins.

Each decision that you make changes the way that you put new products into the warehouse, or how long it takes you to find a product, or what happens when you remove a product from the warehouse.

Similarly, if you had a large quantity of data to store in the memory of a computer, like a list of products that a customer wants to buy, you would want to organize that data by the way in which you plan to access and use the data. The data structures that you are about to study will provide you with these kinds of options.

Understanding arrays
An array is an object that contains a sequence of elements numbered 0, 1, 2, ... . The numbers are called index numbers. Elements of an array can be accessed by their index number.

In the memory of the computer, the elements of the array are stored next to each other. You can imagine the computer's memory as being made up of a large sequence of small boxes, where each box can store exactly one value. Each box is numbered for convenience. When you declare an array, you reserve a number of those boxes for your data.

For example, the following snippet initializes an array with some values.

const names = [
  "Aegnor",
  "Elenwe",
  "Cirdan",
  "Luthien",
  "Amerie",
  "Galion",
  "Daeron",
  "Orophin",
];
Eight memory locations must be reserved to store these values.

Diagram showing the name values stored in eight reserved memory locations.
These locations are numbered from 0 through 7. To access a specific value in the array, you can use the index of that element.

names[0]; // Refers to "Aegnor"
names[2]; // Refers to "Cirdan"
The array reference—names, in this example—points to the location of the first element of the array. That is, the first element of the array is in some arbitrary box in the memory of the computer, and the variable names refers to that box. All the elements of the array may then be found by adding their respective index to that first location.

Because the elements of the array can be accessed directly by their index, it is said that an array is a random-access structure. If the index of an element is known, then accessing that element takes O(1) time.

Common methods
The push() method
The push() method of the array adds one or more elements to the end of the array. Because you have direct access to the end of the array in constant time, and you are simply inserting a new element without touching the other elements, this operation can be done in O(1) time.

names[0]; // Refers to "Aegnor"
names[2]; // Refers to "Cirdan"
Example of push() method adding one element to end of array.
The pop() method
Similarly, the pop() method removes an element from the end of the array without touching the other elements of the array. For this reason, it can be done in O(1) time.

person = names.pop();
Example of pop() method removing one element from end of array.
The unshift() method
The unshift() method adds one or more elements to the beginning of an array. Unlike the push() method, this does affect every other element in the array. Remember that the first element of the array is found in a specific memory location, and all other elements of the array are found in the locations that come immediately after that first one.

To make room for this new element at the beginning of the array, every other element in the array needs to be moved.

names.unshift("Glorfindel");
Example of unshift() method adding one element to beginning of array.
Moving each element takes O(n) time.

The shift() method
The shift() method removes the first element of the array. And similarly to the unshift(), it needs to move each element over by one. This once again results in a running time of O(n).

Example of shift() method removing one element from beginning of array.
The splice() method
The splice() method is used to delete or insert elements at arbitrary positions in an array. Inserting an element at some point in the array requires moving all elements following that position in order to make room for the new element.

Similarly, removing an element from some position in the array requires moving the elements that follow to fill the gap.

The worst-case running time occurs when the element removed is the first element of the array or when an element is inserted in the first position in the array. This running time is O(n).

The map(), filter(), and reduce() methods
You have already used various array iteration methods, such as map(), filter(), and reduce(), as well as several others. In each case, these methods perform some operation for each element of the array. Despite the conciseness of these methods, the running time remains O(n).

The efficiency of array methods
Arrays are most efficient when accessing the elements in random order or modifying only at the end of the array. Any modification to the middle of the array results in O(n) running time.

Description
Notation
Explanation
push()
O(1)
Inserting at the end of the array
pop()
O(1)
Removing an element from the end of the array
unshift()
O(n)
Inserting at the beginning of the array
splice()
O(n)
Inserting or deleting at some arbitrary position of the array
map(), filter(), reduce()
O(n)
Requires iterating over all of the array
Arrays
1.5 hoursEstimated completion time
Learning Objective
By the end of this lesson, you will be able to define and describe arrays. You'll also be able to identify the running time of common array methods.

Overview
Arrays are probably the most common data structure in use, and you've already used them many times in your programming journey.

Key Terms
Data structure
A way to organize data in a computer's memory, facilitating efficient access to and modification of the data
Data structures
A data structure is a way to organize data in a computer's memory, facilitating efficient access to and modification of the data.

Imagine for a moment that you are responsible for a very large warehouse. Suppose that when new products arrive, you simply place them in some unused place on the floor of the warehouse, without any particular organization. Then, imagine that someone eventually asks how many packets of blueberries are in stock. You would find yourself searching the warehouse from top to bottom to answer the question.

But there are various ways to organize the products in the warehouse. You can install shelves and organize the products by name, type, category, or even how soon the product will be needed. Or you can get bins and place products in various bins.

Each decision that you make changes the way that you put new products into the warehouse, or how long it takes you to find a product, or what happens when you remove a product from the warehouse.

Similarly, if you had a large quantity of data to store in the memory of a computer, like a list of products that a customer wants to buy, you would want to organize that data by the way in which you plan to access and use the data. The data structures that you are about to study will provide you with these kinds of options.

Understanding arrays
An array is an object that contains a sequence of elements numbered 0, 1, 2, ... . The numbers are called index numbers. Elements of an array can be accessed by their index number.

In the memory of the computer, the elements of the array are stored next to each other. You can imagine the computer's memory as being made up of a large sequence of small boxes, where each box can store exactly one value. Each box is numbered for convenience. When you declare an array, you reserve a number of those boxes for your data.

For example, the following snippet initializes an array with some values.

const names = [
  "Aegnor",
  "Elenwe",
  "Cirdan",
  "Luthien",
  "Amerie",
  "Galion",
  "Daeron",
  "Orophin",
];
Eight memory locations must be reserved to store these values.

Diagram showing the name values stored in eight reserved memory locations.
These locations are numbered from 0 through 7. To access a specific value in the array, you can use the index of that element.

names[0]; // Refers to "Aegnor"
names[2]; // Refers to "Cirdan"
The array reference—names, in this example—points to the location of the first element of the array. That is, the first element of the array is in some arbitrary box in the memory of the computer, and the variable names refers to that box. All the elements of the array may then be found by adding their respective index to that first location.

Because the elements of the array can be accessed directly by their index, it is said that an array is a random-access structure. If the index of an element is known, then accessing that element takes O(1) time.

Common methods
The push() method
The push() method of the array adds one or more elements to the end of the array. Because you have direct access to the end of the array in constant time, and you are simply inserting a new element without touching the other elements, this operation can be done in O(1) time.

names[0]; // Refers to "Aegnor"
names[2]; // Refers to "Cirdan"
Example of push() method adding one element to end of array.
The pop() method
Similarly, the pop() method removes an element from the end of the array without touching the other elements of the array. For this reason, it can be done in O(1) time.

person = names.pop();
Example of pop() method removing one element from end of array.
The unshift() method
The unshift() method adds one or more elements to the beginning of an array. Unlike the push() method, this does affect every other element in the array. Remember that the first element of the array is found in a specific memory location, and all other elements of the array are found in the locations that come immediately after that first one.

To make room for this new element at the beginning of the array, every other element in the array needs to be moved.

names.unshift("Glorfindel");
Example of unshift() method adding one element to beginning of array.
Moving each element takes O(n) time.

The shift() method
The shift() method removes the first element of the array. And similarly to the unshift(), it needs to move each element over by one. This once again results in a running time of O(n).

Example of shift() method removing one element from beginning of array.
The splice() method
The splice() method is used to delete or insert elements at arbitrary positions in an array. Inserting an element at some point in the array requires moving all elements following that position in order to make room for the new element.

Similarly, removing an element from some position in the array requires moving the elements that follow to fill the gap.

The worst-case running time occurs when the element removed is the first element of the array or when an element is inserted in the first position in the array. This running time is O(n).

The map(), filter(), and reduce() methods
You have already used various array iteration methods, such as map(), filter(), and reduce(), as well as several others. In each case, these methods perform some operation for each element of the array. Despite the conciseness of these methods, the running time remains O(n).

The efficiency of array methods
Arrays are most efficient when accessing the elements in random order or modifying only at the end of the array. Any modification to the middle of the array results in O(n) running time.

Description Notation Explanation
push() O(1) Inserting at the end of the array
pop() O(1) Removing an element from the end of the array
unshift() O(n) Inserting at the beginning of the array
splice() O(n) Inserting or deleting at some arbitrary position of the array
map(), filter(), reduce() O(n) Requires iterating over all of the array

*** 43.3 Sets and Maps
Sets and Maps
1.5 hoursEstimated completion time
Learning Objective
By the end of this lesson, you will be able to define and use Sets and Maps and their associated methods.

Overview
A Set is a collection of unique values. A Map, on the other hand, is a collection of key-value pairs that remembers the insertion order of the keys. These two data structures provide many advantages when solving various algorithmic problems.

Understanding Sets
The following video provides a brief introduction to Sets.


The most basic collection of values is the array. It is fast and efficient. However, sometimes, you are interested in ensuring that there are no duplicate values in a collection of data. This is essentially the main advantage that a Set brings.

Key Term
Set: A collection of unique values

To create a new empty Set, invoke the Set constructor:

const numbers = new Set(); // Empty Set []
To add some values to the Set, use the add() method:

numbers.add(3); // Set contains [3]
numbers.add(7); // Set contains [3, 7]
numbers.add(5); // Set contains [3, 7, 5]
If you try to add duplicate values to the Set, it remains unchanged:

numbers.add(3); // Set still contains [3, 7, 5]
To remove an item from the Set, use the delete() method:

numbers.delete(5); // Set contains [3, 7]
Iterating the Set
Sometimes, you need to access each item of a Set. The Set provides the values() method to facilitate iterating the values of the Set.

// Find the sum of all numbers in the Set
let sum = 0;
for (let num of numbers.values()) {
  sum = sum + num;
}
console.log(sum); // 3 + 7 = 10
Alternatively, you may use the built-in forEach() method to iterate a Set. This method is very similar to the forEach() method of Array objects. Take a look:

// Find the sum of all numbers in the Set
let sum = 0;
numbers.forEach((e) => (sum = sum + e));
console.log(sum); // 3 + 7 = 10
Relationship to arrays
Sometimes, while working with a collection of values, you need the methods of an array. And at other times, you need the properties of the Set. It is easy to convert an array into a Set and a Set into an array.

const studentGrades = ["B", "F", "A", "A", "D", "B", "A", "F"];
const uniqueGrades = new Set(studentGrades); // Duplicates will be removed

for (let grade of uniqueGrades.values()) {
  console.log(grade); // 'B', 'F', 'A', 'D'
}
And if you want to create an array from the Set, you can use the spread operator.

const distinctGrades = [...uniqueGrades]; // Array contains ['B', 'F', 'A', 'D']
Checking for membership
If you need to check if a value exists in the Set, you can use the has() method.

numbers.has(3); //true
numbers.has(8); //false
uniqueGrades.has("C"); // false

Common uses of Sets
One common use of Sets is to remove duplicates from a collection of values. For example, suppose that you have a string and want to return only the unique characters from that string.

// How many unique characters are in the term 'Data Structures'?
const word = "Data Structures";

// Create a Set - lowercase the word
const set = new Set(word.toLowerCase());

// Size of the Set is the number of unique characters
console.log(set.size);
The efficiency of Sets
The JavaScript specification doesn't say how the implementation of the Set is to be done by the various JavaScript engines. But it does say that the operations on a Set must be at most O(n). There are several ways that the implementation may be carried out. These values are the expected runtime for one such implementation.

Description
Notation
Explanation
add()
O(1)
The specific implementation may differ, but you can generally expect this to be O(1).
delete()
O(1)
Also depends on the implementation details.
forEach()
O(n)
It is clear that this requires a loop through all the values of the Set.
has()
O(1)
At best, it takes a single operation to determine if a value exists or not.
Understanding Maps
The following video provides a brief introduction to Maps.


Key Term
Map: A collection of key-value pairs that remembers the insertion order of the keys

The keys in a Map are unique. Maps are used when you need a very fast lookup table.

For example, you may wish to create a table of users and their addresses. To create a new empty Map, invoke the constructor of Map:

const addressBook = new Map(); // An empty Map

To add an entry to the Map, use the set() method. The set() method takes two arguments: a key and a value.

addressBook.set("sherlock", {
  email: "holmes@scotlandyard.com",
  address: "221B Baker Street, London",
});
A key-value pair in the Map.
The key "sherlock" identifies the value.

Adding a second entry creates a new key-value pair in the Map.

addressBook.set("hercule", {
  email: "poirot@scotlandyard.com",
  address: "Flat 203 at 56B Whitehaven Mansions",
});
A second key-value pair in the Map.
Attempting to add a second entry with the same key results in replacing the original value. The keys are never duplicated.

addressBook.set("sherlock", {
  email: "jones@nypd.com",
  address: "13 5th Avenue, Queens",
});
Example showing keys cannot be duplicated.
Maps are similar to objects
You may think of a Map as similar to an object, but there are a few significant differences:

With a Map, there are no keys unless you explicitly added to the Map yourself. An object may inherit keys and cause some surprising side effects.

The keys of a Map may be of any type of value, including functions and objects. The keys in an object may only be strings and Symbol objects.

A Map remembers the insertion order of the keys. When iterating the keys, they are always in order of insertion.

You can get the number of entries in a Map by its size property:

console.log(addressBook.size); // 2
An object, on the other hand, doesn't have a size property.

Retrieving a value
To retrieve a value from the Map, use the get() method with the specific key.

const poirot = addressBook.get("hercule"); // gets {email: "poirot@scotlandyard.com", address: "Flat 203 at 56B Whitehaven Mansions"}
And to completely remove an entry from the Map, use the delete() method.

addressBook.delete("sherlock");
Iterating the Map
There are several methods for iterating a Map. You may iterate by keys, values, or entries, using the built-in keys(), values(), or entries() methods, respectively.

To print the list of usernames in the Map created above, for example, you can use a for ... of loop over the keys.

for (let key of addressBook.keys()) {
  console.log(key);
}
Or, to simply print the values in the Map, you can run the following code:

for (let value of addressBook.values()) {
  console.log(value);
}
An entry is a single key-value pair in a Map. Consider the following Map and its entries:

const months = new Map();
months.set(0, "January");
months.set(1, "February");
months.set(2, "March");
months.set(3, "April");
months.set(4, "May");
months.set(5, "June");
months.set(6, "July");
months.set(7, "August");
months.set(8, "September");
months.set(9, "October");
months.set(10, "November");
months.set(11, "December");
Using the entries() method, you can iterate over the 12 key-value pairs in that Map. Each entry is given as an array with two values. For example, the first entry is [0, "January"].

for (let [key, value] of months.entries()) {
  console.log(`${key + 1} : ${value}`); // print 1: January etc
}
Additionally, Maps have a built-in forEach() method that facilitates a similar iteration.

months.forEach((value, key) => console.log(`${key + 1} : ${value}`)); // print 1: January etc
Relationship to arrays
If you have data in the correct format, you may create a Map directly from that data. For example, an array of arrays in the form [[key1, value1], [key2, value2], ... ] may be used to create a Map.

const days = [
  [0, "Sunday"],
  [1, "Monday"],
  [2, "Tuesday"],
  [3, "Wednesday"],
  [4, "Thursday"],
  [5, "Friday"],
  [6, "Saturday"],
];

const dayMap = new Map(days);

The resulting Map has keys 0, 1, and so on, corresponding to the values "Sunday", "Monday", and so on.

Conversely, you may use the spread operator to create an array of arrays from a Map.

const dayArray = [...dayMap];
// results in:
// [
//   [0, "Sunday"],
//   [1, "Monday"],
//   [2, "Tuesday"],
//   [3, "Wednesday"],
//   [4, "Thursday"],
//   [5, "Friday"],
//   [6, "Saturday"],
// ]
The efficiency of Maps
Maps are very efficient at inserting and looking up values by keys.

Description
Notation
set()
O(1)
delete()
O(1)
get()
O(1)
has()
O(1)

*** 43.4 Solving problems with Sets, Maps, and arrays

Solving problems with Sets, Maps, and arrays
1.5 hoursEstimated completion time
Learning Objective
By the end of this lesson, you will be able to use Sets and Maps to implement solutions to simple algorithmic problems.

Overview
In this lesson, you will walk through the process of thinking about and solving several problems using the linked list data structure. You'll attempt to implement each of these problems in JavaScript.

Starter code
This lesson requires you to have the following GitHub repository running on your local machine.

Starter: Solving problems with Sets and Maps
Fork and clone the repository above. Then, follow the instructions on how to get it to run.

Now, it's time to solve a few algorithmic problems with Sets and Maps. The starter code provided contains tests that will help indicate if the implementation is correct.

Intersection of arrays
Suppose you were given two arrays, a and b, and you needed to find all elements that are present in both arrays. This is a common problem that turns up in many applications.

Here's an example:

Inputs
Output
Explanation
a = [10, 20, 30, 40, 50], b = [50, 40, 30, 20, 10]
[10, 20, 30, 40, 50]
All the elements in array a are also present in array b.
a = ['A', 'B', 'C', 'A', 'D'], b = ['A', 'A', 'E', 'C', 'D']
['A', 'C', 'D']
Even though 'A' appears twice in both arrays, it only occurs once in the output. 'B' is in the first array but not the second, and 'E' is in the second but not the first. Hence, these elements do not appear in the output.
One way to approach the problem is as follows: Iterate through one of the arrays. For each element, search for the existence of that element in the second array. You could write that approach in pseudocode as follows:

result = []
for each element e of a do:
  search for e in b
  if e is found in b
     add e to the result array if it isn't already there
return result
The search for the element in the second array is an O(n) operation which is executed once for each element in the first array. This gives a running time of roughly O(n²).

Using a Map, you may be able to improve that running time. Maps are very good at lookups. If you were to put all the elements of one array into a Map, then each lookup for that element will only take O(1) time. A second improvement is to use a Set to track the common elements, because that automatically avoids duplication.

initialize a new Map
initialize a new Set
for each element e of a do:
  add e to the Map

for each element e of b do:
  lookup e in the Map
  if e is in the Map then
    add e to the Set
convert the Set to an array and return
Overall, this second algorithm has a running time of O(n).

Do this
Implement the array intersection algorithm
Use the starter repository to implement the algorithm outlined above. The tests in the repository will help you determine if the algorithm is correct.

Find all pairs whose sum is equal to given numbers
Given an array of distinct numbers, you wish to find all pairs of numbers in the array that sum to a given number.

For example, suppose that you had the array [3, 2, 4, 6, 7, 5], and you want to find all pairs of numbers in this array that sum to the value 10. You can see that the pair [3, 7] and the pair [4, 6] both sum to 10. No other pair of numbers sum to 10. The returned value will be [[3, 7], [4, 6]].

Also note the following:

The pair [4, 6] and [6, 4] are the same and considered just one pair.

[5,5] isn't considered a pair because 5 only occurs once in the array. The array does not contain duplicates, so this would never be possible.

The brute-force approach is to iterate through each element of the array, and for each element, consider every other element of the array. If they sum to the value, then output that pair. In pseudocode, that algorithm may look like this:

for each element i in the array do:
  for each element j in the array do:
    if i is not equal to j then
       if i + j is equal to the value
          add [i, j] to the output
It should be clear that due to the nested for loops in this solution, the running time is O(n²).

Another approach to the problem is to use a Map. As was the case with the previous problem, the issue is the linear search through the array by the nested for loop. What if you can replace that loop with a lookup that takes O(1) time?

If all the elements in the array were in a Map, such a lookup would be possible. But what would you look up? If the target value is 10 and the current element that you are considering is the value 3, the only way that this value is part of a pair is if the value 10-3=7 also exists in the array. So a lookup for the value 7 tells you if the pair [3, 7] is in the solution.

inputs: an array of numbers named A
        a target value named N

initialize a new Map named numbers
initialize a new Map named solution

for each element e in A do:
   add e to numbers

for each element e in A do;
  calculate diff = N - e
  if diff is not equal to e then  (1)
     look up diff in numbers
     if diff is in numbers then
       add [e, diff] to solution (2)

convert solution to an array and return it
There are two points to notice about the above algorithm:

The check on the line marked (1) ensures that you do not end up with a number paired with itself, like [5, 5].

As the line marked (2) is written, it will probably result in both [3, 7] and [7, 3] in the solution. You may want to set the minimum of the two numbers as the key and the other number as the value in the solution Map.

This algorithm has a running time of O(n).

Do this
Implement the sum of pairs algorithm
Use the starter repository to implement the algorithm outlined above. The tests in the repository will help you determine if the algorithm is correct.

Same letters
Given a list of words, find all words made up of the same letters. Duplicates letters are okay, so tok and took are both made up of the same letters: t, o, and k.

The function should return a Map where the keys are strings made up of the letters found in a group of words, and the values are arrays consisting of the words that are made up with the same letters in the key.

For example, given the array ["pair", "per", "pole", "pear", "peer", "reap", "lope"], the result will be as follows:

{
  "aipr": ["pair"],
  "elop": ["pole", "lope"],
  "aepr": ["pear", "reap"],
  "epr": ["per", "peer"]
}
The characters in the keys should be ordered in alphabetical order, and this should be a case-insensitive search. Using Maps, Sets, and arrays will greatly simplify the approach to solving this problem.

The first problem to solve is to determine if a word belongs to a particular group or not. One option is to rearrange all the unique characters in the word into alphabetical order, and then compare it to the existing keys in the Map.

If the key exists, then at least one other word with those same letters was found, so you can add this word to the array associated with that key.

If the key isn't found, then this is the first word made up of those letters that was found. So add that key and word to the Map.

Using pseudocode, the algorithm may look like this:

Input: an array of words named words

instantiate a new Map named results

for each word in the words array do:
  lowercase word
  find all unique letters in word
    create a Set with all the letters of the word to remove duplicates
    create an array of letters from the Set and sort alphabetically
    use join to create a string from the array
  if the sorted string of letters is a key in the results Map then
    add word to the array associated with that key
  else
    make a new entry in the results Map with key = sorted string of letters and the value an array with word as its only element
return the results Map
Do this
Implement the same-letters algorithm
Use the starter repository to implement the algorithm outlined above. The tests in the repository will help you determine if the algorithm is correct.

Complete example
A completed example from this lesson can be in the Solution branch of this GitHub repository:

Data structures: Solving problems with Sets and Maps—Solution branch

*** 43.5 Defining classes
Defining classes
1.5 hoursEstimated completion time
Learning Objective
By the end of this lesson, you will be able to define a class with a constructor. You'll also be able to create an instance, access the attributes of the instance, define a class with methods, and call the methods with an instance of the class.

Overview
In this lesson, you'll learn the basics of object-oriented programming—you'll learn how to create and interact with classes. This will help you organize chunks of related code together, thus improving the readability and reusability of your code.

Key Terms
Object-oriented programming
OOP, a programming paradigm where data is stored into objects as individual fields, attributes, or parameters, which the code manipulates through different procedures and methods
Class instance
An object built to the specifications of a given class
Constructor
A special function that defines how a new instance of a class is created
Object-oriented programming (OOP) is often seen as an alternative programming style to functional programming. The OOP style is based on the use of classes. Classes are a way of creating objects; they are very useful when you know that you will create multiple objects that all have similar properties.

Key Term
Class: A blueprint for creating objects which makes it easy to create multiple objects that all have similar properties

For example, if you want to create a series of objects that hold information about the birthday parties of all your friends, creating a birthdayParty class will help you make each object using the same pattern. In this case, each birthdayParty object will have an event name, the start time, the end time, and the location.

JavaScript was designed as a functional programming language, but given the popularity of OOP, classes were introduced in ES6. Many other languages, like Java and C++, have always relied heavily on classes. Learning to write classes and design object-oriented code is an important part of being a well-rounded software developer.

The benefit of using OOP is that it will help you organize or encapsulate chunks of related code, such as data and functions, and make your code more reusable and readable overall. As you progress through the rest of the data structures and algorithms curriculum, you will follow the OOP style to implement more complex data structures.

Class instances
Classes define what properties and functionality the resulting instances will have. A class instance is an object built to the specifications of the class. Although many instances may share a class definition, each one is a unique object.

Classes are abstract creations that represent an idea. For example, an Event class doesn't represent a specific event but rather the idea of events. This is in contrast to an instance of an Event, which describes a specific event.

For example, the Event class would specify that all Event instances which follow from it have the following properties.

Name

Start time

End time

Location

Every event you create is an instance of the class. For example, an instance of the Event class could have the following values.

Name: Lunch

Start Time: 12:00

End Time: 13:00

Location: Chipotle

The table below includes some examples of a class and what an individual instance might look like.

Class name
Individual instance
BoardGame
Name: Photosynthesis; NumberOfPlayers: 2-4
Dog
Name: Rufio; Age: 8 months; Breed: Labrador
House
Bedrooms: 4; Bathrooms: 3; Type: Tudor; Location: Massachusetts
Song
TrackName: Patterns; ArtistName: Steve Gibbs; AlbumName: Adrift
Tip
As you can see in the table above, class names are conventionally written in PascalCase, with each word beginning with a capital letter.

Class syntax
You can use the keyword class to define a new class. The class definition will start like this.

class Event {
  // TODO
}
To create a new instance of the Event class, you can use the new keyword and invoke the Event class like a function.

const lunch = new Event();
Stored inside of the lunch variable is an empty object that is an instance of the Event class.

Do this
Create a new event
Try running the code above. If you run your code in a Node REPL, you should see the following as output of lunch.

Event {}
As you can see, the above is an empty object but has the word Event in front of it. This is showing you that although the object is simply an empty object, it was created from the Event class.

Constructors
You used the keyword class to define a new class. Classes may have a constructor function.

A constructor is a special function that defines how a new instance of the class is created. In the constructor, you can define the specific properties to be included in the newly created instance.

An example of a constructor function is below, although this constructor function does not set any properties for the resulting instances.

class Event {
  constructor() {
    console.log("Calling the constructor function!");
  }
}
If you were to create a new instance using the Event class, you would see the following.

new Event();
//> Calling the constructor function!
//> Event {}
Each time that you create a new instance of the class, the constructor function is called.

Great job on making it this far in the lesson! So far in this lesson, you've learned how to use classes to define objects and create object instances. Next, you will learn about the this keyword.

The this keyword
Like other functions, the constructor function is most useful when it has access to parameters that can be used to customize each instance of the class. The values passed in to the constructor function can then be assigned to keys on the this keyword.

The this keyword is used to reference the internals of a specific instance. The this keyword is used most often in class definitions to define how instances of that class should function.

Below is an example of the Event class with a constructor function that accepts four parameters (name, startTime, endTime, and location) and sets each instance's properties to the inputted values.

class Event {
  constructor(name, startTime, endTime, location) {
    this.name = name;
    this.startTime = startTime;
    this.endTime = endTime;
    this.location = location;
  }
}
You can then create a new event by running the following code:

const lunch = new Event("Lunch", "12:00", "13:00", "Chipotle");
console.log(lunch);
/*
  Event {
    name: "Lunch",
    startTime: "12:00",
    endTime: "13:00",
    location: "Chipotle"
  }
*/
Despite being an instance of the Event class, this object works just like any other. For example, you can use the dot notation to access the startTime and location properties on the lunch instance, like this:

lunch.startTime; //> "12:00"
lunch.location; //> "Chipotle"
Do this
Create another event
Try creating a new event instance using the class definition above.

const dinner = new Event("Dinner", "18:00", "19:30");
Notice that the fourth argument is missing. What's the value for dinner.location, then? Test it out for yourself and see.

Flexible classes
In the above examples, you've only used strings as values, but any value can be assigned to an individual instance of a class. It's also possible to structure your instances however you like or perform whatever operations you need inside the constructor, since it works just like a regular function except that it is a special function defined on a class.

Here's an example that demonstrates the use of conditional statements inside the constructor:

class Event {
  constructor(name, attendees, startTime, locationName, address) {
    let guests = null;
    if (attendees.length === 1) {
      guests = attendees[0];
    } else if (attendees.length > 1) {
      guests = `${attendees.length} attendees`;
    }

    this.name = guests ? `${name} with ${guests}` : name;
    this.details = { startTime, attendees, groupSize: attendees.length };
    this.location = { name: locationName, address };
  }
}

const event = new Event("Dinner", ["Sal"], 1830, "Frankie's", "477 Rain St.");
/*
  Event {
    name: 'Dinner with Sal',
    details: { startTime: 1830, attendees: [ 'Sal' ], groupSize: 1 },
    location: { name: "Frankie's", address: '477 Rain St.' }
  }
*/
Defining a method
Start with the following Event class.

class Event {
  constructor(name, startTime, endTime, location) {
    this.name = name;
    this.startTime = startTime;
    this.endTime = endTime;
    this.location = location;
  }
}
As you've seen, the constructor works just like a regular function except it is a special function defined on a class. You can add additional functions or methods to a class definition.

Now, add a toString() method to the Event class.

Key Term
Method: Any function that can be called from another object

The toString() method will put all the event info into a single string so that it can easily be displayed. A method is defined similarly to a function. You can define it right after the definition of constructor(), like this:

class Event {
  constructor(name, startTime, endTime, location) {
    // ...
  }
  toString() {
    // TODO
  }
}
Like the constructor() method, the toString() method accesses each instance using the this keyword. Here is the toString() method:

  toString() {
    const { name, startTime, endTime, location } = this;
    return `${startTime} - ${endTime}: ${name} at ${location}`;
  }
const { name, startTime, endTime, location } = this; destructures the properties from each instance into distinct variables that are then used to construct the returned output.

Calling a method
You can now create an Event class instance called lunchEvent and call the toString() method on lunchEvent, like this:

const lunchEvent = new Event("Lunch", "12:00", "13:00", "Chipotle");
console.log(lunchEvent.toString()); // > 12:00 - 13:00: Lunch at Chipotle
Do this
Use the toString() method
Try copying the code above and create a few different instances of an event. Then, use the toString() method each time. Notice how the toString() method changes depending on what values were given to the constructor function.

Defining methods with parameters
Now, define the isBefore() method, which takes another instance of the Event class and returns true if and only if the event ends before the other event starts.

This is an outline of what the Event class will look like after the isBefore() method is added:

class Event {
  constructor(name, startTime, endTime, location) {
    // ...
  }
  toString() {
    // ...
  }
  isBefore(otherEvent) {
    // ...
  }
}
Notice that the isBefore() method takes a parameter otherEvent. This will be another instance of the Event class.

Before defining isBefore(), write a couple of helper methods that will find the hours and minutes of the event, like this:

  getStartHours() {
    return Number(this.startTime.split(":")[0]);
  }
  getStartMinutes() {
    return Number(this.startTime.split(":")[1]);
  }
  getEndHours() {
    return Number(this.endTime.split(":")[0]);
  }
  getEndMinutes() {
    return Number(this.endTime.split(":")[1]);
  }
Tip
Number() is a built-in function that converts a string to the Number type.

These new methods should work as follows:

const lunchEvent = new Event("Lunch", "12:00", "13:00", "Chipotle");
console.log(lunchEvent.getStartHours()); // > 12
console.log(lunchEvent.getStartMinutes()); // > 0

console.log(lunchEvent.getEndHours()); // > 13
console.log(lunchEvent.getEndMinutes()); // > 0
Now you can use these methods to define the isBefore() method, like this:

  isBefore(other) {
    return this.getEndHours() < other.getStartHours() || (
        this.getEndHours() === other.getStartHours() &&
        this.getEndMinutes() <= other.getStartMinutes()
      );
  }
isBefore() will return true if the current event ends before or right at the start of the other event. Otherwise, it will return false.

To see how this method works, first define another instance of the Event class.

const meeting = new Event("Meeting", "14:00", "15:30", "Conference room");
Now you can use the isBefore() method.

console.log(lunchEvent.isBefore(meeting)); //> true
console.log(meeting.isBefore(lunchEvent)); // > false
console.log(lunchEvent.isBefore(lunchEvent)); // > false
As you can see from these results, the lunchEvent instance is before meeting, and meeting isn't before lunchEvent.

Now you can better appreciate how OOP can help you make your code more reusable and readable by grouping related data and functionality into objects. You will use classes to define more complex data structures, such linked lists, stacks, queues, and trees.

*** 43.6 Linked lists
 Linked lists
1.5 hoursEstimated completion time
Learning Objective
By the end of this lesson, you will be able to define and implement a linked list and give the runtime of basic operations.

Overview
Linked lists are a common type of data structure in web development. As you'll learn in this lesson, a linked list is a type of linear data structure which connects elements through a chain of references.

Key Terms
Linked list
An ordered, linear data structure in which each item contains a reference to the next item
Head
A reference to the first node in a linked list
Tail
The last node in a linked list
Singly linked list
A linked list in which each node contains exactly one reference to the next node
Doubly linked list
A linked list in which each node contains two references: a reference to the next node and a reference to the previous node
Circular linked list
A linked list in which the last node points to the first node or another node before it, thereby forming a loop
Understanding linked lists
The video below provides a brief introduction to linked lists.


A linked list is an ordered, linear data structure that's similar to an array. However, unlike in an array, elements aren't stored at a particular index; instead, they are connected through a chain of references. In a linked list, each item contains a reference to the next item. Each element (or node) is stored as a separate object that contains two properties: the data stored and a link to the next node.

Linked lists and arrays each have their own benefits and drawbacks. For example, linked lists use slightly more memory than arrays because of the storage needed for the reference to the next node. But unlike arrays, which store data contiguously in memory, linked lists can easily insert or remove nodes from the list without reorganizing all of the data. Additionally, linked lists and arrays perform tasks related to accessing, adding, and removing data at different speeds. Search operations on a linked list are slow because data can't be accessed randomly; nodes in a linked list are accessed sequentially, starting from the first node.

The entry point into a linked list is a reference to the first node in the list. This reference is called the head. The last node in a linked list points to null. So if a linked list is empty, the head is a null reference.

The diagram below shows a common linked list.

Example of a linked list, with the head pointing to three successive nodes, the last of which points to null.
The linked list depicted above can be written in JSON as follows:

{
    head: {
        value: 50
        next: {
            value: 9
            next: {
                value: 23
                next: null
            }
        }
    }
};
As you can see, a linked list contains objects nested deeply inside of one another. The next property of each node in the list is a reference to the next node.

There are three types of linked lists:

Singly linked lists: Each node contains exactly one reference to the next node. This is what you will learn about in this lesson.

Doubly linked lists: Each node contains two references: a reference to the next node and a reference to the previous node.

Circular linked lists: The last node points to the first node or another node before it, thereby forming a loop.

Linked lists in JavaScript
A singly linked list works by storing a series of nodes. Each node consists of a value and a reference to the next node in the sequence.

Do this
Implement a linked list
Now you will implement a singly linked list.

First, create a file named linkedList.js. In linkedList.js, add the following code:

/**
 * `Node` is used to store values in a linked list
 */
class Node {
  constructor(value, next = null) {
    this.value = value;
    this.next = next;
  }
}

/**
 * The `LinkedList` class holds a reference to the `head` node and has functions that update the list.
 */

class LinkedList {
  constructor() {
    this.head = null;
  }

  /**
   * The number of nodes in the linked list
   *
   * @returns {number}
   *   The number of nodes in the linked list
   */

  get length() {
    let result = 0;
    let node = this.head;

    while (node) {
      result++;
      node = node.next;
    }
    return result;
  }

  /**
   * Insert a new value at the head of the list
   * @param value
   *  The new value to insert
   *
   * @returns {LinkedList}
   *  this linked list so methods can be chained
   */
  insertAtHead(value) {
    this.head = new Node(value, this.head);
    return this;
  }
}

module.exports = LinkedList;
The above is a very simple linked list: you can only insert values in the list and then get the length of the list. You will add new functionality to this linked list later in this lesson.

The linked list has a head property to indicate the beginning of the list. The head always contains the first node. In the code above, you start with an empty list, represented by the head having a null value.

The above linked list allows you to insert new values only at the beginning of the list. Each new value becomes the head and is linked to the previous head. Note that any time that a value is added to the linked list, it creates an instance of the Node class. The Node class's constructor accepts a value variable that holds the data and a next variable that is a reference to the next node.

Next, create a new file named useLinkedList.js. In useLinkedList.js, add the following code:

const LinkedList = require("./linkedList");

const linkedList = new LinkedList();

linkedList.insertAtHead("One");
linkedList.insertAtHead(2);

console.log(linkedList);
Then run the code, using the command node useLinkedList.js. You will see the following:

LinkedList {
  head: Node { value: 2, next: Node { value: 'One', next: null } }
}
As you can see from the output above, 2 was added last, but it's the first item in the list. In other words, 2 is the head of the list.

Next, you will add a method to insert a value at the end of the list. The end of the list is often called the tail.

Insert at the end
Inserting at the end of the list is a bit more complex than inserting at the beginning of the list. Inserting at the end involves the following steps:

Create a new node with the value that you want to insert.

If the list is empty, set head to the new node.

If the list isn't empty, iterate through the list until you reach the end of the list. Then set the end node's next property to the new node.

Do this
Implement insert()
In linkedList.js, add the following code:

...
  insert(value) {
    const newNode = new Node(value)

    if (this.head) {
      let tail = this.head;
      while(tail.next){
        tail = tail.next;
      }
      tail.next = newNode;
    } else {
      this.insertAtHead(value)
    }
    return this;
  }
...
Next, update useLinkedList.js to use the new method, as follows:

const LinkedList = require("./linkedList");

const linkedList = new LinkedList();

linkedList.insert("One");
linkedList.insert(2);

console.log(linkedList);
Now when you run this code, the values are in the order of insertion. Take a look:

LinkedList {
  head: Node { value: 'One', next: Node { value: 2, next: null } }
}
Now, you can insert new values at the start or end of the linked list. Next, you will implement a find() function that will allow you to retrieve a node from the list.

The find() method
Finding values in the linked list is straightforward. You iterate through the list until a matching node is found. When the matching node is found, return the node.

Do this
Implement find()
In linkedList.js, add the following implementation of the find() function:

...
 /**
   * Find a node in the linked list.
   *
   * @param isMatch
   *  Function that returns `true` if the current node matches the search criteria
   *
   * @returns {Node|null}
   *  The first node where `isMatch(node, index) === true`,
   *  or `null` if no match is found
   */
  find(isMatch) {
    let index = 0;
    let node = this.head;
    while (node) {
      if (isMatch(node, index)) {
        return node;
      }
      index++;
      node = node.next;
    }
    return null;
  }
...
Next, call the find() function in useLinkedList.js, as follows:

const LinkedList = require("./linkedList");

const linkedList = new LinkedList();

linkedList.insert("One");
linkedList.insert(2);

// You can use `find()` to update the value of a node in the list.
linkedList.find((node) => node.value === "One").value = 1;

console.log(linkedList);

console.log(
  "find 2",
  linkedList.find((node) => node.value === 2)
);

console.log(
  "find 3",
  linkedList.find((node) => node.value === 3)
);
When you run the above code, you will see the following:

LinkedList {
  head: Node { value: 1, next: Node { value: 2, next: null } }
}
find 2 Node { value: 2, next: null }
find 3 null
Sometimes you will only have an index available to find a node. You can find a node by index as follows:

console.log(linkedList.find((node, index) => index === 1));
You may notice a pattern at this point. Both the insert() and find() functions iterate through the list starting at the head. The insert() function is finding the last node in the list. Next, you will update the insert() function to use the find() function.

In linkedList.js, replace the insert() function with the following code:

...
  insert(value, isMatch = (node, index) => index === this.length - 1) {
    if (this.head) {
      const previousNode = this.find(isMatch);

      if (!previousNode) {
        throw new Error("No match found.");
      }

      previousNode.next = new Node(value, previousNode.next);
    } else {
      this.insertAtHead(value)
    }
    return this;
  }
...
This updated insert() function uses find() to locate the last item in the list, by default. However, it also has the added benefit of allowing the caller to insert() a value after any node in the list. You can do this by passing in an isMatch() function that matches any node in the list, and the new value gets inserted after the matched node.

Next, update useLinkedList.js to insert a new value between "One" and 2, as follows:

const linkedList = new LinkedList();

linkedList.insert("One");
linkedList.insert(2);

linkedList.insert(1.5, (node) => node.value === "One");

console.log(linkedList);
When you run the above code, you will see the following:

LinkedList {
  head: Node { value: 'One', next: Node { value: 1.5, next: [Node] } }
}
As you can see, the value 1.5 was inserted after "One" rather than at the end of the list.

Now, you can insert a value anywhere into the linked list, and you can update an existing value. Next, you'll learn how to remove a value from the list.

Removing an item from the list requires finding the node before the node that you are removing, and updating its next reference to skip over the removed node.

Remove
When deleting an item, there are three cases that you need to consider. You can do the following:

Remove from the beginning of the list

Remove from the end of the list

Remove a node between two other nodes

The first item in a list is indicated by head. If you delete the first item in the list, you need to change the head to indicate the new first item on the list.

In the remaining two cases, you find the node before the node that you are removing and update its next reference to skip over the removed node. For example, if you want to delete Dragon fruit from the following list, you have to find the node that contains Dragon fruit.

Diagram showing list of fruits as nodes.
After you find the previous node (orange), set its next property to the node that is after Dragon fruit (apple).

Diagram explaining how to set the next property of the previous node (orange) to the node that is after dragon fruit node (apple).
Now you have a linked list with the item removed.

Diagram showing dragon fruit is removed from the list.
You already have a find() function that will locate any node in the list. So now, you will write a very similar function that will find any node and the previous node.

Do this
Implement findWithPrevious()
Add the following code to linkedList.js:

...

  /**
   * Find a node, and its previous node, in the linked list.
   * @param isMatch
   *  Function that returns `true` if the current node matches the search criteria
   *
   * @returns {[Node|null, Node|null]}
   *  The first element is the node where `isMatch(node, index) === true`, or `null` if no match is found.
   *  The second element is the previous Node, or `null` if no match is found.
   *  This second element is also `null` if `this.head` is the matched node.
   */
  findWithPrevious(isMatch) {
    let index = 0;
    let previous = null;
    let node = this.head;
    while (node) {
      if (isMatch(node, index)) {
        return [node, previous];
      }
      index++;
      previous = node;
      node = node.next;
    }
    return [null, null];
  }
...
Because the above findWithPrevious() function is nearly identical to find(), you will also update find() to just call findWithPrevious().

Replace the find() function in linkedList.js with the following code:

find(isMatch) {
  return this.findWithPrevious(isMatch)[0];
}
Next, add the remove() function to linkedList.js.

Do this
Implement the remove() function
Add the following to linkedList.js:

  /**
   * Remove the first node where `isMatch(node, index, this) === true`.
   *
   * @param isMatch
   *  Function that returns `true` if the current node matches the node to be removed
   *
   * @returns {*}
   *  The value of the removed node where `isMatch(node, index) === true`, or `null` if no match is found
   */

  remove(isMatch) {
    const [matchedNode, previousNode] = this.findWithPrevious(isMatch);

    if (!matchedNode) {
      return null;
    }

    if (this.head === matchedNode) {
      this.head = this.head.next;
    } else {
      previousNode.next = matchedNode.next;
    }
    return this;
  }
The following is the complete linked list implementation:

/**
 * Node is used to store values in a LinkedList
 */
class Node {
  constructor(value, next = null) {
    this.value = value;
    this.next = next;
  }
}

/**
 * LinkedList class holds a reference to the `head` node and has functions that update the list.
 */

class LinkedList {
  constructor() {
    this.head = null;
  }

  /**
   * The number of nodes in the linked list
   *
   * @returns {number}
   *   The number of nodes in the linked list
   */

  get length() {
    let result = 0;
    let node = this.head;

    while (node) {
      result++;
      node = node.next;
    }
    return result;
  }

  /**
   * Find a node in the linked list.
   *
   * @param isMatch
   *  Function that returns `true` if the current node matches the search criteria
   *
   * @returns {*|null}
   *  The first node where `isMatch(node, index) === true`, or `null` if no match is found
   */
  find(isMatch) {
    return this.findWithPrevious(isMatch)[0];
  }

  /**
   * Insert the value after a matched node in the list.
   * By default, the value is inserted at the end of the list.
   *
   * @param value
   *  The value to add
   *
   * @param isMatch
   *  Optional function that returns `true` if the current node matches the search criteria
   *
   * @returns {LinkedList}
   *  this linked list so methods can be chained
   *
   * @throws 'No match found.'
   *  If list isn't empty and no matching element is found
   */
  insert(value, isMatch = (node, index) => index === this.length - 1) {
    if (this.head) {
      const previousNode = this.find(isMatch);

      if (!previousNode) {
        throw new Error("No match found.");
      }

      previousNode.next = new Node(value, previousNode.next);
    } else {
      this.insertAtHead(value);
    }
    return this;
  }

  /**
   * Insert a new value at the head of the list.
   * @param value
   *  The new value to insert
   *
   * @returns {LinkedList}
   *  this linked list so methods can be chained
   */
  insertAtHead(value) {
    this.head = new Node(value, this.head);
    return this;
  }

  /**
   * Find a node, and its previous node, in the linked list.
   * @param isMatch
   *  Function that returns `true` if the current node matches the search criteria.
   *
   * @returns {[Node|null, Node|null]}
   *  The first element is the node where `isMatch(node, index) === true` or `null` if no match is found.
   *  The second element is the previous node, or `null` if no match is found.
   *  This second element is also `null` if `this.head` is the matched node.
   */
  findWithPrevious(isMatch) {
    let index = 0;
    let previous = null;
    let node = this.head;
    while (node) {
      if (isMatch(node, index, this)) {
        return [node, previous];
      }
      index++;
      previous = node;
      node = node.next;
    }
    return [null, null];
  }

  /**
   * Remove the first node where `isMatch(node, index, this) === true`.
   *
   * @param isMatch
   *  Function that returns `true` if the current node matches the node to be removed
   *
   * @returns {*}
   *  The value of the removed node, where `isMatch(node, index) === true`, or `null` if no match is found
   */

  remove(isMatch) {
    const [matchedNode, previousNode] = this.findWithPrevious(isMatch);

    if (!matchedNode) {
      return null;
    }

    if (this.head === matchedNode) {
      this.head = this.head.next;
    } else {
      previousNode.next = matchedNode.next;
    }
    return matchedNode.value;
  }
}

module.exports = LinkedList;
The efficiency of linked lists
Linked lists are most efficient when inserting and removing data at the beginning of the list.

Description
Notation
Explanation
insertAtHead()
O(1)
Inserting only in the first position, regardless of the length of the list
insert()
O(n)
Requires iterating over all of the nodes until you reach the end of the list
find()
O(n)
Requires iterating over all of the nodes until you find the node
remove()
O(n)
Requires iterating over all of the nodes until you find the node to remove

*** 43.7 Solving problems with linked lists
Solving problems with linked lists
1.5 hoursEstimated completion time
Learning Objective
By the end of this lesson, you will be able to create algorithms to solve simple problems using linked lists.

Overview
It's important to understand the inner workings of the linked list data structure and common methods associated with a linked list. But it's just as important to know how to use the linked list data structure to solve problems. The best way to learn to solve problems with linked lists is to practice. So in this lesson, you will be guided through the solutions to several problems.

Starter code
This lesson requires you to have the following repository running on your local machine.

GitHub: Solving problems with linked lists starter
Fork and clone the repository above. Then, follow the instructions on how to get it to run.

In this lesson, you will walk through the process of thinking about and solving several problems using the linked list data structure. You'll attempt to implement each of these problems in JavaScript. The starter code provided contains tests that will help indicate if the implementation is correct.

Problem 1: Reverse a list
Write an algorithm to reverse a linked list. That is, imagine that you're starting with the following linked list:

Linked list with the head pointing to a node with value D, then a node with value A, then a node with value C, then a node with value B.
Given such a list, your algorithm should return the following linked list:

Linked list with the head pointing to a node with value B, then a node with value C, then a node with value A, then a node with value D.
The time complexity of your algorithm should be linear (O(n)).

An iterative solution
The first step to solving such a problem is to consider the various use cases that may occur. For example, you would typically think of a list containing many elements. But would your algorithm work for lists that are empty or that contain just a single element?

To solve this problem iteratively, you can take advantage of the insertAtHead() method of the linked list. If you took each value in the list, starting from the head, and inserted it at the head of another list, then the values would be in the list in reverse order.

Sometimes, drawing a diagram and trying the algorithm by hand can help.

The algorithm begins by declaring a new empty linked list and a node pointer to keep track of the position in the original list.

Given list with the node pointer pointing to the first node, D, next to a new empty list.
Then, for each node in the list, insert the current node value into the new list at the head. Move the node pointer to the next node.

Inserting the value of the first node from the given list into the new list at the head.
This process is repeated for the next node.

Inserting the value of the second node from the given list into the new list at the head.
This continues until the node pointer is null.

Diagram for reversing linked list continued until the next pointer on the last node in the goven list is null.
After the last node is processed, return the new list.

Pseudocode
Next, try writing this process in pseudocode.

function reverseIterative(list)
   // accepts list - a linked list, potentially empty, to be reversed

   declare a variable named reversedList and initialize it to an empty LinkedList
   declare a variable node initialized to the head of the list
   while the node pointer is not null do
     insert the node value at the head of the reversedList
     move the node pointer along to the next node
  return the reversedList
JavaScript implementation
For this first problem, a possible solution implemented in JavaScript is provided here. In the other problems, you will attempt to write the JavaScript code yourself.

function reverseIterative(list) {
  const reversedList = new LinkedList();
  let node = list.head;
  while (node) {
    reversedList.insertAtHead(node.value);
    node = node.next;
  }
  return reversedList;
}
Problem 2: Third from the end
Write an algorithm to find the third element from the end of a linked list, without using the length property.

Diagram showing the third element from the tail of a linked list.
If the list is less than three elements long, return null. The time complexity of your solution should be linear (O(n)).

Solution
Once again, you should consider the various cases that need to be covered. A list of arbitrary length greater than three has an obvious element that is the third from the end. But a list shorter than three elements long doesn't have an element that is third from last. In such a case, you have to return null.

The most straightforward idea is to find the length of the list, then do a second iteration through the list and count length-3 elements. This solution is technically O(n), but you can still improve the running time by traversing the list only once.

The general idea is to use two pointers instead of one to traverse the list. First, use a loop to take one pointer from the head of the list to the third element of the list. If the list is fewer than three elements long, this loop will discover that, and you can return null.

Next, start another pointer at the head. Then keep moving the pointers simultaneously until the first pointer reaches the last element of the list. The second pointer will be pointing to the third element from the end.

First loop takes pointer 1 to the third node, and the second loop moves pointer 1 and pointer 2 simultaneously.
Pseudocode
function thirdFromEnd(list) {
  if the list is empty then return null

  declare a variable named pointer1 and initialize it to the head of the list
  declare a variable named i and initialize it to 0
  while i is less than 2 and the next pointer of pointer1 is not null do
    set pointer1 to its next pointer
    increment i
  if i is less than 2 then return null

  declare a variable named pointer2 and initialize it to the head of the list
  while the next pointer of pointer1 is not null do
    set pointer1 to its next pointer
    set pointer2 to its next pointer

  return pointer2
Problem 3: Swap nodes
Write an algorithm to swap two nodes x and y (and not just their contents) in a singly linked list L, given references to only x and y.

A solution
In this problem, you are given a list and two pointers to specific nodes in the list. It doesn't matter how those nodes were found prior to starting this procedure.

Linked list with pointer x pointing to the second node and pointer y pointing to the fourth node.
You can start by considering how such a swap can be done in a typical case. That is, there are two distinct nodes in a list, and you want to swap them. Before rearranging any of the pointers, you need to find the previous nodes for both x and y. Call those previous node pointers x_prev and y_prev.

For this, you would typically need to start from the head of the list. Then you would iterate through until you find the node that is just before the node that you are looking for. In the given implementation of the linked list that you are working with, the findWithPrevious() method already solves this problem. It would be useful to review how that method gets the job done.

x_prev pointer pointing to the node before the x pointer, and y_prev pointer pointing to the node before the y pointer.
To perform the actual swap, you will need a temporary pointer to keep track of the next node of x. Call that pointer x_next.

Linked list with x_next pointer pointing to the node after x.
Now that you have pointers to all the relevant nodes, you can start rearranging the pointers. Be careful that the order in which you rearrange the pointers does not cause you to lose references to any nodes.

Set x_next to y_next.

Set y_next to x_next.

Swapping the node pointers.
Finally, you need to set the previous nodes to point to the swapped ones.

Set x_prev.next to y.

Set y_prev.next to x.

Swapping the node pointers.
Before you implement this, consider the edge cases. For example, what if one of the nodes was the head of the list or the last node of the list? What if both x and y pointed to the same node? What if the list was empty?

In the case of the empty list, both x and y would also be null. It would be reasonable to return the empty list.

In the case of one of the nodes being the last node of the list, the algorithm outlined above will work. Take a moment to verify this.

In the case where one of the nodes is the head of the list, you just need to ensure that the previous pointers are not null before attempting to reassign them. Whichever one is the head means that the other needs to become the head.

Pseudocode
function swap
  // accepts list - a linked list
             x - a node in the linked list
             y - a node in the linked list

  if the list is empty then
    return  the list

  declare a variable x_next and initialize it to the next pointer of x
  declare a variable x_prev and initialize it to the previous node to x, null if x is the head
  declare a variable y_prev and initialize it to the previous node to y, null if y is the head

  set the next pointer of x to the next pointer of y
  set the next pointer of y to x_next

  if x is not the head  of the list then
    set next pointer of x_prev to y
  else
    set head to y

  if y is not the head  of the list then
    set next pointer of y_prev to x
  else
    set head to x

  return the list
Problem 4: The Josephus problem
To create a circular list, the last node in the list should point to the head of the list.

Use a circular list to solve the Josephus Problem, which is defined like this: People are standing in a circle waiting to be executed. Counting begins at a specified point in the circle and proceeds around the circle in a specified direction. After a specified number of people are skipped, the next person is executed. The procedure is repeated with the remaining people, starting with the next person, going in the same direction and skipping the same number of people, until only one person remains, and is freed. The problem—given the number of people, starting point, direction, and number to be skipped—is to choose the position in the initial circle to avoid execution.

Write an algorithm that accepts a list of people, the number of persons n, and the number to be skipped m. The algorithm should return the name of the last person left alive.

Solution
The first step in this solution is to create a circular list. This involves finding the last node in the list and setting its next pointer to the head of the list.

Circular list with five nodes.
Starting at the head, count m nodes. For example, here, let m=7:

Counting up to m=7 in the circular list and removing the eighth node.
The removal of a node in this case simply involves changing the next pointer of the current node. Then the count starts again at the next node.

Counting up to m=7 in the modified circular list and removing the eighth node.
Similarly, the next node is removed, leaving only three nodes in the list.

Counting up to m=7 in the modified circular list and removing the eighth node.
And now that there are just two nodes in the list, just one more count and removal is necessary.

Counting up to m=7 in the modified circular list and removing the eighth node.
Now that there is only a single node left, the name of the last remaining person is known and can be returned.

Pseudocode
function josephus
  // Accepts: list the LinkedList of names
              m the number to skip

  declare variable node and initialize it to the head
  declare variable tail and initialize it to the last node in the list
  assign the next pointer of the tail to the head of the list

  while there are more than one nodes left do:
    move node to node.next m times
    if the next pointer of the current node points to the head
      set the head to the next node after the head
    set node.next  to  node.next.next
    node = node.next;
  }
  return node.value;
}
Complete example
A completed example from this lesson can be in the Solution branch of this GitHub repository:

Data structures: Solving problems with linked lists—Solution branch

*** 43.8 Stacks and queues

Stacks and queues
1.5 hoursEstimated completion time
Learning Objective
By the end of this lesson, you will be able to define and implement a stack and a queue.

Overview
Two of the most commonly used data structures in web development are stacks and queues. The history of pages visited in a web browser and the undo operation in a text editor are examples of operations made possible using stacks. And the handling of events in web browsers often uses queues.

Key Terms
Event loop
A design pattern that a program may use to prioritize certain lines of code for execution
Starter code
This lesson requires you to have the following repository running on your local machine.

GitHub: Stacks and queues starter

Fork and clone the above repository. Then, follow the instructions on how to get it to run.

Stacks
A stack is one of the most common data structures. It's similar to a list, with access restricted to only one end. It stores elements in a last in, first out (LIFO) order.

Key Term
Stack: A vertical data structure that stores elements in a last in, first out order

Stacks are usually thought of as vertical data structures, unlike lists and arrays, which are horizontal. Hence, the first item—the only directly accessible item on the stack—is referred to as top of the stack.

Imagine a stack of plates in your kitchen. The last plate that you put on the stack stays on the top of the stack, and it is the first plate that you get to take out and use.

Stack with a node with key A on the bottom and a node with key B above that.
A standard way to implement a stack is using a singly linked list with constraints on its operations, where items can be inserted and deleted at only one place: the end of the list.

A stack has two primary functions:

push(): Places data onto the top of a stack (insertion)

pop(): Removes data from the top of the stack (deletion)

Do this
Create a Stack class
In Stack.js, define a constructor for the Stack class, as follows:

// Creates a node containing the data and a reference to the next item
class Node {
  constructor(value, next) {
    this.value = value;
    this.next = next;
  }
}

class Stack {
  constructor() {
    this.top = null;
  }
}
The constructor is nice and straightforward. You start with an empty first node, represented by null. This indicates the top of the stack.

Stack insertion
The insertion operation in a stack is limited to inserting only onto the top of the stack. This operation is called push(). The following illustration shows how a node with the key C is pushed (inserted) onto the stack and what the stack looks like after the item is pushed.

Pushing a node with key C onto the existing stack.
Do this
Create a push() method
In Stack.js, add a push() method to the Stack class that you defined earlier, as follows:

  ...
  push(value) {
    // Create a new node,
    // add data to the new node, and
    // have the pointer point to the top
    this.top = new Node(value, this.top);
    return this;
  }
Complexity analysis for push()
Since you are only adding to the top of the stack, the time complexity of inserting on a stack is constant, O(1).

Stack removal
Removal from a stack is also limited to the top of the stack. This operation is called pop(). The following illustration shows how a node with the key C is popped (removed) from the stack and what the stack looks like after the item is popped.

 Popping a node with the key C from the existing stack.
Do this
Create the pop() method
In Stack.js, add a pop() method to the Stack class:

  ...
  pop() {
      /* In order to remove the top of the stack, you have to point
          the pointer to the next item. That next item becomes the
          top of the stack. */
      const popped = this.top;
      this.top = popped.next;
      return popped.value;
  }

Complexity analysis for pop()
Because you are removing only from the top of the stack, the time complexity of removing an item from a stack is constant, O(1).

Queues
A stack can remove only the most recently added data. It's in LIFO order. So what if you want an operation that is first come, first served? You can use a queue, which is a data structure that models a first in, first out (FIFO) operation. A queue is a type of list where data is inserted at the end and is removed from the front. Queues are used to store data in the order in which they occur—as opposed to a stack, in which the last piece of data entered is the first element used for processing.

Key Term
Queue: A horizontal data structure that stores data in a first in, first out order

An example of a queue is the fast-food service at McDonald's. You line up, and service is provided in the order that you (and everyone else) lined up. If you are first to line up, you get served first.

A more practical example of a queue is the event loop of a web browser. As different events are being triggered (for example, the click of a button), they are added to an event loop's queue and handled in the order they entered the queue. Another example would be a print spooler.

The following is an example of a queue. Rachael is the first item in the queue, and Pris is the last item in the queue.

Queue with Rachael as the first item and Pris as the last item.
Just like stacks, queues can be implemented using a singly linked list or a doubly linked list. An item can be inserted only at the end of the list, and items can be deleted only at the beginning of the list.

The main functions of a queue include the following:

enqueue(data): Adds data to a queue (insertion)

dequeue(): Removes the oldest data added to a queue (deletion)

Do this
Create a Queue class
In Queue.js, define a constructor for the Queue class:

// Creates a node containing the data and a reference to the next item
class Node {
  constructor(value) {
    this.value = value;
    this.next = null;
  }
}

class Queue {
  constructor() {
    this.first = null;
    this.last = null;
  }
}
Queue insertion
The insertion operation in a queue is limited to only one place: the end of the queue. This operation is called enqueue(). For example, in the following illustration, suppose that you have an existing queue, and you want to add an item Pris in the queue. The only place where Pris will be added is the end of the queue.

Enqueuing Pris to the existing queue.
Do this
Create the enqueue() method
In Queue.js, add a enqueue() method to the Queue class that you defined earlier, as follows:

  ...
	enqueue(value) {
    const newNode = new Node(value);

    if (this.first) {
      this.last.next = newNode;
    } else {
      // Set the node of the queue's next pointer to be the new node
      this.first = newNode;
    }

    // Make the new node the last item on the queue
    this.last = newNode;
	}
Queue enqueue() complexity analysis
Since you are adding items only at one place (the end of the queue), the time complexity of inserting in a queue is constant, O(1).

Queue removal
The removal operation in a queue is limited to only one place as well: the beginning of the queue. This operation is called dequeue(). For example, in the following illustration, suppose that you have an existing queue. You can remove an item from the beginning of the queue only. So, if you want to remove something from this queue, Rachael will be removed.

Dequeuing Rachael from the existing queue.
Do this
Create the dequeue() method
In Queue.js, add a dequeue() method to the Queue class, as follows:

  ...
	dequeue() {
    if (this.first) {
      const dequeued = this.first;

      // Update first pointer to point to the next node of the dequeued node
      this.first = dequeued.next;

      // If the dequeued node is the last node in the queue,
      // update the last pointer to point to `null`
      if (dequeued === this.last) {
        this.last = null;
      }

      return dequeued.value;
    }
	}
Queue dequeue() complexity analysis
Because you are removing items from only one place (the beginning of the queue), the time complexity of removing an item from a queue is constant, O(1).

Complete example
A completed example from this lesson can be found here:

GitHub: Stacks and queues—stacks-and-queues-complete branch

*** 43.9 Solving problems with stacks and queues

Solving problems with stacks and queues
1.5 hoursEstimated completion time
Learning Objective
By the end of this lesson, you will be able to create algorithms to solve simple problems using stacks and queues.

Overview
Now that you have fully implemented stack and queue data structures, it is time to apply these structures to problems. Stacks and queues are used in many areas of computing, and they can be quite useful in solving some fairly common problems.

The following problems can all be solved with stacks or queues. For each problem, a brief description of the approach is given, along with a pseudocode algorithm. Your task is to implement the algorithms and observe their behaviors.

Starter code
Starter code for this lesson may be found in this repository:

GitHub: Solving problems with stacks and queues starter
Fork and clone the above repository. Then, follow the instructions on how to get it to run.

Problem 1: Check for a palindrome
A palindrome is a word, phrase, or number that is spelled the same forward and backward. For example, "dad" is a palindrome; "A man, a plan, a canal: Panama" is a palindrome if you take out the spaces and ignore the punctuation; and "1,001" is a numeric palindrome. You can use a stack to determine whether a given string is a palindrome.

Write an algorithm that uses a stack to determine whether a given input is a palindrome. Use the following template as a starting point.

const isPalindrome(sentence) => {
  sentence = sentence.toLowerCase().replace(/[^a-zA-Z0-9]/g, "");
  // Your code goes here
}
Discussion
There are several ways to check if a given sentence is a palindrome. They all boil down to comparing the first half of the string to the second half.

The first step is to remove all non-letter and non-digit characters.

Removing the punctuation, spaces, and capitals from the string "No lemon, no melon" results in the string "nolemonnomelon".
Then split the string into two by dividing its length by two.

Dividing the string "nolemonnomelon" in two results in two strings: "nolemon" and "nomelon".
Lastly, compare the first half with the second half in reverse.

Diagram comparing "nolemon" to "nomelon" in reverse.
Using a stack makes this comparison fairly straightforward. You can just push all the characters from the first half of the string onto a stack. Then continue iterating the second half of the string and comparing each character with a character popped off the stack. This way, the reversal is automatic.

Pushin "nolemon" onto a stack.
When iterating through the second half of the string, pop a single character off of the stack and compare it to the current character from the second half of the string. If they don't match, the string isn't a palindrome.

Note: If a string has an odd number of characters, you will essentially ignore the middle character. This is because the middle character doesn't determine whether the string is a palindrome. It's the characters on either side of the middle value that determine whether the sentence is a palindrome.

If you get to the end of the string and the stack is empty, then the string is a palindrome.

Popping the first character off of the stack and comparing it to the first character from the second half of the string.
Pseudocode
In pseudocode, the above algorithm may be written as follows:

Remove all spaces and punctuation from the sentence and make all characters lowercase.

Declare a variable middle and initialize it to half the length of the sentence, rounding down to an integer value for odd-length strings. For example, if the sentence is of length 7, then middle is 3.

Initialize a new stack.

Iterate through the sentence, from the first character up to middle. Push each character onto the stack.

Iterate from middle to the end of the sentence. If the sentence is an odd length, then iterate from middle+1 to skip the middle character of the sentence. On each iteration, pop a character from the stack and compare it to the current character. If they don't match, return false.

When the loop is done, return true.

Do this
Implement the palindrome algorithm
Use the starter repository to implement the algorithm outlined above. The tests in the repository will help you determine if the algorithm is correct.

Problem 2: Matching parentheses in an expression
Another problem that can be solved using a stack is making sure that equations have matching parentheses.

Arithmetic expressions may contain parentheses for clarification. For example, the following expression is valid:

(a + b) * c
But the following expression is not:

((a + b) * c
Write an algorithm that accepts an expression as a string and returns true if the parentheses in the expression match and false otherwise.

Discussion
A naive approach to solving this problem is to count the number of opening parentheses and the number of closing parentheses and compare them. If they match, then the expression must be correct, right? Not necessarily! For example, what about a+)b-c(+d? There is exactly one opening parenthesis ( and one closing parenthesis ), but the expression isn't correct.

Each closing parenthesis must be matched to an opening parenthesis for this to work. This is where using a stack is useful.

The basic concept is to iterate through each character in the expression. If the character is (, then push it onto the stack.

Note: Because parentheses are the only characters of import in this problem, you will only put ( onto the stack. When you encounter ) in the string, you will compare the ) to the stack of (.

In the example below, by the time that you iterate to index=2, you have placed two ( characters onto the stack.

Two ( characters in the stack.
When iterating through a string, when you reach a ) character, you will pop one ( character off of the stack. This is how you match opening and closing parentheses. As illustrated below, index=5 is a closing parenthesis, so you pop one opening parenthesis off of the stack.

Stack with only one ( character after reaching the first ) character in the expression and popping one ( off the stack.
If the stack is empty and you come across a ) in your string, then you know that the expression is invalid, so return false.

At the end of the expression, an empty stack means that the expression was valid, so return true.

An empty stack at end of expression means valid expression.
If the stack isn't empty at the end of the expression, it means that there is at least one extra ( in the expression.

Pseudocode
In pseudocode, this algorithm goes as follows:

Initialize a new empty stack.

Start a loop to iterate through each character in the expression.

If the current character is (:

Push it onto the stack.

Else:

If the current character is ):

If the stack isn't empty:

Pop one item off the stack.

Else:

Return false.

If the stack is empty:

Return true.

Else:

Return false.

Do this
Implement the parentheses-matching algorithm
Use the starter repository to implement the algorithm outlined above. The tests in the repository will help you determine if the algorithm is correct.

Problem 3: Infix to postfix
Stacks can also be used to convert notations from infix to postfix for disambiguating mathematical operations, as explained below.

An arithmetic expression is said to be in infix notation when it takes the following form:

left-operand operator right-operand
For example, the expressions 1 + 2 and 3 * 5 are infix expressions.

Sometimes, to avoid ambiguities with the order of operations (also called the precedence of operators), parentheses are used. The following are all valid infix expressions:

(2 + 3) * 4
(2 + (4 - 5) * 3)
8 / (6 + 2)
Parsing and evaluating expressions in this form is particularly slow. So typically, your compiler converts arithmetic expressions from infix notation to postfix notation.

An arithmetic expression is said to be in postfix notation when it takes the following form:

left-operand right-operand operator
For example, the infix expression 2 + 3 may be written as 2 3 + in postfix notation.

The following are all valid postfix forms of the infix expressions above:

2 3 + 4 *
2 4 5 - 3 * +
8 6 2 + /
Write an algorithm that will take an arithmetic expression in infix form as a string and return the expression in postfix form.

Assume the following:

The four operators +, -, /, and * are the only ones used.

 * and / have the highest precedence, followed by + and -.

The operands provided are all single characters.

All expressions provided are valid arithmetic expressions, so there's no need to validate them.

Discussion
This algorithm is a little more involved than the previous two. You need to consider the precedence of operators as well as the meaning of parentheses in the expression.

There are several paths to consider. A simple example may help. Suppose that you wanted to convert the infix expression (a + b) * c into postfix notation. Start by initializing a stack and an empty result string.

Then iterate through each character in the expression.

If you come across a (, push it onto the stack. If you come across an operand, add it to the result.

Iterating through each character in the expression, pushing ( characters onto the stack and adding operands to the result.
When you come across an operator, there are a few options to consider. If there is already another operator at the top of the stack, then compare the precedence of the operators. Push the current operator onto the stack if one of the following criteria is met:

It has higher precedence than the one on the stack

The stack is empty

The top of the stack is (

Continuing to iterate through the expression and pushing an operators onto the stack when index=5.
If the current operator has lower precedence than the one on the stack, then simply pop each character off of the stack and append it to the result until you either find an operator that has lower precedence than the current operator, or you find ). Only then, push the current operator onto the stack.

When you find a ), then pop from the stack and append to the result until you find (.

Continuing to iterate through the expression.
When you have processed every character in the expression, simply pop any remaining items from the stack and append them to result. At this point, the result contains the postfix form of the expression.

Pseudocode
Declare a variable named result and initialize it to an empty string.

Iterate through each character in the expression, ignoring spaces.

If the current character is an operand, append it to result.

Otherwise, if it is an operator, do the following:

Look at the operator at the top of the stack.

If the current operator has higher precedence than the operator on the top of the stack, or if the stack is empty or the top of the stack is (, then push the current operator onto the stack.

Otherwise, start popping operators off of the stack. Continue until you either find an operator that isn't of higher or equal precedence to the current operator, or until you find a parenthesis. Append each operator that is popped from the stack to result. Push the current operator onto the stack.

Otherwise, if the current character is (, push it onto the stack.

Otherwise, if the current character is ), then start popping characters off of the stack and append each character to result until you find a (. Do not append the parentheses to result.

Pop any remaining operators from the stack and append them to result.

Return result.

Do this
Implement the infix-to-postfix algorithm
Use the starter repository to implement the algorithm outlined above. The tests in the repository will help you determine if the algorithm is correct.

Problem 4: Are two people connected?
Suppose you work for a massive social network with many millions of users. You need to determine if two given users of the network are connected to each other.

Users of the social network may "follow" each other, but the relationship isn't necessarily mutual. That is, Alice may follow Bob, but Bob may not necessarily follow Alice.

Consider the following diagrammatic representation of a part of the network:

Diagram of social network connections between Alice, Bob, and few other people.
In this diagram, an arrow indicates who a user follows.

Alice follows Bob and Carl

Bob follows Dave, and Dave follows Bob

Both Bob and Eve follow Dave and Fred

Fred doesn't follow anyone

Alice has no followers

The objective is to search for a path from any given user to any other user. For example, there is a path from Bob to Eve. That path is Bob -> Dave -> Carl -> Eve. But there is no path from Eve to Alice.

Internally, this network of users and their connections may be represented as an object where the keys are the user's identity and the values are an array of the users they follow. The above network may be represented like this:

const graph = {
  Alice: ["Bob", "Carl"],
  Bob: ["Dave", "Fred"],
  Carl: ["Eve"],
  Dave: ["Carl", "Bob"],
  Eve: ["Dave", "Fred"],
  Fred: [],
};
Write a function that accepts the above network object and two users. The network object will be named graph, the first user will be named startUser, and the second user will be named endUser. The function will return true if there is a path from startUser to endUser, and false otherwise.

Discussion
A naive approach would require a nested loop that starts at startUser and considers every possible path from startUser until endUser is found. On a large graph, this will simply take too long. Also, keeping track of which paths have already been considered may occupy too much space.

Instead, you can use a breadth-first search (BFS) strategy to search the network of friends, such that you consider each friend in the network only once. The algorithm uses a queue to track which friends have been discovered, and it uses an enqueued array to track which users have been added to the queue.

To understand how the algorithm proceeds, take a look at the following diagrams.

Initially, each user node in the following diagram is white, indicating that the node hasn't been discovered yet. Once discovered, the user is enqueued, and the node is filled with gray lines. After everyone the user is following has been discovered, the user node is colored solid gray. These colors are simply indicators of the state of the user; they're there to make it easier to see how the algorithm proceeds.

In the following example, you are trying to determine if there is a path from Alice to Fred. To implement this algorithm, you will take the following steps:

First step for determining if there is a path from Alice to Fred.
The algorithm proceeds by dequeuing a value—Alice, in this case—from the queue. Everyone who is followed by the dequeued user is added to the queue, and they are now considered discovered.

Second step for determining if there is a path from Alice to Fred.
The iteration continues by dequeuing the next value from the queue and repeating the process. The enqueued array helps ensure that any user is added to the queue only once.

Third step for determining if there is a path from Alice to Fred.
Once the endUser is found in the list of followers, the algorithm can stop because there is a path from startUser to endUser.

If all the nodes are fully explored and endUser isn't found, then there is no path from startUser to endUser.

Pseudocode
The pseudocode algorithm is as follows:

If graph is empty (has no keys), return false.

If startUser is equal to endUser, return true.

Initialize a new array, enqueued, that contains startUser.

Initialize a new empty queue named discovered.

Enqueue startUser.

While discovered isn't empty, do the following:

Dequeue a value from discovered and name it user.

For each friend followedUser in graph[user], do the following:

If followedUser is equal to endUser, return true.

If enqueued doesn't include followedUser, do the following:

Add followedUser to enqueued.

Enqueue friend to discovered.

Return false.

In this algorithm, the enqueued array is used to make the algorithm more efficient by making sure that any given user is explored only once. If a similar problem comes up, you can adapt this algorithm to track information other than enqueued. For example, you might track the number of nodes between two users, or track which user is following the most people.

Do this
Implement the connected-friends algorithm
Use the starter repository to implement the algorithm outlined above. The tests in the repository will help you determine if the algorithm is correct.

Complete example
A completed example from this lesson can be found in the Solution branch of this repository:

GitHub: Solving problems with stacks and queues starter—Solution branch

*** 43.10 Assessment: Data structures

** Module 44 - Trees
*** 44.1 Overview: Trees

Overview: Trees
9 minutesEstimated completion time
Overview
So far, you have learned about linear data structures such as linked lists, stacks, and queues. In this module, you will learn about trees, which are a type of hierarchical data structure.

Key Terms
Tree structure
A type of structure that includes a single root and multiple levels of organization
Root node
A special node with no parent, from which all other nodes descend
Leaf node
A node without any children
Binary tree
A tree with an additional limitation: each node can only have zero, one, or two children (at most)
Subtree
A mini tree within a binary tree, whose root can be any node and all of its descendants rooted at that node
Trees are data structures that consist of nodes that are linked together in a certain way. Nodes in a tree have a parent-child relationship. Each node is linked to zero or more child nodes, and at most one parent. There is a special node called the root node at the top of the tree. The root node has no parent, and it's the node from which all other nodes descend. A nodes without any children is called a leaf node.

Tree data structures are very common. For example, you might have noticed that the file system on your computer and the HTML structure of the websites that you've been building are organized in a treelike structure. Tree data structures can exist in several forms, including a special variant called the binary tree.

A binary tree is a tree with an additional limitation: each node can only have zero, one, or two children (at most). A branch in a tree signifies a decision path, a choice that connects one node to another. A binary tree may have a left branch and a right branch. It may also have subtrees. A subtree is a mini tree within a binary tree, whose root can be any node and all of its descendants rooted at that node.

The following is an example of a binary tree with its different components:

Example of a binary tree with its different components.
In this module, you will learn how to use a common form of the binary tree, the binary search tree, to solve algorithmic problems.

Do this
The purpose of the Do this sections in this module is to give you important hands-on experience. In these sections, you'll complete coding challenges that require you to use the tree data structure. Ultimately, these practice sections will help you successfully complete graded assessments, such as projects, mock interviews, and capstones.

*** 44.2 Defining binary search trees

Defining binary search trees
1.5 hoursEstimated completion time
Learning Objective
By the end of this lesson, you will be able to define what a binary search tree is and describe its key characteristics.

Overview
Recall that one key challenge in computer science is searching and sorting through large volumes of data. If not done properly, sorting and finding the right data can be time consuming. Binary search trees can help you search through data efficiently.

The video below provides a brief introduction to binary search trees.


What are binary search trees?
Binary search trees (BSTs) are an important data structure in computer science. They are a special kind of binary tree with the following characteristics:

Each node has zero, one, or two children.

All of the nodes in the left-hand branch of a node are guaranteed to have lower values than the node itself.

All of the nodes in the right-hand branch of a node are guaranteed to have a higher value than the node itself.

Both the left and right subtrees are guaranteed to be BSTs themselves.

For example, this would be a valid binary search tree:

Example of a valid binary search tree.
As you will see in this module, BSTs allow you to perform efficient search operations, such as looking up, inserting into, or removing values from a tree.

*** 44.3 Implementation and runtime
Implementation and runtime
1.5 hoursEstimated completion time
Learning Objective
By the end of this lesson, you will be able to implement insertion, retrieval, and removal of nodes in a BST. You'll also be able to define the runtime of various operations on a BST.

Key Terms
Balanced tree
A tree in which each row contains two times as many nodes as the row before
Starter code
This lesson requires you to have the following repository running on your local machine.

GitHub: Trees starter

Fork and clone the following repository. Then, follow the instructions on how to get it to run.

Implementing a BST
The video below provides an introduction to binary search tree implementation.


Recall that a BST has the following characteristics: each node in a BST holds a key, a value, a left pointer, and a right pointer. The left and right pointers point to the left and right child nodes. Each node has a parent unless it's the root node.

Do this
Create a BST class
In BinarySearchTree.js, define a constructor for the BinarySearchTree class:

class BinarySearchTree {
    constructor(key = null, value = null, parent = null) {
        this.key = key;
        this.value = value;
        this.parent = parent;
        this.left = null;
        this.right = null;
    }
    ...
Here's the breakdown of the syntax above:

This constructor represents a single node in the tree. You can optionally pass in a key, a value, and a pointer to the parent node.

If the key property is null, then the object represents an empty tree.

If the parent pointer is null, then you are dealing with a root node.

The node starts with the left and right pointers to their child nodes being null.

BSTs support three fundamental operations: insert(), find(), and remove(). You will implement these operations next.

Insertion
The video below provides an overview of the topic.


When you're inserting a new node into a tree, there are two base cases that you should consider:

If there is no existing tree, then the first item that you insert will be inserted as the root of the tree.

If you start with an existing tree, then you have to find the right place for the item that you want to insert and then insert it.

Suppose that you have an existing tree, as shown below, and you want to insert the key 10 into this existing tree.

Example of a binary tree.
Here is an algorithm that you might follow:

Starting at the root, which is 5, compare the root with 10 (the key to insert) to see if 10 should be the left or the right child of 5. 10 is greater than 5. So, as you may recall from the definition of BST, it will be a right child of 5.

5 already has a right child, which is 19. So now you have to check to see if 10 should be on the left or the right side of 19. Because 10 is less than 19, it should be a left child of 19, so you follow the left path of 19.

19 already has a left child, 15, so now you have to check to see if 10 should be on the left or the right side of 15. Because 10 is less than 15, and 15 doesn't have a left child, you can insert 10 as a left child of 15.

This creates a tree that looks like this:

Binary tree resulting from algorithm.
Binary search trees tend to be recursive in nature.

Think about it for a moment. You start at a root node, and you can only have a left and a right child. So if a node can have only two children, then each child node can also be a parent that has its own two child nodes, the left child and the right child. Each tree then can be composed of many subtrees. Each node in a tree is the root of a subtree beginning at that node.

As you may remember from the definition of recursion, recursion generally involves solving a problem in terms of similar subproblems and a base case. In tree recursion, you start with a root, perform an action, and then move to the left or right subtree (or both, successively). This process continues until you reach a null reference, which is the end of a tree (and a good base case).

Do this
Create the insert() method
In BinarySearchTree.js, add an insert() method to the BinarySearchTree class that you defined earlier using recursion. This should look like this:

    ...
    insert(key, value) {
        // If the tree is empty, then this key being inserted is the root node of the tree.
        if (this.key == null) {
            this.key = key;
            this.value = value;
        }

        /* If the tree already exists, then start at the root,
           and compare it to the key that you want to insert.
           If the new key is less than the node's key,
           then the new node needs to live in the left-hand branch. */
        else if (key < this.key) {
            /* If the existing node does not have a left child,
               meaning that the `left` pointer is empty,
               then you can just instantiate and insert the new node
               as the left child of that node, passing `this` as the parent. */
            if (this.left == null) {
                this.left = new BinarySearchTree(key, value, this);
            }
            /* If the node has an existing left child,
               then you recursively call the `insert()` method
               so that the node is added further down the tree. */
            else {
                this.left.insert(key, value);
            }
        }
        /* Similarly, if the new key is greater than the node's key,
           then you do the same thing, but on the right-hand side. */
        else {
            if (this.right == null) {
                this.right = new BinarySearchTree(key, value, this);
            }
            else {
                this.right.insert(key, value);
            }
        }
    }
    ...
Complexity analysis for insert()
How does insertion perform?

Essentially, with insertion, you have to iterate to the height of the tree.

In the best case, you would be inserting the root only, which would be O(1).

In the average case, nodes are inserted equally on the left and right branches. This produces what is known as a balanced tree. A balanced tree is a tree in which each row contains two times as many nodes as the row before; in other words, the width grows exponentially with the number of nodes. Conversely, the height must grow logarithmically with the number of nodes. So the average case for inserting a new key is O(log n).

In the worst case, a BST takes its worst possible shape: the tree skews either left or right. A skewed binary search tree is basically a linked list. Therefore, you will need to iterate through each of those nodes in order to get to the bottom of the tree to insert something. This makes the time complexity O(n).

Retrieval
The video below provides an introduction to node retrieval.


Retrieval of nodes follows a similar pattern as insertion. You check the value of the key against the key stored in a node in the BST and recursively follow the left or right branch.

Suppose you want to know if the key 18 is in the following tree:

Diagram illustrating question of whether 18 is in the binary tree.
You would use this process:

Starting at the root, you have to decide which path to follow to find 18. You check and see that 18 isn't the root. You see that 18 is greater than the root, so you follow the right child of the root, which is 19.

You compare 18 with the right child. Because 18 is less than the right child (19), you follow the left child of 19 (15).

You compare 18 with the left child. Because 18 is greater than the left child, you follow the right path of 15 and find 18.

The find() operation described above is implemented next.

Do this
Create the find() method
Again, as mentioned above, you can make use of recursion to find an item in a BST.

In BinarySearchTree.js, add a find() method to the BinarySearchTree class, as follows:

    ...
    find(key) {
        // If the item is found at the root, then return that value.
        if (this.key == key) {
            return this.value;
        }
        /* If the item that you are looking for is less than the root,
           then follow the left child.
           If there is an existing left child,
           then recursively check its left and/or right child
           until you find the item. */
        else if (key < this.key && this.left) {
            return this.left.find(key);
        }
        /* If the item that you are looking for is greater than the root,
           then follow the right child.
           If there is an existing right child,
           then recursively check its left and/or right child
           until you find the item. */
        else if (key > this.key && this.right) {
            return this.right.find(key);
        }
        // You have searched the tree, and the item isn't in the tree.
        else {
            throw new Error('Key Not Found');
        }
    }
    ...
Complexity analysis for find()
In the best case, the node that you are trying to find is the root node, which would be O(1).

In the average case, you would traverse the height of a balanced tree, which would be O(log n).

In the worst case, just like insert(), the tree is skewed left or right, and you are searching for the node at the bottom where everything is inserted to one side, so it would take O(n) time.

Removal
The video below provides an introduction to removing items from binary search trees.


Removing items from a binary search tree is a little more tricky. First, use the find process described above to find the item that you want to remove. At this point, the removal process will differ depending on how many children that item has. The item that you want to remove will fit into one of the following three categories:

No children (a leaf node)

One child (left or right, doesn't matter)

Two children (left and right)

Node with no children
If you find the item that you want to remove and it has no children, this will be the simplest case of removal. The idea is to detach this node from the parent. Suppose in the following example, you want to delete the item 18.

Diagram showing intent to delete 18 from binary tree.
After you find 18, and see that 18 has no children, you remove it from the BST by breaking the link to the parent.

Diagram showing removal of 18 from the BST by breaking the link to the parent.
The resulting tree looks as follows:

Diagram showing the resulting binary tree with 18 removed.
Node with one child
Consider the following tree, and say that you want to remove the key 28 from it. The key 28 has one child, a left child: 21.

Diagram showing the intent to remove 28 from the binary tree.
If the item that you want to remove has one child, then you make the parent of the item that you are removing point to this one child. Then you detach the item that you want to remove from the parent. You will make 21 the right child of 19 and detach 28 from being the right child of 19.

This way, the child of the node that you removed (21) has a new parent (19), which was the parent of the node that you just removed.

Diagram showing that the child (21) of the removed node (28) has a new parent (19), which was the parent of the removed node.
The resulting tree looks as follows:

Diagram showing the resulting binary tree with 28 removed
Similarly, if you were to remove 15, which has one child (10), you would make 10 the left child of 19 and detach 15 from the tree.

Node with two children
The trickiest part of removal from a BST occurs when an item that you want to remove has both a left and a right child.

Imagine that you want to delete the root node of a BST. You are faced with the decision of which should replace the root. Each left and right child can have subtrees. And more importantly, you must retain the properties of the BST after you remove the node.

To delete a node from a BST that has two children, you must find a successor that will replace the removed node. To do so, follow these steps:

Find the minimum value in the right subtree.

Replace the value of the node to be removed with the found minimum. Now, the right subtree contains a duplicate.

Apply remove() to the right subtree to remove the duplicate.

Suppose that you have the following BST. You will remove the node 19 from it. Notice that 19 has both a left and a right child.

Diagram showing intent to remove 19 from the binary tree.
Start by finding a successor to replace 19. Find the minimum element in the right subtree of 19, which is 20. (Remember, according to the definition of a BST, the minimum value on the right side of a BST is the node in its leftmost subtree.)

Diagram showing how to find a successor to replace 19 before removing it.
Replace 19 with 20. In this case, only the values are replaced, not the nodes. You now have two nodes with the same value.

Diagram showing two nodes with the same value.
Remove 20 from the left subtree. The resulting BST looks as follows:

Resulting BST after removing 20.
Do this
Create the remove() method
In BinarySearchTree.js, add a remove() method to the BinarySearchTree class, as follows:

    ...
    remove(key) {
        if (this.key == key) {
            if (this.left && this.right) {
                const successor = this.right._findMin();
                this.key = successor.key;
                this.value = successor.value;
                successor.remove(successor.key);
            }
            /* If the node only has a left child,
               then you replace the node with its left child. */
            else if (this.left) {
                this._replaceWith(this.left);
            }
            /* And similarly, if the node only has a right child,
               then you replace it with its right child. */
            else if (this.right) {
                this._replaceWith(this.right);
            }
            /* If the node has no children, then
               simply remove it and any references to it
               by calling `this._replaceWith(null)`. */
            else {
                this._replaceWith(null);
            }
        }
        else if (key < this.key && this.left) {
            this.left.remove(key);
        }
        else if (key > this.key && this.right) {
            this.right.remove(key);
        }
        else {
            throw new Error('Key Not Found');
        }
    }
    ...
As you may have noticed from the code above, you will need to add two helper methods that you will use to remove a node: _replaceWith() and _findMin():

_replaceWith() is used to find the node you want to use to replace a node that has children. If the node that you are replacing has a parent, then you need to wire up the references from the parent to the replacement node, and the replacement node back to the parent. Otherwise, if the node is a root node, then you simply copy over the properties of the replacement node.

_findMin() is used to find the minimum value from the right subtree. When you are removing a node from the tree that has two children, you want to replace the node with the smallest node from the right subtree.

In BinarySearchTree.js, add _replaceWith() and _findMin() to the BinarySearchTree class, as follows:

    ...
    _replaceWith(node) {
        if (this.parent) {
            if (this == this.parent.left) {
                this.parent.left = node;
            }
            else if (this == this.parent.right) {
                this.parent.right = node;
            }

            if (node) {
                node.parent = this.parent;
            }
        }
        else {
            if (node) {
                this.key = node.key;
                this.value = node.value;
                this.left = node.left;
                this.right = node.right;
            }
            else {
                this.key = null;
                this.value = null;
                this.left = null;
                this.right = null;
            }
        }
    }

    _findMin() {
        if (!this.left) {
            return this;
        }
        return this.left._findMin();
    }
    ...
Complexity analysis for remove()
For removal of a node from a tree, you can use similar logic as insertion and retrieval to reason that the best case is O(1), the average case is O(log n), and the worst case is O(n).

Complete example
A completed example from this lesson can be found here:

GitHub: Trees starter—implementation and runtime branch

*** 44.4 Depth-first search

Depth-first search
1.5 hoursEstimated completion time
Learning Objective
By the end of this lesson, you will be able to implement in-order, pre-order, and post-order tree traversals.

Overview
Searching through trees can be done in a few different ways. A depth-first search is one of the most common ways.

Key Terms
Depth-first search
DFS, a tree-traversal algorithm that starts from the root node, explores as far as possible in a subtree, and then backtracks before moving to the next subtree
In-order traversal
Traversal in which the left branch of the node is visited, then the current node is handled, and then the right branch is visited
Pre-order traversal
Traversal in which the current node is handled first, then the left branch of the node is visited, and then the right branch is visited
Post-order traversal
Traversal in which the left branch of the node is visited, then the right branch is visited, and then the current node is handled last
Starter code
This lesson continues using the project that you created in the previous lesson. If you need to, you can download that code here:

GitHub: Trees starter

What is depth-first search?
Depth-first search (DFS) is an algorithm for traversing or searching a tree. It's typically implemented recursively. You can use DFS to search a tree using in-order, pre-order, or post-order traversals; each traversal will process the nodes in a different order. Next, you'll learn how to explore a tree with each of these traversals.

In-order traversal
The video below provides an introduction to binary tree in-order traversal. Start by watching the video, and then read through the rest of the lesson and complete the practice tasks. This will give you a thorough understanding of these topics.


In-order traversal means that the left branch of the node is visited, then the current node is handled, and then the right branch is visited.

Do this
Create a dfsInOrder() method
In src/BinarySearchTree.js, add a dfsInOrder() method to the BinarySearchTree class, as follows:

  ...
    dfsInOrder(values = []) {
      // First, process the left node recursively
      if (this.left) {
        values = this.left.dfsInOrder(values);
      }

      // Next, process the current node
      values.push(this.value);

      // Finally, process the right node recursively
      if (this.right) {
        values = this.right.dfsInOrder(values);
      }

      return values;
    }
  ...
Pre-order traversal
The video below provides an introduction to the topic.


Pre-order traversal means that the current node is handled first, then the left branch of the node is visited, and then the right branch is visited.

Do this
Create a dfsPreOrder() method
In src/BinarySearchTree.js, add a dfsPreOrder() method to the BinarySearchTree class, as follows:

  ...
    dfsPreOrder(values=[]) {
      // First, process the current node
      values.push(this.value);

      // Next, process the left node recursively
      if (this.left) {
        values = this.left.dfsPreOrder(values);
      }

      // Finally, process the right node recursively
      if (this.right) {
        values = this.right.dfsPreOrder(values);
      }

      return values;
    }
  ...
Post-order traversal
The video below provides an overview of the topic.


Post-order traversal means that the left branch of the node is visited, then the right branch is visited, and then the current node is handled last.

Do this
Create a dfsPostOrder() method
In src/BinarySearchTree.js, add a dfsPostOrder() method to the BinarySearchTree class, as follows:

  ...
    dfsPostOrder(values = []) {
      // First, process the left node recursively
      if (this.left) {
        values = this.left.dfsPostOrder(values);
      }

      // Next, process the right node recursively
      if (this.right) {
        values = this.right.dfsPostOrder(values);
      }

      // Finally, process the current node
      values.push(this.value);

      return values;
    }
  ...
DFS complexity analysis
Because each node in the BST is visited, the time complexity in the worst case is O(n), where n represents the number of nodes in the tree.

You will be using the same repository in the next lesson, so make sure to save the work that you complete in this lesson.

Complete example
A completed example from this lesson can be found here:

GitHub: Trees—tree-dfs branch

*** 44.5 Breadth-first search

Breadth-first search
1.5 hoursEstimated completion time
Learning Objective
By the end of this lesson, you will be able to implement and analyze the complexity of a breadth-first search algorithm.

Overview
In the previous lesson, you learned about one algorithm that you can use to traverse a tree. In this lesson, you'll learn about another: breadth-first search.

Key Terms
Breadth-first search
BFS, a tree-traversal algorithm that starts at the root node and proceeds level by level
Starter code
This lesson continues using the project that you created in the previous lesson. If you need to, you can download that code here:

GitHub: Trees starter

Breadth-first search
The video below provides a brief introduction to tree breadth-first search. Start by watching the video, and then read through the rest of the lesson and complete the practice tasks. This will give you a thorough understanding of this topic.


Breadth-first search (BFS) works across the rows of a tree. In other words, the top row will be handled first, followed by the second row, and so on and so forth.

The tree is visited level by level. To implement a BFS algorithm, you need a first-in, first-out (FIFO) queue. When you visit a node, you add it to the queue. When all the nodes in the current level have been added to the queue, they are processed according to their order in the queue. The nodes are then removed from the queue. Then their children are visited, adding more values onto the queue.

This process continues until all the nodes in the tree have been visited.

Do this
Create a bfs() method
In src/BinarySearchTree.js, import the Queue class at the top of the file, like this:

const Queue = require("./Queue");
Then, add a bfs() method to the BinarySearchTree class, as follows:

    ...
    bfs(tree, values = []) {
    const queue = new Queue();
    queue.enqueue(tree); // Start the traversal at the tree and add the tree node to the queue to kick off the BFS
    let node = queue.dequeue(); // Remove from the queue
    while (node) {
      values.push(node.value); // Add that value from the queue to an array

      if (node.left) {
        queue.enqueue(node.left); // Add the left child to the queue
      }

      if (node.right) {
        queue.enqueue(node.right); // Add the right child to the queue
      }
      node = queue.dequeue();
    }

    return values;
  }

    ...
BFS complexity analysis
The runtime for BFS is O(n), where n represents the number of nodes in the tree. This is because each node needs to be visited once.

You will be using the same repository in the next lesson, so make sure to save the work that you complete in this lesson.

Complete example
A completed example from this lesson can be found here:

GitHub: Trees—tree-bfs branch

*** 44.6 Solving problems with trees

Solving problems with trees
1.5 hoursEstimated completion time
Learning Objective
By the end of this lesson, you will be able to solve common BST coding challenges.

Overview
Binary search trees are a common topic in technical interviews. Now that you know how to implement and traverse a BST, you can apply your knowledge to solve a variety of interesting coding challenges related to BSTs. In this lesson, you will learn how to find the height of a BST, check whether or not an arbitrary binary tree is a BST, and find the kth largest node in a BST.

Starter code
This lesson continues using the project you created in the previous lesson. If you need to, you can download that code here:

GitHub: Trees starter
You will use the BST that you implemented to understand how you can use this data structure to solve problems in various scenarios.

Challenge 1: Height of a BST
The video below provides an introduction to finding the height of a BST. Start by watching the video, and then read through the rest of the lesson and complete the practice tasks. This will give you a thorough understanding of this topic.


Challenge: Write an algorithm to find the height of a BST.

The height of a binary tree is equal to the number of edges between the root and the deepest node in the tree that has no children (the leaf node). For example, take a look at the binary tree below:

Example of a binary tree.
The height of the binary tree is 3, because there is a total of three edges between 5 (the root node) and 18 (the leaf node).

Before reading on, take a moment to consider how you would solve the problem on your own.

You can follow a recursive approach to solve this problem. You can calculate the height for the current node, and then pass the height down as a variable to the left and right child nodes so they can evaluate their heights. After traversing the entire tree, the variable will contain the result.

In src/BinarySearchTree.js, add a getHeight() method to the BinarySearchTree class, as follows:

  ...
  getHeight(currentHeight = 0) {
    // BASE CASE:
    // If the current node doesn't have a left or right child,
    // then the base case is reached, and the function can return the height.
    if (!this.left && !this.right) return currentHeight;

    // RECURSIVE CASE:
    // Otherwise, compute the new height.
    const newHeight = currentHeight + 1;

    // If there's no left child, recurse down the right subtree only,
    // passing down the height of the current node.
    if (!this.left) return this.right.getHeight(newHeight);

    // If there's no right child, recurse down the left subtree only,
    // passing down the height of the current node.
    if (!this.right) return this.left.getHeight(newHeight);

    // If both children exist, recurse down both subtrees,
    // passing down the height of the current node.
    const leftHeight = this.left.getHeight(newHeight);
    const rightHeight = this.right.getHeight(newHeight);

    // Return the greater of the left or right subtree heights.
    return Math.max(leftHeight, rightHeight);
  }
  ...
The time complexity of the algorithm above is O(n), where n is the number of nodes in the BST. The space complexity is O(h), where h is the height of the tree. Space is allocated for the recursion stack.

Challenge 2: Is it a BST?
The video below provides an introduction to determining whether an arbitrary binary tree is a BST. Start by watching the video, and then read through the rest of the lesson and complete the practice tasks. This will give you a thorough understanding of this topic.


Challenge: Write an algorithm to check whether an arbitrary binary tree is a BST, assuming that the tree doesn't contain duplicates.

Before reading on, take a moment to consider how you would solve the problem on your own.

Recall that earlier in this module, you learned that an in-order traversal of a BST generates a sorted array as an output. Therefore, to check if a given binary tree is a BST, you can traverse the binary tree in order, while comparing the current node's value to the previously visited node's value. If the previous value is always less than the current value, then the binary tree is a BST.

In src/BinarySearchTree.js, add an isBST() method to the BinarySearchTree class, as follows:

  ...
  isBST() {
    // Use the existing `dfsInOrder()` method to traverse the tree.
    const values = this.dfsInOrder();

    // Check if the array returned by the in-order DFS is a sorted array.
    for(let i = 1; i < values.length; i++) {
      // Compare the current and previous values.
      if(values[i] < values[i-1]) {
        return false;
      }
    }
    return true;
  }
  ...
The time complexity of the algorithm above is O(n), where n is the number of nodes in the BST. The space complexity is also O(n), since space is allocated for the auxiliary array created to store the nodes.

Optimizing the space complexity
Can you think of a way to optimize the space used in the algorithm above?

Instead of creating the auxiliary array, you can simply keep track of the previously visited node and compare it to the currently visited node while doing the in-order traversal. The space complexity becomes O(h), where h is the height of the tree, because the algorithm still needs to allocate space for the recursion stack.

Challenge 3: Third-largest node
The video below provides an introduction to finding the third largest node in a BST. Start by watching the video, and then read through the rest of the lesson and complete the practice tasks. This will give you a thorough understanding of this topic.


Challenge: Write an algorithm to find the third-largest node in a BST.

Before reading on, take a moment to consider how you would solve the problem on your own. As in the previous challenge, how can you leverage tree traversal to solve this problem?

Again, you know that an in-order traversal of a BST would generate a sorted array as an output. For example, if the output is [1, 2, 3, 4, 5, 6], then the third-largest node would be found in the third index position from the end of the array. In fact, as a general rule, you can find the index position of the kth largest node by subtracting k from the length of the array.

In src/BinarySearchTree.js, add a findKthLargestValue() method to the BinarySearchTree class, as follows:

  ...
  findKthLargestValue(k) {
    // Use the existing `dfsInOrder()` method to traverse the tree.
    const values = this.dfsInOrder();
    const kthIndex = values.length - k;

    // Ensure that the index is within the bounds of the array.
    if (kthIndex >= 0) {
      return values[kthIndex];
    } else {
      console.error("k value exceeds the size of the BST.");
    }
  }
  ...
The time complexity of the algorithm above is O(n), where n is the number of nodes in the BST. The space complexity is also O(n), because space is allocated for the new array created to store the nodes. How would you optimize the space complexity for this algorithm?

Hopefully, with these examples, you're starting to see how you can use the traversal techniques that you learned in previous lessons to solve different kinds of algorithmic problems for trees.

Note: The solutions presented above are by no means the only ways to solve these problems. It's always recommended to think of alternative approaches or research online to see how other developers approach the same problems differently. However, do pay attention to the time and space complexity trade-offs of different approaches.

Complete example
A completed example from this lesson can be found here:

GitHub: Trees starter—solving-problems-with-trees branch

*** 44.7 Assessment: Trees
*** 44.8 DSA: Mock interview instructions
